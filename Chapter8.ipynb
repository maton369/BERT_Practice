{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a1aa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maton/BERT_Practice/chap8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maton/.pyenv/versions/3.10.13/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# 8-1\n",
    "!mkdir chap8\n",
    "%cd ./chap8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba06fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-3\n",
    "import itertools\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForTokenClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 日本語学習済みモデル\n",
    "MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8cf15c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ＡＢＣ -> ABC\n",
      "ABC -> ABC\n",
      "１２３ -> 123\n",
      "123 -> 123\n",
      "アイウ -> アイウ\n",
      "ｱｲｳ -> アイウ\n"
     ]
    }
   ],
   "source": [
    "# 8-4（説明コメント付き：Unicode 正規化 NFKC による表記ゆれの吸収）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - Unicode の **互換性分解 + 合成**（NFKC）で、全角/半角・互換文字（例：半角ｶﾅ・ローマ数字・① 等）\n",
    "#    を正規化し、検索/重複排除/機械学習前処理での表記ゆれを減らす。\n",
    "#\n",
    "# 理論メモ（NFC/NFD/NFKC/NFKD の違い）：\n",
    "#  - NFC: 正規分解 → 合成（互換性は無視）。見た目を保ちながら正規の合成形式へ。\n",
    "#  - NFD: 正規分解のみ（結合文字にバラす）。検索・照合の下処理に使うことがある。\n",
    "#  - NFKC: **互換分解**（見た目は同じでも “意味的に同一視される” 文字を分解）→ 合成。\n",
    "#          例：全角英数/記号、半角ｶﾅ、ローマ数字 Ⅳ、丸数字 ① などを通常の文字に畳み込む。\n",
    "#  - NFKD: 互換分解のみ。\n",
    "# 互換分解は “情報の落ち” が起こり得る（例：①→1、㍍→メートル→m など）。監査用途では注意。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "# NFKC で正規化する関数\n",
    "# - 全角英数→半角、半角ｶﾅ→全角カタカナ、結合記号の統合 等を一括で行う\n",
    "normalize = lambda s: unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "# 動作例：全角/半角の統一（学習前の前処理・検索キー作成などで有効）\n",
    "print(f'ＡＢＣ -> {normalize(\"ＡＢＣ\")}')  # 全角アルファベット → 半角 \"ABC\"\n",
    "print(f'ABC -> {normalize(\"ABC\")}')  # 既に半角 → 変化なし\n",
    "print(f'１２３ -> {normalize(\"１２３\")}')  # 全角数字 → 半角 \"123\"\n",
    "print(f'123 -> {normalize(\"123\")}')  # 既に半角 → 変化なし\n",
    "print(f'アイウ -> {normalize(\"アイウ\")}')  # 全角カタカナ → 変化なし\n",
    "print(f'ｱｲｳ -> {normalize(\"ｱｲｳ\")}')  # 半角ｶﾅ → 全角カタカナ \"アイウ\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 追加の知見（必要ならテストして確認）：\n",
    "#  - 濁点付き半角ｶﾅ（例：ｶﾞ）も結合が解決され全角「ガ」に統一される：\n",
    "#      normalize(\"ｶﾞ\") == \"ガ\"\n",
    "#  - 互換文字の折り畳み：\n",
    "#      normalize(\"ⅠⅡⅢ\") -> \"III\"  （ローマ数字 → ラテン大文字）\n",
    "#      normalize(\"①②\")   -> \"12\"   （丸数字 → 通常数字）\n",
    "#  - 正規化は “可逆でない” ことがあるため、**原文は別項目で保存**しておくと安全。\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bea67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-5（説明コメント付き：日本語BERT用 NER 前処理ユーティリティ）\n",
    "# =============================================================================\n",
    "# 前提：\n",
    "#  - BertJapaneseTokenizer（transformers）を継承して、固有表現抽出（NER）向けの\n",
    "#    2種類のエンコード（教師あり/教師なし）と、モデル出力から固有表現スパンを復元する\n",
    "#    補助関数を提供するクラスである。\n",
    "#  - ラベリング方式は BIO ではなく「整数 ID によるスパン連結」方式（O=0, その他=タイプID）。\n",
    "#    → 連続する同一ラベルのトークン列を 1 つのエンティティにまとめる前提。\n",
    "#  - 重複・ネスト・オーバーラップするエンティティは表現できない（単純スパンのみ）。\n",
    "#  - prepare_for_model により [CLS]/[SEP] の特殊トークンが付与される前提で、ラベルを先頭/末尾0に整合。\n",
    "#  - max_length を超える部分は切り捨てられるため、切断されたエンティティは欠落し得る点に注意。\n",
    "#  - encode_plus_untagged では、MeCab 分かち書き（word_tokenizer）→ WordPiece（subword_tokenizer）\n",
    "#    で二段階トークナイズを行い、トークン表層を原文へシーケンシャルマッチしてスパンを得る。\n",
    "#    同一トークンの繰り返しが多い文でも、左から貪欲にマッチしていくため順次対応できる。\n",
    "#  - return_tensors='pt' 指定時、encoding のみ Tensor 化する（spans はリストのまま返る）。\n",
    "#    下流でテンソルを期待する場合は適宜変換すること。\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class NER_tokenizer(BertJapaneseTokenizer):\n",
    "\n",
    "    def encode_plus_tagged(self, text, entities, max_length):\n",
    "        \"\"\"\n",
    "        文章とそれに含まれる固有表現が与えられた時に、\n",
    "        符号化とラベル列の作成を行う。\n",
    "\n",
    "        引数:\n",
    "          text: str\n",
    "            元の文章（原文）。エンティティ抽出の対象。\n",
    "          entities: List[Dict]\n",
    "            [{'span': [start, end], 'type_id': int}, ...] の形を想定。\n",
    "            span は原文 text における [start, end) の半開区間、type_id は 0 以外の整数。\n",
    "          max_length: int\n",
    "            BERT 入力の最大系列長。prepare_for_model で [CLS]/[SEP] を含む長さに正規化される。\n",
    "\n",
    "        戻り値:\n",
    "          encoding: Dict[str, List[int]]\n",
    "            input_ids / attention_mask / token_type_ids / labels を含む辞書。\n",
    "            labels は [0]=CLS, 末尾 [0]=SEP/必要に応じ PAD を 0 で詰める。\n",
    "        \"\"\"\n",
    "        # --- 前処理: エンティティを開始位置で昇順ソート ---\n",
    "        entities = sorted(entities, key=lambda x: x[\"span\"][0])\n",
    "\n",
    "        # --- 原文を「非固有表現片(O=0)」「固有表現片(=type_id)」に分割して並べる ---\n",
    "        splitted = []  # 分割後の文字列片を順に蓄積\n",
    "        position = 0  # 直前に処理した末尾の次インデックス\n",
    "        for entity in entities:\n",
    "            start = entity[\"span\"][0]\n",
    "            end = entity[\"span\"][1]\n",
    "            label = entity[\"type_id\"]\n",
    "            # 非固有表現部分（直近の position からエンティティ開始まで）→ ラベル 0\n",
    "            splitted.append({\"text\": text[position:start], \"label\": 0})\n",
    "            # 固有表現部分（[start:end)）→ ラベル type_id\n",
    "            splitted.append({\"text\": text[start:end], \"label\": label})\n",
    "            position = end\n",
    "        # 最後のエンティティ以降の末尾テキストも非固有表現として追加\n",
    "        splitted.append({\"text\": text[position:], \"label\": 0})\n",
    "        # 空文字は除去（連続するエンティティで start==end 等により空が生じ得るため）\n",
    "        splitted = [s for s in splitted if s[\"text\"]]\n",
    "\n",
    "        # --- 片ごとにトークン化し、各トークンへ片に対応するラベルを付与 ---\n",
    "        tokens = []  # WordPiece トークン列\n",
    "        labels = []  # トークンに対するラベル（0 or type_id）\n",
    "        for text_splitted in splitted:\n",
    "            text = text_splitted[\"text\"]\n",
    "            label = text_splitted[\"label\"]\n",
    "            # BertJapaneseTokenizer.tokenize は日本語前処理（MeCab）+ WordPiece を内部で実施\n",
    "            tokens_splitted = self.tokenize(text)\n",
    "            labels_splitted = [label] * len(tokens_splitted)\n",
    "            tokens.extend(tokens_splitted)\n",
    "            labels.extend(labels_splitted)\n",
    "\n",
    "        # --- トークン列を ID 列に変換し、BERT 入力辞書へ整形 ---\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids, max_length=max_length, padding=\"max_length\", truncation=True\n",
    "        )  # prepare_for_model が [CLS]/[SEP] 等を付与し、長さを揃える\n",
    "\n",
    "        # --- ラベルの特殊トークン・PAD 整合 ---\n",
    "        # 先頭に CLS=0 を付与、本文ラベルは最大長-2（CLS/SEP）までに切詰め、末尾に SEP=0\n",
    "        labels = [0] + labels[: max_length - 2] + [0]\n",
    "        # さらに不足分（PAD 部分）を 0 で埋め、最終的に長さを max_length に一致させる\n",
    "        labels = labels + [0] * (max_length - len(labels))\n",
    "        encoding[\"labels\"] = labels\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def encode_plus_untagged(self, text, max_length=None, return_tensors=None):\n",
    "        \"\"\"\n",
    "        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
    "\n",
    "        引数:\n",
    "          text: str\n",
    "            元の文章。\n",
    "          max_length: Optional[int]\n",
    "            指定時は padding='max_length', truncation=True で固定長化。\n",
    "            未指定時は可変長（特殊トークン付与は維持）。\n",
    "          return_tensors: Optional[str]\n",
    "            'pt' を指定すると encoding の各値を torch.Tensor(batch次元つき) に変換。\n",
    "\n",
    "        戻り値:\n",
    "          encoding: Dict[str, List[int]] or Dict[str, torch.Tensor]\n",
    "            prepare_for_model の出力（[CLS]/[SEP] 付与後）。\n",
    "          spans: List[List[int]]\n",
    "            各トークンに対応する原文上の [start, end) 位置。\n",
    "            特殊トークン([CLS],[SEP],[PAD])の位置は [-1,-1] のダミーにする。\n",
    "        \"\"\"\n",
    "        # --- 形態素（MeCab）→ サブワード（WordPiece）の二段トークナイズ ---\n",
    "        tokens = []  # WordPiece トークン\n",
    "        tokens_original = (\n",
    "            []\n",
    "        )  # スパン計算用の「表層文字列」列（'##' 除去/UNK は単語そのまま）\n",
    "        words = self.word_tokenizer.tokenize(text)  # MeCab による単語列\n",
    "        for word in words:\n",
    "            # 単語を WordPiece に分割\n",
    "            tokens_word = self.subword_tokenizer.tokenize(word)\n",
    "            tokens.extend(tokens_word)\n",
    "            if (\n",
    "                tokens_word[0] == \"[UNK]\"\n",
    "            ):  # 未知語は WordPiece に分割できない → 原単語をそのまま使う\n",
    "                tokens_original.append(word)\n",
    "            else:\n",
    "                # '##' 接頭辞を削って表層形を復元（スパン同定に用いる）\n",
    "                tokens_original.extend(\n",
    "                    [token.replace(\"##\", \"\") for token in tokens_word]\n",
    "                )\n",
    "\n",
    "        # --- 原文上のスパンを左から順次マッチで同定（空白/記号も考慮して進める）---\n",
    "        position = 0  # 原文上の走査位置（左から前進のみ）\n",
    "        spans = []  # 各トークンの [start, end) を蓄積\n",
    "        for token in tokens_original:\n",
    "            l = len(token)\n",
    "            while 1:\n",
    "                # 現在位置から長さ l を切り出して一致判定（空白等を飛ばすため不一致なら1文字進める）\n",
    "                if token != text[position : position + l]:\n",
    "                    position += 1\n",
    "                else:\n",
    "                    spans.append([position, position + l])\n",
    "                    position += l\n",
    "                    break\n",
    "        # ここでの手法は「逐次マッチ」のため、同一部分文字列の繰り返しがあっても左から順に整合が取れる。\n",
    "        # ただし、原文編集（正規化や空白折り畳み）を別途行う場合は、同じ変換を token 側にも適用しておくこと。\n",
    "\n",
    "        # --- BERT 入力辞書へ整形（固定長/可変長を分岐）---\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\" if max_length else False,\n",
    "            truncation=True if max_length else False,\n",
    "        )\n",
    "        sequence_length = len(\n",
    "            encoding[\"input_ids\"]\n",
    "        )  # [CLS]/[SEP] を含む最終トークン列長\n",
    "\n",
    "        # --- 特殊トークン分のダミー span を付与し、長さを encoding に一致させる ---\n",
    "        # 先頭 [CLS] 用のダミー\n",
    "        spans = [[-1, -1]] + spans[: sequence_length - 2]\n",
    "        # 末尾 [SEP] と PAD 分をダミーで埋める（長さを sequence_length に揃える）\n",
    "        spans = spans + [[-1, -1]] * (sequence_length - len(spans))\n",
    "\n",
    "        # --- 必要なら Tensor 化（encoding のみ。spans はリストで返す） ---\n",
    "        if return_tensors == \"pt\":\n",
    "            encoding = {k: torch.tensor([v]) for k, v in encoding.items()}\n",
    "\n",
    "        return encoding, spans\n",
    "\n",
    "    def convert_bert_output_to_entities(self, text, labels, spans):\n",
    "        \"\"\"\n",
    "        文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n",
    "\n",
    "        引数:\n",
    "          text: str\n",
    "            原文。\n",
    "          labels: Sequence[int]\n",
    "            各トークンのラベル（O=0, その他は type_id）。[CLS]/[SEP]/[PAD] 位置の分も含む想定。\n",
    "          spans: Sequence[List[int]]\n",
    "            各トークンの [start, end)。特殊トークンは [-1,-1]。\n",
    "\n",
    "        戻り値:\n",
    "          entities: List[Dict]\n",
    "            {\"name\": 原文片, \"span\": [start,end], \"type_id\": ラベルID} の配列。\n",
    "        \"\"\"\n",
    "        # --- 特殊トークン（span=-1）に対応するラベル/スパンを除去して本文トークンに限定 ---\n",
    "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
    "        spans = [span for span in spans if span[0] != -1]\n",
    "\n",
    "        # --- 連続する同一ラベルをまとめて 1 スパンのエンティティに復元 ---\n",
    "        # itertools.groupby により、labels の連続区間ごとにグルーピングする。\n",
    "        # 例: labels=[0,0,2,2,0,3] → (0区間)(2区間)(0区間)(3区間)\n",
    "        # ラベル0（O）は無視し、非0の区間のみエンティティを生成する。\n",
    "        entities = []\n",
    "        for label, group in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
    "\n",
    "            group = list(group)  # group は [(idx,label), ...] の列\n",
    "            start = spans[group[0][0]][0]  # 区間先頭トークンの start\n",
    "            end = spans[group[-1][0]][1]  # 区間末尾トークンの end（半開区間の終端）\n",
    "\n",
    "            if label != 0:  # O 以外のラベルのみエンティティ化\n",
    "                entity = {\n",
    "                    \"name\": text[start:end],  # 原文断片（復元テキスト）\n",
    "                    \"span\": [start, end],  # 原文上の [start,end)\n",
    "                    \"type_id\": label,  # ラベル ID（BIO ではなく type_id そのもの）\n",
    "                }\n",
    "                entities.append(entity)\n",
    "\n",
    "        # 注意：\n",
    "        #  - BIO 方式ではないため、同一ラベルが連続すれば 1 エンティティに連結される。\n",
    "        #    同じ type_id が離れて複数回出現する場合は、それぞれ別エンティティとして復元される。\n",
    "        #  - ネスト/オーバーラップは扱えない（単純連続区間のみ）。\n",
    "        #  - max_length による切詰めでエンティティの一部が失われると、復元されない場合がある。\n",
    "\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f4f30b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 8-6\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c10f011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 10271, 28486, 5, 546, 3000, 1518, 233, 13, 5, 1878, 2682, 9, 10750, 308, 10, 8, 3, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], 'labels': [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# 8-7（説明コメント付き：NER_tokenizer.encode_plus_tagged の動作確認）\n",
    "# =============================================================================\n",
    "# 目的：\n",
    "#  - 原文 `text` と、原文上のスパン（[start, end) の半開区間）＋ラベルIDからなる `entities`\n",
    "#    を与え、BERT 入力（input_ids/attention_mask/token_type_ids）に整形すると同時に、\n",
    "#    各トークンに対応する **ラベル列**（O=0, エンティティは type_id）を作成する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - encode_plus_tagged は「BIO 方式」ではなく **連結方式**（同一ラベルが連続するトークン列を1つの\n",
    "#    エンティティとみなす）前提でラベルを作る。特殊トークン [CLS]/[SEP]/[PAD] は 0 として埋める。\n",
    "#  - `span=[start,end]` は **半開区間**（start 文字を含み end 文字を含まない）。インデックスは 0 始まり。\n",
    "#  - トークン化は WordPiece なので、1語が複数トークンに分割されても、分割前の片に付けたラベルが\n",
    "#    そのまま各トークンに複製される設計。\n",
    "#  - `max_length` 超過分は切り捨てられるため、末尾側のエンティティが途中で切れると、対応トークンに\n",
    "#    ラベルが乗らず欠落し得る点に注意（長さ設計 or スライディングで対処）。\n",
    "#  - 直前に正規化（例：NFKC）を行った場合、**スパンは正規化後の文字列基準**で与えること（前後で\n",
    "#    文字長が変わるとズレる）。\n",
    "# =============================================================================\n",
    "\n",
    "# （互換/保守）NER_tokenizer が未インポート/未定義でも動くように最小限のフォールバックを用意\n",
    "try:\n",
    "    NER_tokenizer\n",
    "except NameError:\n",
    "    # ここではクラス本体は 8-5 で定義済み想定。未定義なら明示エラーにする。\n",
    "    raise RuntimeError(\n",
    "        \"NER_tokenizer クラスが未定義です。先に 8-5 の定義を実行してください。\"\n",
    "    )\n",
    "\n",
    "# 既存の `tokenizer` が NER_tokenizer でない場合は、同名で置き換えても構わない運用なら差し替える\n",
    "try:\n",
    "    tokenizer\n",
    "    has_encode_tagged = hasattr(tokenizer, \"encode_plus_tagged\")\n",
    "except NameError:\n",
    "    has_encode_tagged = False\n",
    "\n",
    "if not has_encode_tagged:\n",
    "    # 学習時と同じ語彙を使う（未定義なら東北大BERTにフォールバック）\n",
    "    try:\n",
    "        MODEL_NAME\n",
    "    except NameError:\n",
    "        MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --------------------------- 元のコード（＋説明コメント） ---------------------------\n",
    "\n",
    "# 入力テキスト（インデックスは 0 始まり）\n",
    "# 文字位置: 0:昨,1:日,2:の,3:み,4:ら,5:い,6:事,7:務,8:所,9:と,10:の,11:打,12:ち,13:合,14:わ,15:せ,16:は,17:順,18:調,19:だ,20:っ,21:た,22:。\n",
    "text = \"昨日のみらい事務所との打ち合わせは順調だった。\"\n",
    "\n",
    "# エンティティの指定：\n",
    "#  - name は人間可読の補助で、実際のラベル付与は span と type_id によって行われる\n",
    "#  - span=[3,9) → 原文中の「みらい事務所」（3〜8文字目）を指定（end=9 は「と」なので非包含）\n",
    "#  - type_id=1 → O（背景）を 0 とするクラス設計の「1番」カテゴリ\n",
    "entities = [{\"name\": \"みらい事務所\", \"span\": [3, 9], \"type_id\": 1}]\n",
    "\n",
    "# ラベル列作成つきの符号化（最大長は [CLS]/[SEP] を含むトークン長に正規化される）\n",
    "# labels は先頭[CLS]=0、本文（max_length-2 まで）、末尾[SEP]=0、残りPAD=0 で埋められる。\n",
    "encoding = tokenizer.encode_plus_tagged(text, entities, max_length=20)\n",
    "\n",
    "# 出力の中身は Hugging Face の標準キー（input_ids, attention_mask, token_type_ids）に加え、\n",
    "# 1:1 に対応する labels が入っている。確認用にそのまま表示。\n",
    "print(encoding)\n",
    "\n",
    "# 追加の確認（任意）：\n",
    "# - トークン列長とラベル列長が一致していること\n",
    "# - ラベル中の 1 の連続区間が「みらい事務所」の WordPiece 分割数に一致していること\n",
    "# 例：\n",
    "# tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "# print(tokens)\n",
    "# print(encoding[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85a51968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# encoding\n",
      "{'input_ids': tensor([[    2,     1,     5,  1543,   125,     9,  6749, 28550,  2953, 28550,\n",
      "         28566, 21202, 28683, 14050, 12475,    12,    31,     8,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "# spans\n",
      "[[-1, -1], [0, 2], [2, 3], [3, 5], [5, 6], [6, 7], [7, 9], [9, 10], [10, 12], [12, 13], [13, 14], [15, 18], [18, 19], [19, 23], [24, 27], [27, 28], [28, 30], [30, 31], [-1, -1]]\n"
     ]
    }
   ],
   "source": [
    "# 8-8\n",
    "text = \"騰訊の英語名はTencent Holdings Ltdである。\"\n",
    "encoding, spans = tokenizer.encode_plus_untagged(text, return_tensors=\"pt\")\n",
    "print(\"# encoding\")\n",
    "print(encoding)\n",
    "print(\"# spans\")\n",
    "print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55cb9eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': '騰訊の', 'span': [0, 3], 'type_id': 1}, {'name': 'ncent Holdings Ltdで', 'span': [9, 28], 'type_id': 1}]\n"
     ]
    }
   ],
   "source": [
    "# 8-9\n",
    "labels_predicted = [0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "entities = tokenizer.convert_bert_output_to_entities(text, labels_predicted, spans)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb0ff5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] device = mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 8-10（説明コメント付き：日本語BERTをトークン分類（NER）用に初期化）\n",
    "# =============================================================================\n",
    "# 目的：\n",
    "#  - 日本語BERTの語彙・前処理（MeCab + WordPiece）に対応した **NER_tokenizer** を読み込み、\n",
    "#    事前学習モデルの上に **BertForTokenClassification** ヘッド（num_labels=4）を載せる。\n",
    "#  - MacBook 環境（Apple Silicon/MPS）・CUDA・CPU のいずれでも動くように **デバイス自動選択**で配置。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - Token Classification は各トークン t に対してクラスのロジット z_{t,c} を出力し、\n",
    "#    学習時は通常 CrossEntropyLoss（各位置の多クラス）を用いる。\n",
    "#  - NER ではしばしば **BIO/IOBES** を採用するが、本書式は「O=0, エンティティ種別ID>0」\n",
    "#    のシンプル方式（連結方式）も可能。`num_labels=4` は例として\n",
    "#       0: \"O\", 1: \"ORG\", 2: \"PER\", 3: \"LOC\"\n",
    "#    のように解釈できる（実際の定義はデータに合わせて固定・共有すること）。\n",
    "#  - 学習時に `[CLS]/[SEP]/[PAD]` 位置のラベルを損失から除外するには、\n",
    "#    データ側のラベルを **ignore_index（例：-100）** にしておくのが Hugging Face の定石。\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "from transformers import BertForTokenClassification\n",
    "\n",
    "# --- （参考）元のコード（実行はしない。可搬性の観点で .cuda() 固定は避けるため） ---\n",
    "# tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "# bert_tc = BertForTokenClassification.from_pretrained(\n",
    "#     MODEL_NAME, num_labels=4\n",
    "# )\n",
    "# bert_tc = bert_tc.cuda()\n",
    "\n",
    "# --- MODEL_NAME が未定義なら東北大BERTにフォールバック ---\n",
    "try:\n",
    "    MODEL_NAME\n",
    "except NameError:\n",
    "    MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "\n",
    "# --- NER_tokenizer クラスの存在確認（8-5 で定義済み想定） ---\n",
    "try:\n",
    "    NER_tokenizer\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\n",
    "        \"NER_tokenizer クラスが未定義です。先に 8-5 の定義を実行してください。\"\n",
    "    ) from e\n",
    "\n",
    "# --- ラベル定義（例）。実データに合わせて **固定・共有** することが重要 ---\n",
    "num_labels = 4\n",
    "id2label = {0: \"O\", 1: \"ORG\", 2: \"PER\", 3: \"LOC\"}  # 例：組織/人/場所\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "\n",
    "# --- デバイス自動選択（Mac MPS → CUDA → CPU） ---\n",
    "def pick_device() -> torch.device:\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = pick_device()\n",
    "print(f\"[info] device = {device}\")\n",
    "\n",
    "# --- トークナイザ：日本語用の NER_tokenizer（MeCab + WordPiece 連携） ---\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- モデル本体：事前学習BERT + Token Classification ヘッド ---\n",
    "bert_tc = (\n",
    "    BertForTokenClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,  # 推論・可視化で役立つ（config に保存され ckpt へも残る）\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    .to(device)\n",
    "    .eval()\n",
    ")  # 推論時は eval()。学習時は train() に切替。\n",
    "\n",
    "# =============================================================================\n",
    "# 補足（実務向け）：\n",
    "#  - 学習：\n",
    "#      - ラベル列の特殊トークン位置は `-100`（ignore_index）にして DataCollator などで結合。\n",
    "#      - Optimizer は AdamW、Scheduler は linear/warmup を推奨。\n",
    "#  - 推論：\n",
    "#      - `outputs = bert_tc(**batch)` → `logits.argmax(-1)` でラベルID列。\n",
    "#      - `tokenizer` から `offset_mapping`（Fast版）を使える場合、原文スパン復元が堅牢。\n",
    "#  - 再現性：\n",
    "#      - `MODEL_NAME`、`id2label`、`max_length`、正規化の有無（NFKC）を **明示的に固定**・記録。\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29c545c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.4404\n"
     ]
    }
   ],
   "source": [
    "# 8-12（修正版：CUDA 未有効エラーの解消と実務的ガードを追加）\n",
    "# =============================================================================\n",
    "# エラー原因：\n",
    "#   AssertionError: Torch not compiled with CUDA enabled\n",
    "# → 環境が CUDA 非対応なのに `.cuda()` を呼んだため。\n",
    "#\n",
    "# 対処方針：\n",
    "#  1) モデルが載っている実デバイスに合わせて **.to(device)** に統一（MPS/CUDA/CPU どれでも可）。\n",
    "#  2) （任意だが推奨）特殊トークン([CLS]/[SEP]/[PAD])と PAD 部分のラベルを **-100** にして\n",
    "#     CrossEntropyLoss の ignore_index に合わせ、損失に寄与させない。\n",
    "#     ※ 8-5 実装はこれらを 0=O にしているため、何もしないと O クラス過多で学習が歪む。\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- 手作り NER データ（元コードそのまま） ---\n",
    "data = [\n",
    "    {\n",
    "        \"text\": \"AさんはB大学に入学した。\",\n",
    "        \"entities\": [\n",
    "            {\"name\": \"A\", \"span\": [0, 1], \"type_id\": 2},\n",
    "            {\"name\": \"B大学\", \"span\": [4, 7], \"type_id\": 1},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"CDE株式会社は新製品「E」を販売する。\",\n",
    "        \"entities\": [\n",
    "            {\"name\": \"CDE株式会社\", \"span\": [0, 7], \"type_id\": 1},\n",
    "            {\"name\": \"E\", \"span\": [12, 13], \"type_id\": 3},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# --- 符号化：8-5 の encode_plus_tagged を使用（元コードそのまま） ---\n",
    "max_length = 32\n",
    "dataset_for_loader = []\n",
    "for sample in data:\n",
    "    text = sample[\"text\"]\n",
    "    entities = sample[\"entities\"]\n",
    "    encoding = tokenizer.encode_plus_tagged(text, entities, max_length=max_length)\n",
    "    # HF の TokenClassification は labels を long テンソル（クラスID）で受けるのが定石\n",
    "    encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "    dataset_for_loader.append(encoding)\n",
    "\n",
    "# --- DataLoader（元コードの方針：全件を 1 ミニバッチに） ---\n",
    "dataloader = DataLoader(dataset_for_loader, batch_size=len(data))\n",
    "\n",
    "# --- デバイス：モデル側に合わせて自動決定（.cuda() を撤廃） ---\n",
    "device = next(bert_tc.parameters()).device  # 例：mps/cuda/cpu のいずれか\n",
    "\n",
    "\n",
    "# --- 推奨：特殊トークンと PAD を損失から除外（ignore_index=-100）に置換する関数 ---\n",
    "def mask_special_positions_to_ignore_index(batch_dict, tokenizer, ignore_index=-100):\n",
    "    \"\"\"\n",
    "    TokenClassification の損失から除外したい位置（[CLS]/[SEP]/[PAD] と attention_mask=0）を\n",
    "    ラベル上で ignore_index に置き換える。\n",
    "    \"\"\"\n",
    "    input_ids = batch_dict[\"input_ids\"]\n",
    "    attention_mask = batch_dict[\"attention_mask\"]\n",
    "    labels = batch_dict[\"labels\"]\n",
    "\n",
    "    # PAD 位置（attention_mask==0）を -100 に\n",
    "    labels = labels.masked_fill(attention_mask.eq(0), ignore_index)\n",
    "\n",
    "    # CLS/SEP 位置を -100 に（存在チェック込み）\n",
    "    cls_id = tokenizer.cls_token_id\n",
    "    sep_id = tokenizer.sep_token_id\n",
    "    for i in range(labels.size(0)):\n",
    "        cls_pos = (input_ids[i] == cls_id).nonzero(as_tuple=True)[0]\n",
    "        sep_pos = (input_ids[i] == sep_id).nonzero(as_tuple=True)[0]\n",
    "        if cls_pos.numel() > 0:\n",
    "            labels[i, cls_pos] = ignore_index\n",
    "        if sep_pos.numel() > 0:\n",
    "            labels[i, sep_pos] = ignore_index\n",
    "\n",
    "    batch_dict[\"labels\"] = labels\n",
    "    return batch_dict\n",
    "\n",
    "\n",
    "# --- ミニバッチで損失を計算（.to(device) で可搬化） ---\n",
    "for batch in dataloader:\n",
    "    # 元コードの `.cuda()` を **.to(device)** に差し替え（MPS/CPU でも動く）\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    # （任意・推奨）損失に含めない位置を ignore_index=-100 に置換\n",
    "    batch = mask_special_positions_to_ignore_index(batch, tokenizer, ignore_index=-100)\n",
    "\n",
    "    # 推論または学習（ここでは損失取得のみ）\n",
    "    output = bert_tc(**batch)  # labels が含まれているため内部で CrossEntropyLoss を計算\n",
    "    loss = output.loss  # 平均化されたトークン単位 CE 損失\n",
    "    print(f\"loss = {loss.item():.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 理論補足：\n",
    "#  - CrossEntropyLoss はクラス ID（long）を取り、ignore_index（既定 -100）位置は損失計算から除外。\n",
    "#  - ignore_index を適用すると、PAD や特殊トークンの頻度に損失が引っぱられず、学習が安定。\n",
    "#  - 連結方式（O=0, type_id>0）でも動作するが、隣接同タイプを分離したいなら BIO/IOBES へ拡張。\n",
    "#  - 実学習では bert_tc.train()、評価や予測では bert_tc.eval() + no_grad() を併用する。\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4122016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ner-wikipedia-dataset'...\n",
      "remote: Enumerating objects: 35, done.\u001b[K\n",
      "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
      "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
      "remote: Total 35 (delta 11), reused 11 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (35/35), 664.15 KiB | 655.00 KiB/s, done.\n",
      "Resolving deltas: 100% (11/11), done.\n",
      "Note: switching to 'f7ed83626d90e5a79f1af99775e4b8c6cba15295'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch v2.0 https://github.com/stockmarkteam/ner-wikipedia-dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0e3a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-14\n",
    "# データのロード\n",
    "dataset = json.load(open(\"ner-wikipedia-dataset/ner.json\", \"r\"))\n",
    "# ↑ 事前作成済みの NER 用 JSON を読み込む。\n",
    "#    想定フォーマット：\n",
    "#      sample = {\n",
    "#        \"text\": <文字列>,\n",
    "#        \"entities\": [\n",
    "#           {\"name\": <表層>, \"span\": [start, end], \"type\": <カテゴリ名>}, ...\n",
    "#        ]\n",
    "#      }\n",
    "#    ※ 文字インデックス span は 0 始まりの半開区間 [start, end) を想定。\n",
    "#    ※ エンコード（UTF-8 など）に依存する場合は open(..., encoding='utf-8') を明示すると安全。\n",
    "\n",
    "# 固有表現のタイプとIDを対応付る辞書\n",
    "type_id_dict = {\n",
    "    \"人名\": 1,\n",
    "    \"法人名\": 2,\n",
    "    \"政治的組織名\": 3,\n",
    "    \"その他の組織名\": 4,\n",
    "    \"地名\": 5,\n",
    "    \"施設名\": 6,\n",
    "    \"製品名\": 7,\n",
    "    \"イベント名\": 8,\n",
    "}\n",
    "# ↑ 学習・推論で一貫して使う「カテゴリ名 → 整数ID」対応。\n",
    "#    ・TokenClassification/BIO 系では num_labels ≥ 最大ID+1 が必要。\n",
    "#    ・この対応はメタデータとして保存（JSON 等）し、学習・推論で共有すること。\n",
    "\n",
    "# カテゴリーをラベルに変更、文字列の正規化する。\n",
    "for sample in dataset:\n",
    "    sample[\"text\"] = unicodedata.normalize(\"NFKC\", sample[\"text\"])\n",
    "    # ↑ NFKC（互換分解＋合成）で全角/半角・互換文字の表記ゆれを統一。\n",
    "    #    【重要】NFKC は “非可逆” かつ “文字長が変化” することがあるため、\n",
    "    #    もし `entities[*]['span']` が「正規化前の text に基づく」場合は\n",
    "    #    span がズレる＝無効になる点に注意。\n",
    "    #    → 本コードは span を再計算していないため、\n",
    "    #       * データが “すでに正規化済み” である\n",
    "    #       * または 正規化後の text を基準に span が定義されている\n",
    "    #      ことが前提。そうでない場合は offset を再構築する前処理が必要（TODO）。\n",
    "\n",
    "    for e in sample[\"entities\"]:\n",
    "        e[\"type_id\"] = type_id_dict[e[\"type\"]]\n",
    "        del e[\"type\"]\n",
    "        # ↑ 学習で扱うのは数値ラベル（type_id）。人可読名（type）は破棄。\n",
    "        #    推論結果の可視化用に name/type 名称が必要なら、別の辞書を持つか\n",
    "        #    id2type で復号できるようにしておくこと。\n",
    "\n",
    "# データセットの分割\n",
    "random.shuffle(dataset)\n",
    "# ↑ シャッフルしてから split。再現性を担保したい場合は予め random.seed(固定値) を設定する。\n",
    "n = len(dataset)\n",
    "n_train = int(n * 0.6)\n",
    "n_val = int(n * 0.2)\n",
    "dataset_train = dataset[:n_train]\n",
    "dataset_val = dataset[n_train : n_train + n_val]\n",
    "dataset_test = dataset[n_train + n_val :]\n",
    "# ↑ 単純比率 split（60/20/20）。\n",
    "#    ・ラベル分布が偏る可能性があるため、実務では層化分割（stratified split）を推奨。\n",
    "#    ・文長分布やエンティティ有無の偏りも評価指標に影響するため、分割前に統計確認を行うと良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ad7eafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 8-15（説明コメント付き：NER データのエンコード→DataLoader 準備）\n",
    "# =============================================================================\n",
    "# 目的：\n",
    "#  - 文字スパン付き NER データ（sample = {'text': str, 'entities': [{'span':[s,e], 'type_id':k}, ...] }）を\n",
    "#    学習時に直接使える形（BERT 入力辞書＋トークンラベル）へ変換し、DataLoader を構築する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - encode_plus_tagged（8-5で定義）は、原文をエンティティ境界で分割 → WordPiece 化 →\n",
    "#    片ラベル（O=0 / type_id>0）を各トークンへ複写 → [CLS]/[SEP]/PAD を付与して長さ max_length に整形する。\n",
    "#  - Token Classification の学習では **CrossEntropyLoss** を用いるため、labels は **クラスID（LongTensor）** を渡すのが定石。\n",
    "#  - [CLS]/[SEP]/PAD 等の位置は損失から除外するのが一般的（ignore_index=-100）。この関数では 0=O のまま作るので、\n",
    "#    学習ループ側でマスクに置き換える（または DataCollator で処理）と良い。\n",
    "#  - max_length を越える部分は切り捨てられるため、右端でエンティティが途切れる場合がある。実務では\n",
    "#    スライディングウィンドウや長めの max_length を検討する。\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def create_dataset(tokenizer, dataset, max_length):\n",
    "    \"\"\"\n",
    "    データセットをデータローダに入力できる形に整形。\n",
    "\n",
    "    引数:\n",
    "      tokenizer : NER_tokenizer\n",
    "        8-5 で定義した encode_plus_tagged を持つトークナイザ（語彙は学習・推論で固定）。\n",
    "      dataset   : List[Dict]\n",
    "        [{'text': str, 'entities': [{'span':[s,e], 'type_id':k}, ...]}, ...]\n",
    "      max_length: int\n",
    "        BERT 入力系列の上限（[CLS]/[SEP] を含む長さに正規化）。\n",
    "\n",
    "    戻り値:\n",
    "      dataset_for_loader : List[Dict[str, torch.Tensor]]\n",
    "        input_ids/attention_mask/token_type_ids/labels を LongTensor で持つ辞書のリスト。\n",
    "        DataLoader で自動的に key ごとにスタックされミニバッチになる。\n",
    "    \"\"\"\n",
    "    dataset_for_loader = []\n",
    "    for sample in dataset:\n",
    "        text = sample[\"text\"]\n",
    "        entities = sample[\"entities\"]\n",
    "\n",
    "        # 文字スパン→トークンラベル写像（O=0 / type_id>0）。[CLS]/[SEP]/PAD も付与され長さを揃える。\n",
    "        encoding = tokenizer.encode_plus_tagged(text, entities, max_length=max_length)\n",
    "\n",
    "        # HF の TokenClassification は labels を「整数クラスID（Long）」で受けるのが定石。\n",
    "        # attention_mask/input_ids/token_type_ids も Long で問題ない（CE計算は labels の型が重要）。\n",
    "        # ※ 特殊トークン/PAD を損失から除外したい場合は、学習ループ側で -100 に置換すること。\n",
    "        encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "\n",
    "        dataset_for_loader.append(encoding)\n",
    "\n",
    "    return dataset_for_loader\n",
    "\n",
    "\n",
    "# トークナイザのロード\n",
    "# - 事前学習モデルの語彙・前処理（MeCab + WordPiece）と一致させること。\n",
    "# - MODEL_NAME は学習・推論を通じて固定し、再現性のためにメタとして保存しておく。\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# データセットの作成\n",
    "# - 学習安定の観点で 128 などの固定長を採用（動的パディングよりスループットが安定しやすい）。\n",
    "# - 文が長く切れやすいコーパスでは 256/384/512 も検討（VRAM とトレードオフ）。\n",
    "max_length = 128\n",
    "dataset_train_for_loader = create_dataset(tokenizer, dataset_train, max_length)\n",
    "dataset_val_for_loader = create_dataset(tokenizer, dataset_val, max_length)\n",
    "\n",
    "# データローダの作成\n",
    "# - 学習は shuffle=True（i.i.d. 近似と汎化のため）。検証/テストは順序維持で十分。\n",
    "# - 実務では num_workers / pin_memory（CUDA時）や DataCollatorForTokenClassification の導入を検討。\n",
    "# - デバイスへの転送は学習ループ側で `batch = {k: v.to(device) for k,v in batch.items()}` とする。\n",
    "dataloader_train = DataLoader(dataset_train_for_loader, batch_size=32, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-16（説明コメント付き：Lightning v2 対応・可搬デバイス・損失マスク（CLS/SEP/PAD 除外））\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from transformers import BertForTokenClassification, AutoTokenizer\n",
    "\n",
    "# =============================================================================\n",
    "# 目的\n",
    "#  - TokenClassification 用の LightningModule を作成し、学習/検証の損失をロギングする。\n",
    "#  - Lightning v2 で非推奨の `gpus=1` を廃止し、`accelerator`/`devices` を用いた可搬設定に修正。\n",
    "#  - 学習の健全性向上のため、損失計算から [CLS]/[SEP]/PAD を除外（ignore_index 相当のマスク）する。\n",
    "#\n",
    "# 背景理論（要点）\n",
    "#  - トークン分類は位置 t ごとに C クラスのロジット z_{t,c} を出力し、CrossEntropyLoss を計算する。\n",
    "#  - 一般に PAD や特殊トークン（CLS/SEP）は学習に寄与させない（HF の定石は labels を -100 にする）。\n",
    "#  - 本実装では、バッチ受領後に labels を複製し、attention_mask==0 と CLS/SEP 位置を -100 に置換してから\n",
    "#    BertForTokenClassification に渡す。（= ignore_index 処理）\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class BertForTokenClassification_pl(pl.LightningModule):\n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        \"\"\"\n",
    "        model_name: 事前学習BERT（例：tohoku-nlp/bert-base-japanese-whole-word-masking）\n",
    "        num_labels: クラス数（O=0 を含む総数）\n",
    "        lr       : 学習率（微調整の目安 1e-5〜5e-5）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # 本体モデル（トークン分類ヘッド付き）\n",
    "        self.bert_tc = BertForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "        )\n",
    "\n",
    "        # CLS/SEP のトークンIDを取得（損失から除外するために使用）\n",
    "        tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.cls_id = tok.cls_token_id\n",
    "        self.sep_id = tok.sep_token_id\n",
    "\n",
    "    # -------------------- 内部ユーティリティ：損失無視マスクを埋める --------------------\n",
    "    def _apply_ignore_index_mask(self, batch, ignore_index=-100):\n",
    "        \"\"\"\n",
    "        CrossEntropyLoss の対象外にしたい位置（PAD/CLS/SEP）を labels 上で ignore_index に置換する。\n",
    "        - PAD は attention_mask==0 で同定。\n",
    "        - CLS/SEP は input_ids の値から同定。\n",
    "        \"\"\"\n",
    "        if \"labels\" not in batch:\n",
    "            return batch  # 推論時など labels が無い場合はそのまま返す\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"].clone()  # 破壊的変更を避けるため複製\n",
    "\n",
    "        # PAD 位置（attention_mask==0）を ignore_index に\n",
    "        labels = labels.masked_fill(attention_mask.eq(0), ignore_index)\n",
    "\n",
    "        # CLS/SEP の位置も ignore_index に\n",
    "        # 例外的に CLS/SEP が存在しないトークナイザ構成は想定外。存在チェックしつつ置換。\n",
    "        if self.cls_id is not None:\n",
    "            cls_pos = input_ids.eq(self.cls_id)\n",
    "            labels = labels.masked_fill(cls_pos, ignore_index)\n",
    "        if self.sep_id is not None:\n",
    "            sep_pos = input_ids.eq(self.sep_id)\n",
    "            labels = labels.masked_fill(sep_pos, ignore_index)\n",
    "\n",
    "        batch = dict(batch)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    # -------------------- 学習/検証フロー --------------------\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # 損失から除外すべき位置を -100 に置換してから forward\n",
    "        batch = self._apply_ignore_index_mask(batch, ignore_index=-100)\n",
    "        output = self.bert_tc(**batch)\n",
    "        loss = output.loss\n",
    "        # エポック平均で可視化（prog_bar=True で進捗バーにも表示）\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch = self._apply_ignore_index_mask(batch, ignore_index=-100)\n",
    "        output = self.bert_tc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # 最小構成：Adam（実務は AdamW + Scheduler を推奨）\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "\n",
    "# -------------------- チェックポイント（val_loss 最小） --------------------\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,  # 版差あり：必要に応じて state_dict 手動保存を検討\n",
    "    dirpath=\"model/\",\n",
    "    filename=\"epoch={epoch}-val_loss={val_loss:.4f}\",\n",
    ")\n",
    "\n",
    "# -------------------- Trainer（Lightning v2 形式に修正：gpus→accelerator/devices） --------------------\n",
    "# 可搬設定：CUDA があれば GPU、Apple Silicon なら MPS、無ければ CPU を自動選択\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=5,\n",
    "    callbacks=[checkpoint],\n",
    "    # deterministic=True,              # 再現性重視（性能とトレードオフ）\n",
    "    # gradient_clip_val=1.0,           # 勾配クリップ（安定化）\n",
    "    # accumulate_grad_batches=2,       # 勾配蓄積（VRAM 節約）\n",
    "    # precision=16,                    # 混合精度（環境に応じて）\n",
    ")\n",
    "\n",
    "# -------------------- ファインチューニング実行 --------------------\n",
    "model = BertForTokenClassification_pl(MODEL_NAME, num_labels=9, lr=1e-5)\n",
    "\n",
    "# DataLoader は 8-15 で作成済み（dataloader_train / dataloader_val）\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "\n",
    "# 最良モデル（val_loss 最小）のファイルパス\n",
    "best_model_path = checkpoint.best_model_path\n",
    "print(f\"[best ckpt] {best_model_path}\")\n",
    "\n",
    "# 参考：ロード例\n",
    "# loaded = BertForTokenClassification_pl.load_from_checkpoint(best_model_path)\n",
    "# loaded.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668ca10d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
