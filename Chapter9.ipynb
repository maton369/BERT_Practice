{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-1\n",
    "!mkdir chap9\n",
    "%cd ./chap9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3601a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-3\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 日本語の事前学習済みモデル\n",
    "MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ecfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-4\n",
    "# =============================================================================\n",
    "# SC_tokenizer（誤変換補正用トークナイザ）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的（理論）：\n",
    "# - 「誤変換を含む文」を入力 x とし、「正しい文」をラベル y（語彙上のトークンID列）として\n",
    "#   学習することで、**各位置ごとに“正しいトークンID”を分類**するタスクへ還元する。\n",
    "# - これは BERT をエンコーダとし、出力ヘッドが **語彙サイズ |V|** のクラス分類を\n",
    "#   各タイムステップに対して行う設計（トークン分類）に相当する。\n",
    "#   * 損失関数の典型：位置 t ごとに CrossEntropyLoss（教師は正解トークンID）。\n",
    "#   * BERT の “Masked LM” と違い、ここでは**全面的に教師強制**で全位置を監督できる（PAD を除外するなら ignore_index を活用）。\n",
    "# - この形式は **置換型の誤り**（打鍵ミス・漢字/かな揺れ・全半角揺れ等）に強いが、\n",
    "#   **挿入/削除**や**語順入れ替え**などの編集距離を伴う変化は「位置合わせ」の前提を崩すため苦手。\n",
    "#   必要に応じてアライメント戦略（例：差分アルゴリズムでの整列→同一長に拡張）やシーケンス生成系（seq2seq）を検討する。\n",
    "# - 推論時は各位置の予測トークンIDから文字列を復号し、元文の空白を保持しつつ**スパン単位で置換**して復元する。\n",
    "#   サブワード（WordPiece）の '##' 接頭辞は接続時に除去する。\n",
    "#\n",
    "# 実装上の注意：\n",
    "# - encode_plus_tagged では、正解文の input_ids をそのまま labels に格納する。\n",
    "#   学習ループ側で特殊トークンや PAD を損失に含めない場合、labels の該当位置を **-100** に置換しておく（ignore_index）。\n",
    "# - encode_plus_untagged では、WordPiece の各サブワードを元文字列の部分スパンにマッピングする。\n",
    "#   繰り返し部分や空白が多い場合、現在の **前方貪欲マッチ**は誤対応のリスクがある（必要なら正規表現や LCS によるロバスト化を検討）。\n",
    "# - convert_bert_output_to_text では NFKC 正規化を適用している。\n",
    "#   学習・推論で正規化を**一貫**させないと、スパンの境界と生成表記の齟齬を招く可能性がある。\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class SC_tokenizer(BertJapaneseTokenizer):\n",
    "\n",
    "    def encode_plus_tagged(self, wrong_text, correct_text, max_length=128):\n",
    "        \"\"\"\n",
    "        ファインチューニング時に使用。\n",
    "        誤変換を含む文章と正しい文章を入力とし、\n",
    "        符号化を行いBERTに入力できる形式にする。\n",
    "\n",
    "        理論メモ：\n",
    "        - 入力 x = wrong_text のトークン列に対し、教師 y = correct_text のトークンID列を\n",
    "          そのまま labels として与えることで、各位置の正解トークンを分類するタスクにする。\n",
    "        - モデル側は「トークン分類ヘッド（出力次元＝語彙サイズ |V|）」を持ち、\n",
    "          CrossEntropyLoss(logits_t, label_id_t) を位置 t で計算し、時系列平均を取るのが一般的。\n",
    "        - PAD/CLS/SEP を損失から除外したい場合、labels の該当位置を -100 にする（ignore_index）。\n",
    "        \"\"\"\n",
    "        # 誤変換した文章をトークン化し、符号化\n",
    "        encoding = self(\n",
    "            wrong_text, max_length=max_length, padding=\"max_length\", truncation=True\n",
    "        )\n",
    "        # 正しい文章をトークン化し、符号化\n",
    "        encoding_correct = self(\n",
    "            correct_text, max_length=max_length, padding=\"max_length\", truncation=True\n",
    "        )\n",
    "        # 正しい文章の符号をラベルとする\n",
    "        # 注意：このままだと [CLS]/[SEP]/[PAD] も学習対象に含まれる。\n",
    "        # もし PAD を損失から除きたいなら、学習側で labels を -100 に置換する処理を追加する。\n",
    "        encoding[\"labels\"] = encoding_correct[\"input_ids\"]\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def encode_plus_untagged(self, text, max_length=None, return_tensors=None):\n",
    "        \"\"\"\n",
    "        文章を符号化し、それぞれのトークンの文章中の位置も特定しておく。\n",
    "\n",
    "        理論メモ：\n",
    "        - 後段の復号（convert_bert_output_to_text）では、各サブワードが\n",
    "          原文テキストのどのスパンに対応するかが必要になる。\n",
    "        - ここでは形態素（MeCab）→サブワード（WordPiece）→原文スパンという対応を\n",
    "          前方探索で求めている。繰り返しや空白が多い場合は曖昧性が残る点に注意。\n",
    "        \"\"\"\n",
    "        # 文章のトークン化を行い、\n",
    "        # それぞれのトークンと文章中の文字列を対応づける。\n",
    "        tokens = []  # トークンを追加していく。\n",
    "        tokens_original = []  # トークンに対応する文章中の文字列を追加していく。\n",
    "        words = self.word_tokenizer.tokenize(text)  # MeCabで単語に分割\n",
    "        for word in words:\n",
    "            # 単語をサブワードに分割\n",
    "            tokens_word = self.subword_tokenizer.tokenize(word)\n",
    "            tokens.extend(tokens_word)\n",
    "            if tokens_word[0] == \"[UNK]\":  # 未知語への対応\n",
    "                tokens_original.append(word)\n",
    "            else:\n",
    "                tokens_original.extend(\n",
    "                    [token.replace(\"##\", \"\") for token in tokens_word]\n",
    "                )\n",
    "\n",
    "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
    "        # アルゴリズム：前方貪欲一致（最短一致ではなく、順次スライド）\n",
    "        # 注意：同一サブ文字列が繰り返される場合に誤対応のリスクがある。\n",
    "        position = 0\n",
    "        spans = []  # トークンの位置を追加していく。\n",
    "        for token in tokens_original:\n",
    "            l = len(token)\n",
    "            while 1:\n",
    "                if token != text[position : position + l]:\n",
    "                    position += 1\n",
    "                else:\n",
    "                    spans.append([position, position + l])\n",
    "                    position += l\n",
    "                    break\n",
    "\n",
    "        # 符号化を行いBERTに入力できる形式にする。\n",
    "        # prepare_for_model が [CLS]/[SEP] の付与と長さ正規化（padding/truncation）を行う。\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\" if max_length else False,\n",
    "            truncation=True if max_length else False,\n",
    "        )\n",
    "        sequence_length = len(encoding[\"input_ids\"])\n",
    "        # 特殊トークン[CLS]に対するダミーのspanを追加（-1 は「実体なし」の番兵）。\n",
    "        spans = [[-1, -1]] + spans[: sequence_length - 2]\n",
    "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
    "        spans = spans + [[-1, -1]] * (sequence_length - len(spans))\n",
    "\n",
    "        # 必要に応じてtorch.Tensorにする。\n",
    "        if return_tensors == \"pt\":\n",
    "            encoding = {k: torch.tensor([v]) for k, v in encoding.items()}\n",
    "\n",
    "        return encoding, spans\n",
    "\n",
    "    def convert_bert_output_to_text(self, text, labels, spans):\n",
    "        \"\"\"\n",
    "        推論時に使用。\n",
    "        文章と、各トークンのラベルの予測値、文章中での位置を入力とする。\n",
    "        そこから、BERTによって予測された文章に変換。\n",
    "\n",
    "        理論メモ：\n",
    "        - labels は「各位置の語彙ID（予測トークン）」と想定。すなわちモデル出力は\n",
    "          各位置で |V| 分類を行うトークン分類ヘッドの argmax 等。\n",
    "        - サブワード '##' は接続時に除去し、文字列として連結。\n",
    "        - 本実装は NFKC 正規化を通すため、**学習・評価で NFKC を統一**しないと\n",
    "          スパン境界と生成表記のずれが起き得る（評価の完全一致が過小になる可能性）。\n",
    "        \"\"\"\n",
    "        assert len(spans) == len(labels)\n",
    "\n",
    "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
    "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
    "        spans = [span for span in spans if span[0] != -1]\n",
    "\n",
    "        # BERTが予測した文章を作成\n",
    "        # 原文の空白や未対称な部分は、スパンの「穴」をそのまま複製して保持。\n",
    "        predicted_text = \"\"\n",
    "        position = 0\n",
    "        for label, span in zip(labels, spans):\n",
    "            start, end = span\n",
    "            if position != start:  # 空白の処理（原文の未対応部分をそのままコピー）\n",
    "                predicted_text += text[position:start]\n",
    "            predicted_token = self.convert_ids_to_tokens(label)\n",
    "            predicted_token = predicted_token.replace(\"##\", \"\")\n",
    "            predicted_token = unicodedata.normalize(\"NFKC\", predicted_token)\n",
    "            predicted_text += predicted_token\n",
    "            position = end\n",
    "\n",
    "        return predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a08e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'SC_tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 9-5\n",
    "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a503cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 759, 18204, 11, 4618, 15, 10, 3, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], 'labels': [2, 759, 18204, 11, 8274, 15, 10, 3, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# 9-6\n",
    "# =============================================================================\n",
    "# 目的（理論）：\n",
    "# - 「誤変換を含む文（wrong_text）」をモデル入力とし、「正しい文（correct_text）」の\n",
    "#   トークンID列を **教師ラベル（labels）** として与える最小例。\n",
    "# - これにより各位置 t で 「正しいトークンID」を分類する **トークン分類問題** として学習できる。\n",
    "#   * 推奨損失：CrossEntropyLoss（語彙サイズ |V| クラスの多クラス分類、PAD 等は ignore_index=-100 で除外）\n",
    "# - 注意：max_length を超えると **切り詰め（truncation）** されるため、wrong と correct の整合が崩れる。\n",
    "#   学習で両系列が等長にパディング・切り詰めされること（同じ max_length/手順）を必ず保証すること。\n",
    "# - 出力の encoding には、wrong_text の BERT 入力（input_ids/attention_mask/token_type_ids）に加えて、\n",
    "#   correct_text の input_ids が **encoding['labels']** として格納される実装（9-4 の SC_tokenizer 参照）。\n",
    "# =============================================================================\n",
    "\n",
    "wrong_text = \"優勝トロフィーを変換した\"  # 誤変換を含む入力（例：返還→変換）\n",
    "correct_text = \"優勝トロフィーを返還した\"  # 正しい表記\n",
    "encoding = tokenizer.encode_plus_tagged(wrong_text, correct_text, max_length=12)\n",
    "print(encoding)\n",
    "# 期待される中身（例）：\n",
    "#  - 'input_ids'       : wrong_text をトークナイズ→ID化→[CLS]/[SEP] 付与→max_length へパディング/切り詰め\n",
    "#  - 'token_type_ids'  : 単文なので通常は全 0\n",
    "#  - 'attention_mask'  : 実トークン 1 / PAD 0\n",
    "#  - 'labels'          : correct_text の input_ids（= 教師）。学習側で PAD/特殊トークンに -100 を適用推奨。\n",
    "# 理論メモ：\n",
    "#  - 本手法は「置換型誤り」には強い一方、挿入/削除や語順入れ替え等の編集距離を伴う誤りには弱い。\n",
    "#    その場合は、アライメント（diff/LCS）で整列して等長化するか、seq2seq 系（エンコーダ–デコーダ）を検討する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0cfd808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# encoding\n",
      "{'input_ids': tensor([[    2,   759, 18204,    11,  4618,    15,    10,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "# spans\n",
      "[[-1, -1], [0, 2], [2, 7], [7, 8], [8, 10], [10, 11], [11, 12], [-1, -1]]\n"
     ]
    }
   ],
   "source": [
    "# 9-7\n",
    "# =============================================================================\n",
    "# 目的（理論）：\n",
    "# - 学習/推論時に、原文テキストの各サブワード（WordPiece）が「元テキスト中のどの位置（span）」に\n",
    "#   対応するかを取得する最小例である。これにより、トークン分類の予測結果（各位置の語彙ID）を\n",
    "#   文字列へ正確に復号（置換）できる。\n",
    "#\n",
    "# 背景：\n",
    "# - encode_plus_untagged は、形態素分割（MeCab）→サブワード分割（WordPiece）の後、\n",
    "#   各サブワードから '##' を除いた素片を原文に前方探索でアラインし、[start, end) の半開区間で\n",
    "#   スパン列（spans）を構築する。\n",
    "# - prepare_for_model により [CLS]/[SEP] の付与とパディング/切り詰めが行われるため、\n",
    "#   それら特殊トークンや PAD に対応する spans は [-1, -1] の番兵を入れておく（「実体なし」の意味）。\n",
    "#\n",
    "# 出力の読み方：\n",
    "# - encoding: dict（'input_ids', 'token_type_ids', 'attention_mask' など）。return_tensors='pt' のため\n",
    "#   各値は形状 [1, T] の torch.Tensor（バッチ次元 1 を含む）。\n",
    "# - spans: 長さ T のリスト。各要素は [start, end]（原文の0始まりインデックス）。\n",
    "#   * special/PAD の位置は [-1, -1]。\n",
    "#   * 先頭は [CLS] のため必ず [-1, -1]、終端側も [SEP] と PAD による [-1, -1] が並ぶ。\n",
    "#\n",
    "# 注意点：\n",
    "# - 前方貪欲一致は、同一部分文字列が繰り返される文や空白が多い文で誤マッチのリスクがある。\n",
    "#   必要に応じて LCS/編集距離によるアライメントでロバスト化を検討。\n",
    "# - 学習・推論で NFKC 正規化ポリシーを統一しないと、スパン境界と生成表記の差異が生じ得る。\n",
    "# =============================================================================\n",
    "\n",
    "wrong_text = \"優勝トロフィーを変換した\"\n",
    "encoding, spans = tokenizer.encode_plus_untagged(wrong_text, return_tensors=\"pt\")\n",
    "print(\"# encoding\")\n",
    "print(encoding)\n",
    "print(\"# spans\")\n",
    "print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0a5d545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "優勝トロフィーを返還した\n"
     ]
    }
   ],
   "source": [
    "# 9-8\n",
    "# =============================================================================\n",
    "# 目的（理論）：\n",
    "# - 9-7 で得た spans（原文中のサブワード位置列）と、各位置の **予測トークンID列（predicted_labels）** を\n",
    "#   用いて、SC_tokenizer.convert_bert_output_to_text により「正しい文章」を復元する最小例。\n",
    "#\n",
    "# 処理の流れ（convert_bert_output_to_text の要点）：\n",
    "#  1) 事前条件：len(spans) == len(labels)（ここで labels=predicted_labels）。\n",
    "#     - spans は [CLS], 本文サブワード, [SEP]（および PAD があれば PAD）に対応し、特殊/PAD は [-1, -1]。\n",
    "#     - ラベル列も **同じ長さ**で、対応位置に語彙ID（token_id）を持つこと。\n",
    "#  2) 特殊/PAD 位置の除去：\n",
    "#     - span[0] == -1 の要素は「実体なし」として、labels/spans の両方から落とす。\n",
    "#  3) サブワード復元：\n",
    "#     - token_id → token へ変換し、WordPiece の接頭辞 '##' を除去して連結。\n",
    "#     - 連結前に **NFKC 正規化**で全/半角・合成文字の揺れを矯正（学習/推論での正規化ポリシーは統一すること）。\n",
    "#  4) 空白・非対象領域の保持：\n",
    "#     - 連結中、原文の position と次のサブワード開始 start にギャップがあれば、原文の該当部分をそのまま複写。\n",
    "#\n",
    "# 理論メモ：\n",
    "# - ここでの labels は「語彙サイズ |V| クラスの **トークン分類** の予測結果（argmax など）」を想定。\n",
    "# - 位置合わせ（spans）は 9-7 の encode_plus_untagged で算出（MeCab→WordPiece→前方探索）。\n",
    "#   反復文字列や空白が多い場合は前方貪欲一致が誤対応するリスクがあるため、大規模実運用では LCS/編集距離による\n",
    "#   アライメントの頑健化を検討。\n",
    "# - 予測列の長さが spans と一致しないと assert により失敗する。モデル出力 → ラベル化の段で\n",
    "#   [CLS]/[SEP]/PAD を含んだ長さ合わせを行うこと。\n",
    "# =============================================================================\n",
    "\n",
    "predicted_labels = [\n",
    "    2,\n",
    "    759,\n",
    "    18204,\n",
    "    11,\n",
    "    8274,\n",
    "    15,\n",
    "    10,\n",
    "    3,\n",
    "]  # 例：各位置の予測 token_id（語彙ID）\n",
    "predicted_text = tokenizer.convert_bert_output_to_text(\n",
    "    wrong_text, predicted_labels, spans\n",
    ")\n",
    "print(predicted_text)\n",
    "# 出力想定：\n",
    "# - モデルが '変換'→'返還' のような誤変換を修正できていれば、期待する正しい表記に復元される。\n",
    "# - 語彙外/未知などで難しい場合は近傍のサブワード列に置換される可能性がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f3c3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] bert_mlm loaded on: mps\n",
      "# input :  優勝トロフィーを変換した。\n",
      "# output:  優勝トロフィーを獲得した。\n"
     ]
    }
   ],
   "source": [
    "# 9-9 / 9-10 の CUDA エラー修正（Mac/MPS・CPU でも動く汎用化）\n",
    "# =============================================================================\n",
    "# エラー原因：\n",
    "# - 環境の PyTorch が「CUDA 非対応」でビルドされているため、`.cuda()` 呼び出しで失敗。\n",
    "#   → Apple Silicon(Mac) は通常 CUDA が無いので、MPS または CPU を使う必要がある。\n",
    "#\n",
    "# 対応方針：\n",
    "# 1) デバイスを自動選択（MPS → CUDA → CPU の順）。\n",
    "# 2) モデル・テンソル移動は `.cuda()` ではなく **`.to(device)`** に統一。\n",
    "# 3) 推論中心なら **`.eval()`** を付けてドロップアウト無効化。\n",
    "# 4) 9-10 の `encoding` も `.to(device)` に変更。\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "# --- 元のコード（参考：この2行が CUDA 非対応環境で落ちる） ---\n",
    "# bert_mlm = BertForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "# bert_mlm = bert_mlm.cuda()\n",
    "\n",
    "\n",
    "# --- 修正版：可搬なデバイス自動選択 ---\n",
    "def pick_device():\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")  # Apple Silicon (Metal)\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")  # NVIDIA CUDA\n",
    "    return torch.device(\"cpu\")  # フォールバック\n",
    "\n",
    "\n",
    "device = pick_device()\n",
    "bert_mlm = BertForMaskedLM.from_pretrained(MODEL_NAME).to(device).eval()\n",
    "print(f\"[info] bert_mlm loaded on: {device}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 9-10（修正版）：encoding も `.to(device)` に統一\n",
    "#  - 注意：MLM は本来 [MASK] 位置のみを予測するタスク。全位置 argmax は自己再現に寄りやすい。\n",
    "#    実運用では、怪しい部分を [MASK] 化して top-k から候補を選ぶ方が安定。\n",
    "# =============================================================================\n",
    "\n",
    "# 前段で用意済みの tokenizer / SC_tokenizer とする\n",
    "text = \"優勝トロフィーを変換した。\"\n",
    "\n",
    "# spans（原文中のサブワード位置）も同時取得\n",
    "encoding, spans = tokenizer.encode_plus_untagged(text, return_tensors=\"pt\")\n",
    "encoding = {\n",
    "    k: v.to(device) for k, v in encoding.items()\n",
    "}  # ← ここを .cuda() ではなく .to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = bert_mlm(**encoding)  # logits 形状：[B=1, T, |V|]\n",
    "    scores = output.logits\n",
    "    labels_predicted = scores[0].argmax(-1).to(\"cpu\").numpy().tolist()\n",
    "\n",
    "# 予測トークン列 → 文字列復元（special/PAD は内部で除外、'##' は除去、NFKC で正規化）\n",
    "predict_text = tokenizer.convert_bert_output_to_text(text, labels_predicted, spans)\n",
    "\n",
    "print(\"# input : \", text)\n",
    "print(\"# output: \", predict_text)\n",
    "\n",
    "# =============================================================================\n",
    "# メモ（理論）：\n",
    "# - 学習側（9-4）の encode_plus_tagged は「正解文の input_ids を labels に格納」→\n",
    "#   各位置の多クラス CE 損失で学習できる（PAD/特殊は ignore_index=-100 を推奨）。\n",
    "# - 推論側（本コード）は “全位置 argmax” のため、真の誤り訂正というより自己再現に寄る。\n",
    "#   改善案：疑わしい位置のみ [MASK]、top-k 候補をビーム探索 or 言語的制約で選ぶ。\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86a0b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.7928\n"
     ]
    }
   ],
   "source": [
    "# 9-11（説明コメント付き：MLM で「正解文トークン列」を教師にした最小学習ループ）\n",
    "# =============================================================================\n",
    "# 目的（理論）：\n",
    "# - 誤変換文 wrong_text を入力 x とし、正しい文 correct_text の input_ids を教師 y（各位置の正解トークンID）として\n",
    "#   BertForMaskedLM による **トークンごとの多クラス分類（語彙サイズ |V|）** で学習する最小例。\n",
    "# - CrossEntropyLoss は「各位置 t のロジット z_{t,*} と教師 label_{t}（語彙ID）」で計算。\n",
    "#   ただし **[CLS]/[SEP]/[PAD] など損失に含めたくない位置は -100（ignore_index）** にするのが定石。\n",
    "# - 注意：MLM は本来 [MASK] 位置のみの復元で事前学習されているため、全位置を一様に監督すると\n",
    "#   「自己再現」に寄りやすい。実務では（1）誤り疑い位置だけを [MASK] 化、（2）候補 top-k から置換判定、\n",
    "#   などを併用すると安定。\n",
    "# =============================================================================\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"wrong_text\": \"優勝トロフィーを変換した。\",\n",
    "        \"correct_text\": \"優勝トロフィーを返還した。\",\n",
    "    },\n",
    "    {\n",
    "        \"wrong_text\": \"人と森は強制している。\",\n",
    "        \"correct_text\": \"人と森は共生している。\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# 各データを符号化し、データローダへ入力できる形に整形\n",
    "# - encode_plus_tagged（9-4 で実装済み想定）は：\n",
    "#     input_ids/attention_mask/token_type_ids（= wrong_text 側）に加え、\n",
    "#     labels として correct_text の input_ids を格納する。\n",
    "# - max_length は wrong/correct 双方で同じ長さになる（prepare_for_modelの挙動）。\n",
    "max_length = 32\n",
    "dataset_for_loader = []\n",
    "for sample in data:\n",
    "    wrong_text = sample[\"wrong_text\"]\n",
    "    correct_text = sample[\"correct_text\"]\n",
    "    encoding = tokenizer.encode_plus_tagged(\n",
    "        wrong_text, correct_text, max_length=max_length\n",
    "    )\n",
    "    # Transformers の損失は long のラベル ID を想定（-100 も long）\n",
    "    encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "    dataset_for_loader.append(encoding)\n",
    "\n",
    "# データローダを作成（小サンプルなので 1 バッチ）\n",
    "dataloader = DataLoader(dataset_for_loader, batch_size=2)\n",
    "\n",
    "# --- デバイス選択：bert_mlm の実デバイスに合わせる（.cuda() は使わない） ---\n",
    "device = next(bert_mlm.parameters()).device  # 例：mps / cuda / cpu\n",
    "\n",
    "# トークナイザから特殊トークンIDを取得（BERT系では多くが [PAD]=0, [CLS]=101, [SEP]=102）\n",
    "pad_id = tokenizer.pad_token_id\n",
    "cls_id = tokenizer.cls_token_id\n",
    "sep_id = tokenizer.sep_token_id\n",
    "\n",
    "# ミニバッチを BERT へ入力し、損失を計算\n",
    "# - ラベルをそのまま渡すと特殊/PAD まで学習対象になるため、ignore_index=-100 に置換する\n",
    "bert_mlm.train()  # 学習想定（推論なら .eval() と no_grad を使う）\n",
    "for batch in dataloader:\n",
    "    # デバイスへ移動\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    # --- ignore_index マスクの構築（損失に含めない位置を -100 にする）---\n",
    "    # 1) PAD 位置（attention_mask==0）を除外\n",
    "    labels = batch[\"labels\"].clone()\n",
    "    amask = batch[\"attention_mask\"]  # 1=実トークン, 0=PAD\n",
    "    labels[amask == 0] = -100\n",
    "\n",
    "    # 2) [CLS]/[SEP] も除外（多くのケースで学習対象外にするのが定石）\n",
    "    inp = batch[\"input_ids\"]\n",
    "    labels[inp == cls_id] = -100\n",
    "    labels[inp == sep_id] = -100\n",
    "\n",
    "    # BertForMaskedLM は labels を与えると内部で CrossEntropyLoss を計算して output.loss を返す\n",
    "    # （ignore_index は -100 が既定）\n",
    "    # token_type_ids が無い場合は自動で None 扱いだが、あるなら一緒に渡す\n",
    "    output = bert_mlm(\n",
    "        input_ids=inp,\n",
    "        attention_mask=amask,\n",
    "        token_type_ids=batch.get(\"token_type_ids\", None),\n",
    "        labels=labels,\n",
    "    )\n",
    "    loss = output.loss  # 損失（バッチ×時系列の平均）\n",
    "\n",
    "    # 例：逆伝播（最小例。実運用では optimizer/scheduler/AMP/勾配クリップ等を併用）\n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    print(f\"loss: {loss.item():.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 追加メモ（理論と実装の橋渡し）：\n",
    "# - 「正解文の input_ids を labels にする」設計は **トークン置換** には有効。\n",
    "#   ただし挿入/削除や語順入れ替えが頻出する場合、位置合わせの仮定が崩れるため seq2seq/CTC などを検討。\n",
    "# - 本設計を“MLMらしく”するなら、wrong_text のうち置換したい部分だけを [MASK] 化し、\n",
    "#   labels も該当位置以外は -100 にして監督する手もある（局所最適化&副作用低減）。\n",
    "# - 実学習では AdamW + weight decay、学習率 1e-5〜5e-5、warmup/linear スケジューラ、\n",
    "#   勾配クリップ、mixed precision 等を併用すると安定。\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a22d8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   244  100   244    0     0    168      0  0:00:01  0:00:01 --:--:--   168\n",
      "100 64.9M  100 64.9M    0     0  1772k      0  0:00:37  0:00:37 --:--:-- 2082k\n",
      "x jwtd/\n",
      "x jwtd/train.jsonl\n",
      "x jwtd/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 9-12\n",
    "!curl -L \"https://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=https://nlp.ist.i.kyoto-u.ac.jp/nl-resource/JWTD/jwtd.tar.gz&name=JWTD.tar.gz\" -o JWTD.tar.gz\n",
    "!tar zxvf JWTD.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd49ef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習と検証用のデータセット：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'SC_tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 漢字誤変換の総数：235490\n",
      "- トークンの対応関係のつく文章の総数: 173992\n",
      "  (全体の74%)\n",
      "テスト用のデータセット：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'SC_tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 漢字誤変換の総数：3061\n",
      "- トークンの対応関係のつく文章の総数: 2289\n",
      "  (全体の75%)\n"
     ]
    }
   ],
   "source": [
    "# 9-4（説明コメント付き）：誤変換訂正タスク用データセット作成（トークン対応制約あり）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的：\n",
    "# - JWTD（日本語誤変換データ）から「漢字変換」に関するサンプルのみを抽出し、\n",
    "#   BERT 推論/学習に適した形式（wrong_text, correct_text のペア配列）へ整形する。\n",
    "# - SC_tokenizer（BertJapaneseTokenizer系）でトークン列を比較し、\n",
    "#   「トークン数が一致」かつ「差分トークン数が閾値以下」のものだけを採用して\n",
    "#   “位置合わせ” を単純化する（= token-level 監督が可能になる）。\n",
    "#\n",
    "# 理論メモ：\n",
    "# - 本手法は “置換型誤り” を前提とするため、挿入/削除を伴うケースは除外する設計になっている。\n",
    "#   （トークン数一致の制約）→ 位置 t ごとに語彙多クラス分類として監督できる。\n",
    "# - 事前に NFKC 正規化（互換分解＋合成）を行い、全/半角や互換文字の揺れを吸収して\n",
    "#   トークン化の安定性・比較の一貫性を高めている。\n",
    "# - Pandas の `query(..., inplace=True)` / `rename(..., inplace=True)` は\n",
    "#   引数 DataFrame を破壊的に更新する点に注意（呼び出し側とデータ再利用の設計に影響）。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def create_dataset(data_df):\n",
    "\n",
    "    tokenizer = SC_tokenizer.from_pretrained(\n",
    "        MODEL_NAME\n",
    "    )  # SC_tokenizer は 9-4 で定義済み想定\n",
    "    # BertJapaneseTokenizer ベースの tokenize() を使用\n",
    "\n",
    "    def check_token_count(row):\n",
    "        \"\"\"\n",
    "        誤変換の文章と正しい文章でトークンに対応がつくかどうかを判定。\n",
    "        （条件は上の文章を参照）\n",
    "\n",
    "        判定ロジックの要点：\n",
    "        - WordPiece トークン列の長さが一致（= 挿入/削除なし）。\n",
    "        - 差分トークン数 <= 閾値（“局所的な置換” のみを許容）。\n",
    "          ※ 閾値（threthold_count）は実験設計上のハイパーパラメータ。\n",
    "        \"\"\"\n",
    "        wrong_text_tokens = tokenizer.tokenize(row[\"wrong_text\"])\n",
    "        correct_text_tokens = tokenizer.tokenize(row[\"correct_text\"])\n",
    "\n",
    "        # 1) トークン数一致：位置合わせ（t 対応）を保証するための必須条件\n",
    "        if len(wrong_text_tokens) != len(correct_text_tokens):\n",
    "            return False\n",
    "\n",
    "        # 2) 差分が“少数”であることを要求（局所置換の想定）\n",
    "        diff_count = 0\n",
    "        threthold_count = (\n",
    "            2  # NOTE: 'threshold' の綴りミスだが、機能上は問題なし（修正は任意）\n",
    "        )\n",
    "        for wrong_text_token, correct_text_token in zip(\n",
    "            wrong_text_tokens, correct_text_tokens\n",
    "        ):\n",
    "\n",
    "            if wrong_text_token != correct_text_token:\n",
    "                diff_count += 1\n",
    "                if diff_count > threthold_count:\n",
    "                    # 許容差分を超えたら除外（挿入/削除に近い複雑な差分や広範な置換を排除）\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def normalize(text):\n",
    "        \"\"\"\n",
    "        文字列の正規化：\n",
    "        - 前後空白の除去（strip）\n",
    "        - NFKC 正規化（互換文字の正規化、全半角統一など）\n",
    "        ※ 学習・推論・評価の全段で同じ正規化を適用することで、一貫性を担保するのが原則。\n",
    "        \"\"\"\n",
    "        text = text.strip()\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "        return text\n",
    "\n",
    "    # --- データ抽出：カテゴリが「漢字誤変換」のものだけを残す（破壊的操作に注意） ---\n",
    "    category_type = \"kanji-conversion\"\n",
    "    data_df.query(\n",
    "        \"category == @category_type\", inplace=True\n",
    "    )  # inplace=True なので data_df が更新される\n",
    "    data_df.rename(\n",
    "        columns={\"pre_text\": \"wrong_text\", \"post_text\": \"correct_text\"}, inplace=True\n",
    "    )\n",
    "\n",
    "    # --- 正規化とフィルタリング ---\n",
    "    # 1) 文字列正規化（NFKC）で表記揺れを抑制\n",
    "    data_df[\"wrong_text\"] = data_df[\"wrong_text\"].map(normalize)\n",
    "    data_df[\"correct_text\"] = data_df[\"correct_text\"].map(normalize)\n",
    "\n",
    "    # 2) 統計出力のためにフィルタ前件数を保持\n",
    "    kanji_conversion_num = len(data_df)\n",
    "\n",
    "    # 3) トークン対応が成立するサンプルのみ抽出（挿入/削除や大規模置換を排除）\n",
    "    data_df = data_df[data_df.apply(check_token_count, axis=1)]\n",
    "    same_tokens_count_num = len(data_df)\n",
    "\n",
    "    # 4) 抽出率のログ（学習データの “素性” を把握して再現性を高める）\n",
    "    print(\n",
    "        f\"- 漢字誤変換の総数：{kanji_conversion_num}\",\n",
    "        f\"- トークンの対応関係のつく文章の総数: {same_tokens_count_num}\",\n",
    "        f\"  (全体の{same_tokens_count_num/kanji_conversion_num*100:.0f}%)\",\n",
    "        sep=\"\\n\",\n",
    "    )\n",
    "\n",
    "    # 返り値：学習・検証・テスト作成で共通利用しやすい、辞書リスト形式（records）\n",
    "    # 例：{'wrong_text': '...', 'correct_text': '...'}\n",
    "    return data_df[[\"wrong_text\", \"correct_text\"]].to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "# --- データのロード（train/test の JSON Lines） ---\n",
    "# 期待スキーマ（例）：\n",
    "#   {\n",
    "#     \"category\": \"kanji-conversion\",\n",
    "#     \"pre_text\": \"...\",   # 誤変換文\n",
    "#     \"post_text\": \"...\"   # 正しい文\n",
    "#     ...\n",
    "#   }\n",
    "# ※ 列名は直後に rename 済み（wrong_text, correct_text）\n",
    "train_df = pd.read_json(\"./jwtd/train.jsonl\", orient=\"records\", lines=True)\n",
    "test_df = pd.read_json(\"./jwtd/test.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "# --- 学習＋検証用データセットの作成 ---\n",
    "print(\"学習と検証用のデータセット：\")\n",
    "dataset = create_dataset(train_df)\n",
    "\n",
    "# シャッフル（※ 乱数 seed 未固定。再現実験では random.seed の固定推奨）\n",
    "random.shuffle(dataset)\n",
    "n = len(dataset)\n",
    "\n",
    "# ホールドアウト分割（8:2）。データ分布に偏りがある場合は層化分割の検討が望ましい。\n",
    "n_train = int(n * 0.8)\n",
    "dataset_train = dataset[:n_train]\n",
    "dataset_val = dataset[n_train:]\n",
    "\n",
    "# --- テスト用データセットの作成 ---\n",
    "print(\"テスト用のデータセット：\")\n",
    "dataset_test = create_dataset(test_df)\n",
    "\n",
    "# 補足（設計上の注意）：\n",
    "# - この段階では “置換型誤り” に限定したデータだけが残るため、\n",
    "#   モデルが学習する分布も置換中心にバイアスされる。\n",
    "#   実運用で挿入/削除・語順の乱れが多い場合、seq2seq/CTC など別定式化の導入を検討。\n",
    "# - Pandas の inplace 操作により、呼び出し元の DataFrame が更新される点に注意。\n",
    "#   他処理と共有するなら .copy() 運用を推奨（ここでは元コードを尊重して未変更）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf37717f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'SC_tokenizer'.\n",
      "100%|██████████| 139193/139193 [00:45<00:00, 3050.79it/s]\n",
      "100%|██████████| 34799/34799 [00:10<00:00, 3391.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# 9-14（説明コメント付き）：SC（誤変換訂正）用データセットを DataLoader 入力形式へ整形\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的：\n",
    "# - wrong_text（誤変換文）と correct_text（正解文）のペアから、\n",
    "#   BERT の「トークン置換」学習に適したサンプル辞書を作成する。\n",
    "# - SC_tokenizer.encode_plus_tagged() は\n",
    "#     input_ids / attention_mask / token_type_ids / labels\n",
    "#   を返す想定。ここで labels は「正解（correct_text）側の input_ids」として監督信号になる。\n",
    "#\n",
    "# 理論メモ（なぜこの形？）：\n",
    "# - 本タスクは「各トークン位置 t で正解語彙 y_t を当てる」多クラス分類（語彙サイズ）として定式化。\n",
    "#   WordPiece 分割の“位置合わせ”が成立している前提（9-13 の前処理で担保）。\n",
    "# - max_length でパディング/切り詰めを固定化して、テンソル形状を一定にし学習を安定化。\n",
    "#   （CE 計算・バッチ化・スループットの観点で有利）\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def create_dataset_for_loader(tokenizer, dataset, max_length):\n",
    "    \"\"\"\n",
    "    データセットをデータローダに入力可能な形式にする。\n",
    "\n",
    "    引数:\n",
    "        tokenizer: SC_tokenizer（BertJapaneseTokenizer 派生）想定。\n",
    "                   encode_plus_tagged(wrong, correct, max_length) を備える。\n",
    "        dataset  : [{'wrong_text': str, 'correct_text': str}, ...] の配列。\n",
    "        max_length: 固定系列長。パディング/切り詰めの基準（BERT 入力長を揃える）。\n",
    "\n",
    "    戻り値:\n",
    "        dataset_for_loader: 各要素が以下の dict（全て torch.Tensor）から成るリスト\n",
    "            {\n",
    "              'input_ids': LongTensor [max_length],\n",
    "              'attention_mask': LongTensor [max_length],\n",
    "              'token_type_ids': LongTensor [max_length],  # 日本語単文では 0 域のみ\n",
    "              'labels': LongTensor [max_length]           # 正解トークン ID 列\n",
    "            }\n",
    "\n",
    "    実装ノート:\n",
    "    - torch.tensor(v) は v が整数配列なら dtype=long（int64）になるため、CE のターゲットとして適合。\n",
    "    - 大規模データではこの関数内で全件テンソル化するとメモリを食う。\n",
    "      → IterableDataset 化や、.map() スタイルの遅延処理で I/O/メモリを節約する設計も検討。\n",
    "    \"\"\"\n",
    "    dataset_for_loader = []\n",
    "    for sample in tqdm(dataset):\n",
    "        wrong_text = sample[\"wrong_text\"]\n",
    "        correct_text = sample[\"correct_text\"]\n",
    "\n",
    "        # SC_tokenizer: 誤文を BERT 入力、正解文の input_ids を labels に格納して返す。\n",
    "        encoding = tokenizer.encode_plus_tagged(\n",
    "            wrong_text, correct_text, max_length=max_length\n",
    "        )\n",
    "\n",
    "        # DataLoader でそのままバッチ化できるよう、各配列を torch.Tensor 化。\n",
    "        # ※ labels は CE 用のクラスインデックス（語彙 ID）。float ではなく long が前提。\n",
    "        encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "\n",
    "        dataset_for_loader.append(encoding)\n",
    "    return dataset_for_loader\n",
    "\n",
    "\n",
    "# トークナイザのロード\n",
    "# - SC_tokenizer は BertJapaneseTokenizer を継承し、追加ユーティリティを提供。\n",
    "# - from_pretrained で “BertJapaneseTokenizer” と型名が出る警告は、\n",
    "#   サブクラス化（SC_tokenizer）ゆえの情報メッセージ。分割挙動を変えていなければ実害は通常なし。\n",
    "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# データセットの作成\n",
    "# - max_length を小さくし過ぎると切り詰め誤差（末尾損失）が増える。\n",
    "#   一方で大き過ぎると計算/メモリが増大。分布（長さヒストグラム）を見て決めるのが実務的。\n",
    "max_length = 32\n",
    "dataset_train_for_loader = create_dataset_for_loader(\n",
    "    tokenizer, dataset_train, max_length\n",
    ")\n",
    "dataset_val_for_loader = create_dataset_for_loader(tokenizer, dataset_val, max_length)\n",
    "\n",
    "# データローダの作成\n",
    "# - 学習側のみ shuffle=True（確率的最適化の性質上、汎化が向上しやすい）。\n",
    "# - 速度最適化（必要に応じて）: num_workers / pin_memory / persistent_workers を環境に合わせて調整。\n",
    "dataloader_train = DataLoader(dataset_train_for_loader, batch_size=32, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec33aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-15（説明コメント付き・Lightning 2.x/Mac対応版）\n",
    "# ======================================================================================\n",
    "# 目的（タスクの定式化）：\n",
    "# - 事前学習済み日本語BERTの MLM（Masked Language Modeling）ヘッドを用いて、\n",
    "#   「各トークン位置 t における語彙分類（|V| クラス）」として誤変換訂正を学習する最小構成。\n",
    "#   本シリーズでは、wrong_text を BERT 入力、correct_text の input_ids を labels に入れることで、\n",
    "#   “全位置での語彙分類” として CE（交差エントロピー）を計算する設計を採用している。\n",
    "#   ※ MLM 本来は [MASK] 位置のみ監督だが、訂正タスクに合わせて監督範囲を拡張しても学習は可能。\n",
    "#\n",
    "# 理論メモ（損失の中身）：\n",
    "# - 出力 logits の形状は [B, T, |V|]。labels は [B, T] の語彙ID列で、無視したい位置は -100 を指定。\n",
    "#   CrossEntropyLoss は各位置 t の CE をとり、位置方向に平均（labels==-100 はスキップ）。\n",
    "#   → 監督する位置（labels!=-100）をどう選ぶかが、学習目標を規定する（全位置監督 / MASK位置のみ監督）。\n",
    "#\n",
    "# 実装メモ（環境可搬性）：\n",
    "# - PyTorch Lightning 2.x では `gpus=1` は非推奨/無効になり得るため、`accelerator` と `devices` を使用。\n",
    "# - MacBook(Apple Silicon) → MPS、CUDA 環境 → GPU、なければ CPU を自動選択するには `accelerator='auto'` が簡便。\n",
    "# - 大規模化時は AdamW + weight decay、スケジューラ（linear/cosine + warmup）導入を推奨（ここでは最小構成）。\n",
    "# ======================================================================================\n",
    "\n",
    "\n",
    "class BertForMaskedLM_pl(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, lr):\n",
    "        \"\"\"\n",
    "        model_name : 事前学習BERT（例：'tohoku-nlp/bert-base-japanese-whole-word-masking'）\n",
    "        lr         : 学習率（BERT微調整の一般的レンジは 1e-5〜5e-5）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # ハイパーパラメータを保存（チェックポイントへ含まれ、再現性と再ロード性が向上）\n",
    "        self.save_hyperparameters()\n",
    "        # 事前学習済みの MaskedLM モデル（BERT本体 + 語彙分類ヘッド）\n",
    "        self.bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        入力:\n",
    "          batch: {\n",
    "            'input_ids'      : [B, T],\n",
    "            'attention_mask' : [B, T],\n",
    "            'token_type_ids' : [B, T]（単文なら 0 域のみ）,\n",
    "            'labels'         : [B, T]（正解語彙ID列。無視したい位置は -100）\n",
    "          }\n",
    "        出力:\n",
    "          CE（語彙分類）を位置方向に平均した損失（Lightning が逆伝播を実行）\n",
    "        \"\"\"\n",
    "        output = self.bert_mlm(**batch)\n",
    "        loss = output.loss\n",
    "        # エポック平均でログ化（進捗バーにも表示）\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        検証損失。ModelCheckpoint/EarlyStopping の監視対象（monitor='val_loss'）として利用。\n",
    "        \"\"\"\n",
    "        output = self.bert_mlm(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        最小構成として Adam を使用。\n",
    "        実務では AdamW + weight_decay + Scheduler（linear/cosine + warmup）を推奨。\n",
    "        \"\"\"\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "\n",
    "# ----------------------------- チェックポイント設定 -----------------------------\n",
    "# val_loss の最良（最小）時のみ ckpt を保存（ベスト1件）\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,  # 版差で無視される場合あり。必要なら state_dict を別途保存も検討。\n",
    "    dirpath=\"model/\",\n",
    "    # filename='epoch={epoch}-val_loss={val_loss:.4f}'  # メタ付き命名は再現性に有益（任意）\n",
    ")\n",
    "\n",
    "# 参考：過学習抑止のための EarlyStopping（必要に応じて有効化）\n",
    "# early = pl.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=2)\n",
    "\n",
    "# ----------------------------- Trainer（Lightning 2.x 準拠） -----------------------------\n",
    "# Mac(MPS)/CUDA/CPU を自動判定する可搬設定。`gpus=1` は 2.x で非推奨のため置換。\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",  # 'auto' / 'cpu' / 'gpu' / 'mps'\n",
    "    devices=1,  # 単一デバイス\n",
    "    max_epochs=5,\n",
    "    callbacks=[checkpoint],\n",
    "    # callbacks=[checkpoint, early]  # EarlyStopping を使う場合\n",
    ")\n",
    "\n",
    "# ----------------------------- ファインチューニング実行 -----------------------------\n",
    "# 前段（9-14）で用意した dataloader_train / dataloader_val をそのまま利用。\n",
    "# 重要：labels の与え方\n",
    "#   - 全位置監督：correct_text の input_ids を labels に（現状）。\n",
    "#   - MLM 的監督：当てたい位置のみ labels を語彙ID、他は -100（DataCollator で実装しやすい）。\n",
    "model = BertForMaskedLM_pl(MODEL_NAME, lr=1e-5)\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "\n",
    "# 検証損失が最良のチェックポイントのパス\n",
    "best_model_path = checkpoint.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-16（説明コメント付き：MLM による“全位置”補完の最小推論例）\n",
    "# ====================================================================================\n",
    "# 目的：\n",
    "# - 文章 text を BERT（MaskedLM ヘッド）へ入力し，各トークン位置 t について語彙分布 p(v|context_t)\n",
    "#   の argmax を取り，予測トークン列を得る。その後，spans（原文上の文字スパン）に基づき文字列へ復元する。\n",
    "#\n",
    "# 理論メモ：\n",
    "# - Masked Language Modeling は本来 [MASK] に置いた位置のみを予測対象として学習されている。\n",
    "#   本コードのように “全位置で argmax” を行うと，自己再現（copy）に偏りやすく，誤り訂正の力は限定的になりうる。\n",
    "#   実用では「怪しい位置だけ MASK 化して再予測」や「対数尤度差に基づく置換判定」を併用すると良い。\n",
    "# - encode_plus_untagged は MeCab → WordPiece の流れでサブワード列を作りつつ，\n",
    "#   各サブワードの原文内スパン（start, end）を求める。CLS/SEP/PAD 位置のダミーは [-1, -1] 。\n",
    "# - convert_bert_output_to_text は spans の [-1, -1] を除外し，WordPiece の '##' を除去，\n",
    "#   NFKC 正規化で連結して復元する。\n",
    "# - argmax は softmax を通さなくても同じ ID を返す（単調変換）。確率値が必要なら softmax を明示。\n",
    "# ====================================================================================\n",
    "\n",
    "\n",
    "def predict(text, tokenizer, bert_mlm):\n",
    "    \"\"\"\n",
    "    文章を入力として受け、BERTが予測した文章を出力\n",
    "    \"\"\"\n",
    "    # --- 1) 符号化（サブワード列と原文スパンを同時に取得） ------------------------\n",
    "    # returns:\n",
    "    #   encoding: {input_ids, token_type_ids, attention_mask} いずれも [1, T]\n",
    "    #   spans   : 長さ T のリスト。CLS/SEP/PAD は [-1,-1] の番兵。\n",
    "    encoding, spans = tokenizer.encode_plus_untagged(text, return_tensors=\"pt\")\n",
    "    # --- デバイス注意 ---------------------------------------------------------------\n",
    "    # 元コードは .cuda() 前提。Mac(MPS) や CPU 環境では失敗する。\n",
    "    # 実運用では以下のようにモデル実デバイスへ合わせるのが安全：\n",
    "    #   device = next(bert_mlm.parameters()).device\n",
    "    #   encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "    encoding = {k: v.cuda() for k, v in encoding.items()}\n",
    "\n",
    "    # --- 2) 推論（勾配停止） --------------------------------------------------------\n",
    "    # scores 形状：[1, T, |V|]。各位置 t の語彙ロジット分布。\n",
    "    # argmax(-1) で各位置の「最尤トークンID」を取得（softmax 不要：順位は保存される）。\n",
    "    with torch.no_grad():\n",
    "        output = bert_mlm(**encoding)\n",
    "        scores = output.logits\n",
    "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
    "\n",
    "    # --- 3) 復元（予測 ID 列 → 文字列） --------------------------------------------\n",
    "    # convert_bert_output_to_text:\n",
    "    #   - spans が [-1,-1] の特殊/PAD を除外\n",
    "    #   - '##' 除去 + NFKC 正規化で人間可読な文字列へ\n",
    "    predict_text = tokenizer.convert_bert_output_to_text(text, labels_predicted, spans)\n",
    "\n",
    "    return predict_text\n",
    "\n",
    "\n",
    "# いくつかの例に対してBERTによる文章校正を行ってみる。\n",
    "# - 各例には「誤用/同音異義の可能性」が含まれている（例：史料/資料, 発行/発酵/発言 など）。\n",
    "# - 本コードは“全位置 argmax”なので，自然にそのままを返すことも多い点に注意。\n",
    "text_list = [\n",
    "    \"ユーザーの試行に合わせた楽曲を配信する。\",\n",
    "    \"メールに明日の会議の史料を添付した。\",\n",
    "    \"乳酸菌で牛乳を発行するとヨーグルトができる。\",\n",
    "    \"突然、子供が帰省を発した。\",\n",
    "]\n",
    "\n",
    "# --- Tokenizer / モデルのロード ----------------------------------------------------\n",
    "# SC_tokenizer は BertJapaneseTokenizer を継承し，\n",
    "#   - encode_plus_tagged（学習時：wrong→入力, correct→labels）\n",
    "#   - encode_plus_untagged（推論時：spans 同時生成）\n",
    "# などの補助を追加した拡張版。from_pretrained で語彙や分かち書き設定を継承する。\n",
    "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# LightningModule から ckpt をロードし，内部の bert_mlm（BertForMaskedLM）を取得。\n",
    "# 注意：推論時は dropout 無効化のため eval() を推奨（ただし元コードは変更しない方針なのでコメントのみ）\n",
    "model = BertForMaskedLM_pl.load_from_checkpoint(best_model_path)\n",
    "bert_mlm = model.bert_mlm.cuda()\n",
    "# 推奨（コメント）：bert_mlm.eval()\n",
    "\n",
    "# --- 推論ループ --------------------------------------------------------------------\n",
    "# 実用上は「怪しい位置だけ MASK 化して top-k 代替語を探索」「置換判定に閾値や文法制約を併用」など\n",
    "# の戦略が有効。ここでは最小例としてそのまま predict（全位置 argmax）を実施。\n",
    "for text in text_list:\n",
    "    predict_text = predict(text, tokenizer, bert_mlm)  # BERTによる予測\n",
    "    print(\"---\")\n",
    "    print(f\"入力：{text}\")\n",
    "    print(f\"出力：{predict_text}\")\n",
    "\n",
    "# 参考（改善アイデアの設計メモ）：\n",
    "# - “部分監督/部分置換”：トークン列差分で候補位置のみ labels を与える（学習時），推論時も候補位置のみ置換。\n",
    "# - “尤度差による置換判定”：元トークンの logP と置換候補の logP の差が閾値以上のときのみ置換。\n",
    "# - “言語的制約”：品詞タグや文字種の一致（漢字/仮名/記号）などを満たす候補に限定。\n",
    "# - “ビーム探索”：連続誤りに強い（ただし計算量↑）。局所貪欲より文全体整合が取りやすい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9644e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-17（説明コメント付き：MLM ベース文章訂正の厳密一致 Accuracy 計測）\n",
    "# ====================================================================================\n",
    "# 目的：\n",
    "# - テスト集合（dataset_test）について、MLM ベースの訂正関数 predict() の出力テキストと\n",
    "#   正解テキスト（correct_text）を **完全一致**（厳密一致）で比較し、Accuracy を算出する。\n",
    "#\n",
    "# 評価指標（ここでの Accuracy の定義）：\n",
    "# - サンプル単位の「出力テキスト == 正解テキスト」で 1 点、そうでなければ 0 点。\n",
    "#   すなわち **subset accuracy（完全一致率）** に相当する非常に厳しい指標。\n",
    "#   1 文字でも異なれば不正解になるため、訂正箇所が局所的でも全体が一致しないと 0 点。\n",
    "#\n",
    "# 効果と限界（理論的背景）：\n",
    "# - Masked Language Modeling（MLM）は本来 [MASK] 位置の復元を学習しており、全位置 argmax だと\n",
    "#   「入力の自己再現」に流れやすい。したがって predict() の戦略次第で Accuracy は大きく変わる。\n",
    "# - 厳密一致は “最終目標”（完全修復）を素直に測れる反面、**改善の度合い**を捉えにくい。\n",
    "#   例）1 箇所直っても別の位置で僅かな差（空白・記号）があると 0 点になる。\n",
    "#\n",
    "# 実運用向けの補助指標（提案：実装はここでは行わない）\n",
    "# - 文字/サブワードレベルの編集距離（相対距離 1 - ED/len）\n",
    "# - 訂正検出の Precision/Recall（どの位置を置換したかが検出課題）\n",
    "# - 訂正生成のトークン精度（差分位置のみの token-level accuracy）\n",
    "# - 正規化後比較：NFKC、空白・全角/半角・句読点の統一等\n",
    "# - 出力の尤度・Perplexity、対数尤度差（置換前後）の統計\n",
    "# ====================================================================================\n",
    "\n",
    "# BERTで予測を行い、正解数を数える。\n",
    "correct_num = 0  # 厳密一致で正解になったサンプル数（分子）\n",
    "\n",
    "for sample in tqdm(dataset_test):\n",
    "    wrong_text = sample[\"wrong_text\"]  # 入力となる誤用（誤変換）を含む文\n",
    "    correct_text = sample[\"correct_text\"]  # 教師となる正しい文（完全一致の基準）\n",
    "\n",
    "    # predict(): 9-16 で定義。全位置 argmax による復元（=自己再現に寄りやすい設計）\n",
    "    # 実務では「怪しい位置のみ MASK → top-k 候補から選択」などの戦略で訂正能力を高めるのが一般的。\n",
    "    predict_text = predict(wrong_text, tokenizer, bert_mlm)  # BERT予測（テキスト）\n",
    "\n",
    "    # ※ 完全一致判定。1 文字でも異なれば不正解。\n",
    "    #    必要に応じて NFKC 正規化・空白/句読点の正規化を両者に適用してから比較するとロバストになる。\n",
    "    if correct_text == predict_text:  # 正解の場合（完全一致）\n",
    "        correct_num += 1\n",
    "\n",
    "# Accuracy（完全一致率）を出力。\n",
    "# - 値域は [0,1]。.2f で少数第 2 位まで表示。\n",
    "# - この数値が低い場合でも、部分的な改善が起きている可能性はある（上記の補助指標で要分析）。\n",
    "print(f\"Accuracy: {correct_num/len(dataset_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e8cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-18（説明コメント付き：誤変換位置“検出”の厳密判定・文単位 Accuracy）\n",
    "# ======================================================================\n",
    "# 目的：\n",
    "# - 予測テキストそのものの完全一致ではなく、「どのトークンが誤りか」を **検出** できたかを評価する。\n",
    "# - 評価規則（文単位の厳密判定）：\n",
    "#     1) 元の誤文 wrong と正解文 correct を同じトークナイザでトークン化（[CLS]/[SEP] は除外）。\n",
    "#     2) 各位置 i で wrong_i と correct_i を比較し、「正しい位置」は（wrong_i == correct_i）、\n",
    "#        「誤り位置」は（wrong_i != correct_i）と定義。\n",
    "#     3) 予測 predict_i が\n",
    "#        - 正しい位置では **変更しない**（predict_i == wrong_i）\n",
    "#        - 誤り位置では **変更する**（predict_i != wrong_i）\n",
    "#        を **すべての位置で満たした** とき、その文を 1 カウント（= 検出成功）とする。\n",
    "#   → つまり「どこを直すべきか」を完全に当てたか（= 誤り検出の完全一致）を測っている。\n",
    "#\n",
    "# 理論メモ：\n",
    "# - ここでの predict は MLM の **全位置 argmax** に基づくため、「元入力の自己再現」に偏りやすい。\n",
    "#   → 誤り位置だけ [MASK] 化して再予測（top-k/beam）するほうが「検出・訂正」ともに有利。\n",
    "# - トークンは **WordPiece** 単位。1 文字の置換でもサブワード分割の違いで複数位置に波及しうる。\n",
    "#   → kanji 誤変換の評価では、形態素境界/サブワード境界の揺らぎに注意。\n",
    "# - 本コードは zip により「短い方」に合わせて比較するため、長さ不一致の末尾は無視される。\n",
    "#   → 評価の厳密性を高めるなら zip_longest で長さ不一致を即失敗とする運用が堅牢。\n",
    "#\n",
    "# 実運用 Tips（必要なら適用）：\n",
    "# - デバイス可搬化：.cuda() 固定を避け、`device = next(bert_mlm.parameters()).device` →\n",
    "#   `tensor.to(device)` に統一（Mac/MPS・CUDA・CPU 横断）。\n",
    "# - 正規化：比較前に NFKC・空白/句読点の正規化で体裁差の影響を減らす。\n",
    "# - 追加指標：位置単位 Precision/Recall/F1（誤り位置検出タスクとして評価）を併記すると学習方針が立てやすい。\n",
    "# ======================================================================\n",
    "\n",
    "# BERTで予測を行い、正解数を数える。\n",
    "correct_position_num = 0  # 正しく誤変換の漢字を特定できたデータの数\n",
    "\n",
    "for sample in tqdm(dataset_test):\n",
    "    wrong_text = sample[\"wrong_text\"]\n",
    "    correct_text = sample[\"correct_text\"]\n",
    "\n",
    "    # --- 入力のトークナイズ（list<int> の input_ids を得る） -----------------------\n",
    "    # 注意：ここでは max_length を設定していないため、[PAD] は基本入らないが、\n",
    "    # モデルや設定次第では可変長となる。評価の一貫性のため固定長化も選択肢。\n",
    "    encoding = tokenizer(wrong_text)\n",
    "    wrong_input_ids = encoding[\"input_ids\"]  # 誤変換の文の符合列\n",
    "\n",
    "    # デバイス注意：.cuda() 固定は環境依存（CPU/MPS で失敗）\n",
    "    # 実務では：\n",
    "    #   device = next(bert_mlm.parameters()).device\n",
    "    #   encoding = {k: torch.tensor([v], device=device) for k, v in encoding.items()}\n",
    "    encoding = {k: torch.tensor([v]).cuda() for k, v in encoding.items()}\n",
    "\n",
    "    correct_encoding = tokenizer(correct_text)\n",
    "    correct_input_ids = correct_encoding[\"input_ids\"]  # 正しい文の符合列\n",
    "\n",
    "    # --- 予測：MLM の全位置 argmax（本来は [MASK] 位置のみの復元が得意） ----------\n",
    "    with torch.no_grad():\n",
    "        output = bert_mlm(**encoding)\n",
    "        scores = output.logits\n",
    "        # 予測された文章のトークンのID（[CLS]/[SEP]/[PAD] 含む長さ T）\n",
    "        predict_input_ids = scores[0].argmax(-1).cpu().numpy().tolist()\n",
    "\n",
    "    # --- 特殊トークンの除去 ---------------------------------------------------------\n",
    "    # 先頭 [CLS] と末尾 [SEP] を除く。PAD を入れていない前提だが、固定長化している場合は\n",
    "    # PAD 除外も必要（== トークナイザの設定と合わせること）。\n",
    "    wrong_input_ids = wrong_input_ids[1:-1]\n",
    "    correct_input_ids = correct_input_ids[1:-1]\n",
    "    predict_input_ids = predict_input_ids[1:-1]\n",
    "\n",
    "    # --- 誤り位置検出の厳密判定（文単位 And 条件） --------------------------------\n",
    "    # ルール：\n",
    "    #  - 正しい位置（wrong==correct）では「変更しない」（predict==wrong）\n",
    "    #  - 誤り位置（wrong!=correct）では「変更する」（predict!=wrong）\n",
    "    # すべての位置で満たす → detect_flag=True\n",
    "    #\n",
    "    # 注意：\n",
    "    #  - zip により最短長に切り詰め比較。長さ不一致が起きる場合は評価方針の明確化が必要。\n",
    "    #  - WordPiece の分割差で “誤り” が複数サブワードに分散することがある。\n",
    "    detect_flag = True\n",
    "    for wrong_token, correct_token, predict_token in zip(\n",
    "        wrong_input_ids, correct_input_ids, predict_input_ids\n",
    "    ):\n",
    "\n",
    "        if wrong_token == correct_token:  # 正しいトークン位置\n",
    "            # 正しいのに別トークンへ“改変” → 検出失敗\n",
    "            if wrong_token != predict_token:\n",
    "                detect_flag = False\n",
    "                break\n",
    "        else:  # 誤りトークン位置\n",
    "            # 誤りなのに“放置”（predict == wrong） → 検出失敗\n",
    "            if wrong_token == predict_token:\n",
    "                detect_flag = False\n",
    "                break\n",
    "\n",
    "    if detect_flag:  # 誤変換の位置を“すべて正しく”特定できた場合のみ加点\n",
    "        correct_position_num += 1\n",
    "\n",
    "# 文単位の厳密一致（完全検出）Accuracy を表示\n",
    "print(f\"Accuracy: {correct_position_num/len(dataset_test):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
