{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6cba954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maton/BERT_Practice/chap6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maton/.pyenv/versions/3.10.13/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# 6-1\n",
    "!mkdir chap6\n",
    "%cd ./chap6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a66273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-3\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 日本語の事前学習モデル\n",
    "MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "331e402d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using device = mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6-4（説明コメント付き：日本語BERTによる二値分類のセットアップ）\n",
    "# =====================================================================\n",
    "# 目的：\n",
    "#  - 事前学習済みの日本語BERT（MODEL_NAME）を、Sequence Classification（文分類）タスク用ヘッド付きでロードし、\n",
    "#    デバイス（MacのMPS / CUDA / CPU）に配置する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - BertForSequenceClassification は BERT 本体の上に「分類ヘッド（Dropout→Linear）」を載せたモデルである。\n",
    "#  - 推論時の出力 logits は形状 [B, num_labels]。num_labels=2 のとき、\n",
    "#    softmax(logits) によりクラス確率（2クラス）が得られる（argmax で予測ラベル）。\n",
    "#  - 学習時の損失は入力ラベルと num_labels により自動選択される（HFの既定）：\n",
    "#      * num_labels == 1 かつラベルが float       → 回帰（MSELoss）\n",
    "#      * num_labels > 1 かつラベルが整数カテゴリ → 単一ラベル分類（CrossEntropyLoss）\n",
    "#      * それ以外（ラベルが multi-hot 等）      → マルチラベル（BCEWithLogitsLoss）\n",
    "#  - BERT の文表現は概ね [CLS] トークンの隠れ状態（最終層）を用い、分類ヘッドで写像する設計（プール層の出力を使う実装もある）。\n",
    "#  - トークナイザは [CLS]/[SEP]/[PAD] を自動付与（既定）し、attention_mask でPADを無視する。\n",
    "# =====================================================================\n",
    "\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "\n",
    "\n",
    "# --- デバイス選択（Mac優先：MPS → CUDA → CPU） ---\n",
    "def get_best_device() -> torch.device:\n",
    "    # Apple Silicon + macOS Metal（CUDAは不可）\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    # 他環境（Linux/Windows等）ではCUDAを優先\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # それ以外はCPU\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"[info] using device = {device}\")\n",
    "\n",
    "# --- モデル名の決定（未定義時の保険） ---\n",
    "try:\n",
    "    MODEL_NAME\n",
    "except NameError:\n",
    "    # WWM版：語単位での一貫性を意識したマスキングで事前学習（日本語に適した設定）\n",
    "    MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "\n",
    "# --- トークナイザのロード ---\n",
    "# 形態素解析→WordPieceの二段分割。語彙はモデルとペアで一致させる（IDずれ防止）。\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- 文分類モデル（2クラス）をロード ---\n",
    "# num_labels=2：二値分類（例：ネガ/ポジ、真/偽 など）\n",
    "# 出力 logits 形状：[B, 2]。学習時にラベルが整数（0/1）なら CrossEntropyLoss が自動選択される。\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "# --- モデルを最適デバイスへ配置 ---\n",
    "# 元コード：bert_sc = bert_sc.cuda()\n",
    "# → Mac の場合 .cuda() は不可。統一して .to(device) を用いる。\n",
    "bert_sc = bert_sc.to(device)\n",
    "\n",
    "# --- 推論時の基本姿勢 ---\n",
    "# 評価モードにして Dropout を停止（推論の安定化）。学習時は bert_sc.train() に切り替える。\n",
    "bert_sc.eval()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 参考：推論の最小例（コメント解除で動作確認）\n",
    "# ---------------------------------------------------------------------\n",
    "# texts = [\"今日は自然言語処理を学ぶ。\", \"この映画は退屈だった。\"]\n",
    "# enc = tokenizer(\n",
    "#     texts,\n",
    "#     max_length=128,           # 学習・評価で固定長を揃えると効率的（pad_to_max_length 相当）\n",
    "#     padding=\"max_length\",\n",
    "#     truncation=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# )\n",
    "# enc = {k: v.to(device) for k, v in enc.items()}\n",
    "# with torch.no_grad():\n",
    "#     logits = bert_sc(**enc).logits           # [B, 2]\n",
    "#     probs  = logits.softmax(dim=-1)          # クラス確率\n",
    "#     preds  = probs.argmax(dim=-1)            # 予測クラスID（0/1）\n",
    "# print(\"probs:\", probs.detach().cpu().numpy())\n",
    "# print(\"preds:\", preds.detach().cpu().tolist())\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 運用上の注意（理論と実務の橋渡し）\n",
    "# ---------------------------------------------------------------------\n",
    "# - 前処理の一貫性：学習・評価で tokenizer の設定（max_length, padding, truncation）が一致していること。\n",
    "# - ラベル整合性：データセット内で 0/1 とクラス名のマッピングを厳密に固定（モデル保存時に一緒に管理）。\n",
    "# - 不均衡データ：クラス不均衡が強い場合は、重み付き損失（class_weight）や閾値最適化で改善。\n",
    "# - 再現性：random seed、モデルバージョン、辞書バージョンを固定。デバイス差（MPS/CUDA/CPU）で微差が出る点は許容範囲で管理。\n",
    "# - 転移学習：タスクに合わせて最終ヘッドのみならずBERT側も微調整（学習率は本体を小さく、ヘッドを大きくするのが通例）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2330781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# scores (shape):\n",
      "torch.Size([3, 2])\n",
      "# predicted labels:\n",
      "tensor([1, 1, 1], device='mps:0')\n",
      "# accuracy:\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# 6-5（説明コメント付き：二値文分類のバッチ推論と精度計算）\n",
    "# =====================================================================\n",
    "# 目的：\n",
    "#  - 日本語BERTの文分類モデル（bert_sc; num_labels=2）を用いて、複数文の推論を一括で行い、\n",
    "#    予測ラベルと単純精度（accuracy）を計算・表示する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - BertForSequenceClassification の出力 logits 形状は [B, num_labels]。\n",
    "#    num_labels=2 のとき argmax(logits) は「2クラスsoftmaxの最大事後確率クラス」と一致する。\n",
    "#  - tokenizer(..., padding='longest') はバッチ内の最長系列に合わせてPADを付与する\n",
    "#    “動的パディング”。attention_mask により PAD 位置は Self-Attention から無視される。\n",
    "#  - 学習時はラベル（0/1）が整数クラスなら CrossEntropyLoss が自動選択される（HFの既定）。\n",
    "#    本セルは推論のみ（no_grad & eval）。\n",
    "#  - デバイス（MPS/CUDA/CPU）は“モデルが現在置かれている device”に合わせて入力テンソルを移動する。\n",
    "#    （Mac は通常 MPS。`.cuda()` は不可なので `.to(device)` を一貫して用いる。）\n",
    "# =====================================================================\n",
    "\n",
    "text_list = [\n",
    "    \"この映画は面白かった。\",  # 正例のつもり（1）\n",
    "    \"この映画の最後にはがっかりさせられた。\",  # 負例のつもり（0）\n",
    "    \"この映画を見て幸せな気持ちになった。\",  # 正例のつもり（1）\n",
    "]\n",
    "label_list = [1, 0, 1]  # 教師ラベル（0=negative, 1=positive の想定）\n",
    "\n",
    "# --- データの符号化（トークン化→ID化） ---\n",
    "# padding='longest'：バッチ内の最長文の長さに合わせてPADを付与（動的パディング）。\n",
    "# return_tensors='pt'：PyTorchテンソルで返す（input_ids/attention_mask/token_type_ids など）。\n",
    "# ＊長文や学習時は max_length と truncation=True を併用して長さ上限を明示するのが安定。\n",
    "encoding = tokenizer(text_list, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "# --- デバイスをモデルに揃える（MPS/CUDA/CPU いずれでも動く書き方） ---\n",
    "# bert_sc は 6-4 で .to(device) 済みの前提。ここではその device を参照して入力を移す。\n",
    "device = next(bert_sc.parameters()).device\n",
    "encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "# ラベルテンソル作成\n",
    "# dtype=torch.long：分類（CE Loss）のクラスIDは long 整数で表現するのが通例。\n",
    "labels = torch.tensor(label_list, dtype=torch.long, device=device)\n",
    "\n",
    "# --- 推論（評価モード＋勾配無効） ---\n",
    "# eval()：Dropout等を停止して推論安定化。6-4で eval 済みでも冪等。\n",
    "bert_sc.eval()\n",
    "with torch.no_grad():\n",
    "    # モデル呼び出し：.forward(**encoding) と bert_sc(**encoding) は等価（後者が慣用的）\n",
    "    output = bert_sc(**encoding)\n",
    "\n",
    "# --- 出力の解釈と評価 ---\n",
    "scores = output.logits  # 形状 [B, 2]：各クラスのスコア（ロジット）\n",
    "labels_predicted = scores.argmax(-1)  # 予測クラスID（最尤クラス）\n",
    "num_correct = (labels_predicted == labels).sum().item()  # 正解数（スカラー）\n",
    "accuracy = num_correct / labels.size(0)  # 単純精度（micro accuracy）\n",
    "\n",
    "# --- 可視出力（形状と予測・精度） ---\n",
    "print(\"# scores (shape):\")\n",
    "print(scores.size())  # 例：torch.Size([3, 2])\n",
    "print(\"# predicted labels:\")\n",
    "print(labels_predicted)  # 例：tensor([1, 0, 1], device=..., dtype=torch.int64)\n",
    "print(\"# accuracy:\")\n",
    "print(accuracy)  # 例：1.0\n",
    "\n",
    "# 運用上の注意：\n",
    "# - データ分布が偏っている場合、単純精度だけでは不十分。適宜、適合率/再現率/F1、混同行列を併用する。\n",
    "# - 推論の閾値調整が必要な場合（特にコスト非対称なタスク）は、2クラスsoftmaxの確率を取得して閾値最適化を行う。\n",
    "# - 学習と評価で tokenizer の前処理（max_length/padding/truncation）を一致させること。\n",
    "# - デバイス差（MPS/CUDA/CPU）でごく僅かな数値差が出る場合があるが、多くは統計誤差の範囲。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e75180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5852, device='mps:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 6-6（説明コメント付き：ラベル付き入力で損失を計算）\n",
    "# =====================================================================\n",
    "# 目的：\n",
    "#  - 3本のテキストを一括でトークン化し、ラベル（0/1）を入力に同梱して\n",
    "#    BertForSequenceClassification が内部で計算する損失（loss）を取得して表示する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - Hugging Face の BertForSequenceClassification は、forward(**batch) に\n",
    "#    'labels' を渡すと自動で損失を計算して返す。\n",
    "#    * num_labels=2 かつ labels が整数クラス（dtype: long）の場合 → CrossEntropyLoss\n",
    "#    * 出力 logits の形状は [B, num_labels]。loss はバッチ平均のスカラー。\n",
    "#  - tokenizer(..., padding='longest') は“動的パディング”（バッチ内の最長系列に合わせてPAD）。\n",
    "#    attention_mask により PAD 位置の注意は抑制される。\n",
    "#  - デバイス整合：モデル（bert_sc）の置かれている device（MPS/CUDA/CPU）に\n",
    "#    入力テンソルを合わせること（.to(device)）。Mac では .cuda() は不可。\n",
    "# =====================================================================\n",
    "\n",
    "# 符号化（トークン化→ID化）。長さはバッチ内最長に合わせてPAD（学習では max_length+truncation推奨）。\n",
    "encoding = tokenizer(text_list, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "# ラベルを入力 dict に追加。\n",
    "# - CrossEntropyLoss の前提として dtype は long（整数クラスID）。\n",
    "# - デバイス移動はこの後まとめて行うので、ここではCPUでOK。\n",
    "encoding[\"labels\"] = torch.tensor(label_list, dtype=torch.long)\n",
    "\n",
    "# デバイス整合：モデルの実デバイスへ（Macは多くの場合 'mps'）。\n",
    "device = next(bert_sc.parameters()).device\n",
    "encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "# ロスの計算\n",
    "# - labels を渡しているため、出力に loss が含まれる。\n",
    "# - 学習（逆伝播）するなら bert_sc.train() にして、loss.backward() → optimizer.step() を行う。\n",
    "#   ここでは損失値の確認のみ。\n",
    "output = bert_sc(**encoding)\n",
    "loss = output.loss  # スカラー損失（バッチ平均）\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# 補足（運用上の注意）：\n",
    "# - 動的パディングはバッチごとに系列長が変わるため、学習ループでは DataCollator で揃えるか、\n",
    "#   max_length + truncation + padding='max_length' で固定長にすると効率が安定する。\n",
    "# - クラス不均衡が大きい場合、重み付き損失や閾値最適化、評価指標（F1等）の併用が有効。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69d5bca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-23 10:54:21--  https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
      "www.rondhuit.com (www.rondhuit.com) をDNSに問いあわせています... 59.106.19.174\n",
      "www.rondhuit.com (www.rondhuit.com)|59.106.19.174|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 8855190 (8.4M) [application/x-gzip]\n",
      "`ldcc-20140209.tar.gz' に保存中\n",
      "\n",
      "ldcc-20140209.tar.g 100%[===================>]   8.44M  1.33MB/s 時間 5.7s       \n",
      "\n",
      "2025-11-23 10:54:28 (1.49 MB/s) - `ldcc-20140209.tar.gz' へ保存完了 [8855190/8855190]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6-7\n",
    "#データのダウンロード\n",
    "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz \n",
    "#ファイルの解凍\n",
    "!tar -zxf ldcc-20140209.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd7f65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.livedoor.com/article/detail/6342280/\n",
      "2012-03-06T13:00:00+0900\n",
      "USB3.0対応で爆速データ転送！　9倍速のリーダー／ライター登場\n",
      "USB3.0が登場してから今年で4年目となるがパソコン側でのUSB3.0ポート搭載が進んで来ても対応機器がなかなか充実していない現状がある。そんな中で新しく高速な読み取りが可能なメモリーカードリーダー／ライターが登場した。\n",
      "\n",
      "バッファローコクヨサプライがUSB3.0対応のカードリーダー／ライターを発表した。SDHC対応のSD系メディアやコンパクトフラッシュ、メモリースティック系メディア、xDピクチャーカードといったデジカメやスマホ、携帯ゲームといった機器で使われている各種メディアを従来よりも短時間でPCに取り込むことが可能になる。\n",
      "\n",
      "転送速度が5Gbps（理論値）とUSB2.0の480Mbpsと比べて爆速になったUSB3.0はPC側の対応が進んで来ていたが高速転送が生かせる周辺機器としては、外付けHDDや一部のUSBメモリーくらいしかなかった。これに多くのメディアが扱えるリーダー／ライターが加わることで手軽にUSB3.0の恩恵を受けることができるようになる。\n",
      "\n",
      "今回発表されたのは、USB3.0ケーブルとカードリーダー本体が分かれるタイプの「BSCR09U3」シリーズ（3,240円）、USB3.0コネクタをカードリーダー本体に内蔵している「BSCRD04U3」シリーズ（2,690円）だ。共にホワイトとブラックのカラーバリエーションが用意される（発売は3月下旬以降）。\n",
      "\n",
      "■リリースページ\n",
      "■バッファローコクヨサプライ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "■バッファローの記事をもっと見る\n",
      "・約283gでカバンに入る！小型キーボードの驚くべき機能\n",
      "・3種類のホットキーで使いやすい！AndroidとPCで使えるキーボードの魅力\n",
      "・ドラえもんもビックリの新アイテム！マウスとキーボードが合体\"OPAir\"\n",
      "・ありそうでなかった便利機能！ファイル仕分けする画期的なHDD\n",
      "\n",
      "\n",
      "サンディスク SanDisk microSDHC 32GB（microSD 32GB） 超高速クラス4  変換アダプター付 世界国内シェアNo.1 バルク品\n",
      "クチコミを見る\n"
     ]
    }
   ],
   "source": [
    "!cat ./text/it-life-hack/it-life-hack-6342280.txt # ファイルを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b67a76d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# batch 0\n",
      "{'data': tensor([[0, 1],\n",
      "        [2, 3]]), 'labels': tensor([0, 1])}\n",
      "# batch 1\n",
      "{'data': tensor([[4, 5],\n",
      "        [6, 7]]), 'labels': tensor([2, 3])}\n"
     ]
    }
   ],
   "source": [
    "# 6-9（説明コメント付き：PyTorch DataLoader でミニバッチを取り出す最小例）\n",
    "# ======================================================================\n",
    "# 目的：\n",
    "#  - list[dict] で用意した玩具データから DataLoader を構築し、ミニバッチ単位で取り出す流れを確認する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - DataLoader は「確率的勾配降下（SGD）」系学習における **ミニバッチ化** を担う。\n",
    "#    統計的には、全データの勾配（バッチ平均）を近似しつつ、適度なノイズで汎化を促進する。\n",
    "#  - collate（結束）規則：デフォルト collate_fn は **辞書の各キーを軸にテンソル結合** する。\n",
    "#    つまり list[{\"data\": t1, \"labels\": y1}, {\"data\": t2, \"labels\": y2}] →\n",
    "#    {\"data\": stack([t1, t2]), \"labels\": stack([y1, y2])}\n",
    "#  - 分類タスクのラベルは通常 **整数（torch.long）**。CrossEntropyLoss は long クラスIDを想定する。\n",
    "#  - 実運用では DataLoader のパラメータ（shuffle, num_workers, pin_memory, drop_last 等）が\n",
    "#    収束性やスループット、再現性に影響する（後述の注意参照）。\n",
    "# ======================================================================\n",
    "\n",
    "# データローダーの作成\n",
    "# - 各要素は dict で、\"data\" に特徴ベクトル、\"labels\" にクラスID（スカラー）を持つ。\n",
    "# - ここでは玩具データとして 2次元ベクトル＋ラベルを定義（ラベル0～3 → 4クラス相当）。\n",
    "#   ※ 直前までの BERT 例（num_labels=2）とは設定が異なる点に注意（あくまで DataLoader の例）。\n",
    "dataset_for_loader = [\n",
    "    {\"data\": torch.tensor([0, 1]), \"labels\": torch.tensor(0)},  # 例：サンプル1\n",
    "    {\"data\": torch.tensor([2, 3]), \"labels\": torch.tensor(1)},  # 例：サンプル2\n",
    "    {\"data\": torch.tensor([4, 5]), \"labels\": torch.tensor(2)},  # 例：サンプル3\n",
    "    {\"data\": torch.tensor([6, 7]), \"labels\": torch.tensor(3)},  # 例：サンプル4\n",
    "]\n",
    "# - batch_size=2：2件ずつ結束。shuffle を付ければ各エポックで順番をランダム化できる。\n",
    "# - デフォルト collate_fn により、辞書キーごとにテンソルが stack される。\n",
    "loader = DataLoader(dataset_for_loader, batch_size=2)\n",
    "\n",
    "# データセットからミニバッチを取り出す\n",
    "# - enumerate(loader) は各反復で dict({\"data\": Tensor[B, ...], \"labels\": Tensor[B]}) を返す。\n",
    "# - 実際のファインチューニングでは、このループ内で\n",
    "#   1) device へ転送 → 2) forward → 3) loss 計算 → 4) backward → 5) optimizer.step → 6) zero_grad\n",
    "#   を行う（評価時は no_grad + eval()）。\n",
    "for idx, batch in enumerate(loader):\n",
    "    print(f\"# batch {idx}\")\n",
    "    print(batch)\n",
    "    ## ファインチューニングではここでミニバッチ毎の処理を行う\n",
    "    # 例（擬似手順；実コードはモデルと損失に依存）：\n",
    "    # - batch = { \"data\": Tensor[B, D], \"labels\": Tensor[B] }\n",
    "    # - features = batch[\"data\"].to(device)         # モデルの device（CPU/MPS/CUDA）へ\n",
    "    # - targets  = batch[\"labels\"].to(device).long()# CE を使うなら long へ\n",
    "    # - logits   = model(features)                  # 前向き計算\n",
    "    # - loss     = criterion(logits, targets)       # 例：CrossEntropyLoss\n",
    "    # - loss.backward(); optimizer.step(); optimizer.zero_grad()\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 実務上の注意（BERT/Transformers 文脈へのブリッジ）\n",
    "# ----------------------------------------------------------------------\n",
    "# - 本例は玩具の \"data\"/\"labels\" 構造だが、Transformers では通常\n",
    "#   {\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"} の dict を DataLoader が返す。\n",
    "# - トークン列は長さがまちまちのため、DataCollator（例：DataCollatorWithPadding）で\n",
    "#   バッチごとに動的パディングするのが一般的。固定長にしたい場合は\n",
    "#   tokenizer(..., max_length, truncation=True, padding=\"max_length\")。\n",
    "# - 速度最適化：\n",
    "#   * num_workers>0 で入出力を並列化（ただし MPS は制約があるため環境依存で調整）。\n",
    "#   * pin_memory=True（CUDA で有効）、prefetch_factor の調整。\n",
    "# - 再現性：\n",
    "#   * 乱数seed固定、shuffle=True の際は Sampler も含めて管理。\n",
    "#   * drop_last=True はバッチ正規化などで端数バッチを避けたい時に有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e46a542a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# batch 0\n",
      "{'data': tensor([[2, 3],\n",
      "        [4, 5]]), 'labels': tensor([1, 2])}\n",
      "# batch 1\n",
      "{'data': tensor([[0, 1],\n",
      "        [6, 7]]), 'labels': tensor([0, 3])}\n"
     ]
    }
   ],
   "source": [
    "# 6-10（説明コメント付き：DataLoader をシャッフルしてミニバッチ列挙）\n",
    "# ======================================================================\n",
    "# 目的：\n",
    "#  - 直前セル（6-9）で用意した list[dict] 形式のデータを DataLoader に渡し、\n",
    "#    shuffle=True で「各エポックごとにサンプル順をランダム化」したうえで、\n",
    "#    ミニバッチ単位で取り出す挙動を確認する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - SGD 系学習では、データ順をランダム化（シャッフル）することで\n",
    "#    勾配推定のバイアスを減らし、汎化性能を高めやすい（局所最適・順序依存の緩和）。\n",
    "#  - PyTorch の DataLoader は「エポックの開始時にランダムな順列をサンプル集合に適用」し、\n",
    "#    その順に batch_size ごとに切り出す（drop_last=False なら端数も返す）。\n",
    "#  - デフォルトの collate_fn は「同じキーでテンソルを stack」するため、\n",
    "#    list[{\"data\": t_i, \"labels\": y_i}] → {\"data\": stack(t_i), \"labels\": stack(y_i)} になる。\n",
    "#  - 再現性が必要なら、DataLoader(generator=...) に seed を渡すか、\n",
    "#    グローバルに torch.manual_seed(...) と Sampler を適切に設定する。\n",
    "# ======================================================================\n",
    "\n",
    "# DataLoader の生成\n",
    "# - dataset_for_loader（6-9で定義済み）からバッチサイズ2で取り出す。\n",
    "# - shuffle=True によって、反復のたびに（厳密にはエポックのたびに）順序が変わり得る。\n",
    "loader = DataLoader(dataset_for_loader, batch_size=2, shuffle=True)\n",
    "\n",
    "# ミニバッチの列挙\n",
    "# - 各反復で返る `batch` は dict で、{\"data\": Tensor[B, ...], \"labels\": Tensor[B]} の形。\n",
    "# - 実際の学習では、このループ内で device 転送 → forward → loss → backward → step を行う。\n",
    "for idx, batch in enumerate(loader):\n",
    "    print(f\"# batch {idx}\")\n",
    "    print(batch)\n",
    "\n",
    "    ## ファインチューニングではここでミニバッチ毎の処理を行う\n",
    "    # 例（擬似手順）：\n",
    "    # features = batch[\"data\"].to(device)\n",
    "    # targets  = batch[\"labels\"].to(device).long()  # CE を使うなら long\n",
    "    # logits   = model(features)\n",
    "    # loss     = criterion(logits, targets)\n",
    "    # loss.backward(); optimizer.step(); optimizer.zero_grad()\n",
    "\n",
    "# 補足：\n",
    "# - 同じコードを何度実行しても、shuffle=True のためバッチ内容の順序は実行のたびに変わる。\n",
    "# - 再現性を担保したい場合（ベンチマークや回帰テスト）は、\n",
    "#   gen = torch.Generator().manual_seed(42)\n",
    "#   loader = DataLoader(..., shuffle=True, generator=gen)\n",
    "#   のように「生成器を明示」するのが確実。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2589b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:16<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# 6-11（説明コメント付き：livedoorニュース等のカテゴリ別テキストをBERT学習用に前処理）\n",
    "# ======================================================================\n",
    "# 目的：\n",
    "#  - カテゴリ名のリスト（例：livedoor ニュースコーパス相当）から各カテゴリの記事ファイルを走査し，\n",
    "#    トークナイザで ID 列へ符号化して，学習用データ（辞書の配列）を構築する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - 分類タスクでは，各サンプルは {input_ids, token_type_ids, attention_mask, labels} の辞書で表すのが標準。\n",
    "#  - BERT 系は [CLS] を用いて文（文書）表現を得るため，長文は max_length で**切り詰め（truncation）**される。\n",
    "#    → 重要情報が後半にある記事は情報落ちのリスク。必要に応じ「スライディングウィンドウ」や要約等を検討。\n",
    "#  - padding='max_length' により**固定長化**することでミニバッチの効率・再現性が上がる（メモリ見積りが安定）。\n",
    "#  - labels は整数クラス ID（long 型）で CrossEntropyLoss に適合する。カテゴリ順序＝ラベル割当になるため，\n",
    "#    後段の評価・可視化で**クラス名との対応表**を必ず保存すること。\n",
    "# ======================================================================\n",
    "\n",
    "# 前提（このセルの外でインポート済みであること）：\n",
    "#   from tqdm import tqdm\n",
    "#   import glob\n",
    "#   import torch\n",
    "#   from transformers import BertJapaneseTokenizer\n",
    "#   MODEL_NAME は 6-4 と同一（トークナイザとモデルの語彙は一致させる）\n",
    "\n",
    "# カテゴリーのリスト\n",
    "# - enumerate により 0,1,2,... の整数がラベルとして付与される。\n",
    "# - 順序はそのままクラスIDになるため、将来の再現・可視化のためにこのリストを保存しておくこと。\n",
    "category_list = [\n",
    "    \"dokujo-tsushin\",\n",
    "    \"it-life-hack\",\n",
    "    \"kaden-channel\",\n",
    "    \"livedoor-homme\",\n",
    "    \"movie-enter\",\n",
    "    \"peachy\",\n",
    "    \"smax\",\n",
    "    \"sports-watch\",\n",
    "    \"topic-news\",\n",
    "]\n",
    "\n",
    "# トークナイザのロード\n",
    "# - BERT 本体と同じ MODEL_NAME を使用（語彙IDの不一致を防ぐ）。\n",
    "# - BertJapaneseTokenizer は形態素解析→WordPiece の二段分割を内部で行う。\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 各データの形式を整える\n",
    "# - max_length：系列長の上限。大きくすると情報保持は増えるが，計算負荷とメモリも増える。\n",
    "# - 固定長（padding='max_length'）はミニバッチ効率を安定させる。\n",
    "max_length = 128\n",
    "dataset_for_loader = []\n",
    "\n",
    "# ループ構造：\n",
    "# - カテゴリごとにファイルを探索し（glob），各ファイルから本文を抽出→トークナイズ→テンソル化→labels 付与。\n",
    "# - tqdm により進捗が可視化される（長時間処理の観測性を確保）。\n",
    "for label, category in enumerate(tqdm(category_list)):\n",
    "    # ファイル探索：\n",
    "    # - 例：./text/it-life-hack/it-life-hack* のようなパターンをすべて拾う。\n",
    "    # - ディレクトリ構成はデータセット依存。I/O 例外やエンコーディングの揺れがある場合は try/except と encoding 指定を検討。\n",
    "    for file in glob.glob(f\"./text/{category}/{category}*\"):\n",
    "        # 記事の読み取り：\n",
    "        # - livedoor コーパス慣例では 1行目:URL, 2行目:日付, 3行目:タイトル, 4行目以降:本文 となっているため，\n",
    "        #   ここでは 4 行目（index=3）以降を本文として使用している。\n",
    "        # - 改行は '\\n' で連結。open の文字コードは環境により 'utf-8' 明示が安全。\n",
    "        lines = open(file).read().splitlines()\n",
    "        text = \"\\n\".join(\n",
    "            lines[3:]\n",
    "        )  # ファイルの4行目からを抜き出す（タイトル・メタ情報は除外）。\n",
    "\n",
    "        # トークナイズ：\n",
    "        # - max_length：上限に達したトークンは切り詰め（truncation=True）。\n",
    "        # - padding='max_length'：不足分は [PAD] で埋め，attention_mask=0 で無視される。\n",
    "        # - 戻り値は dict（input_ids/token_type_ids/attention_mask などのリスト）。\n",
    "        encoding = tokenizer(\n",
    "            text, max_length=max_length, padding=\"max_length\", truncation=True\n",
    "        )\n",
    "\n",
    "        # ラベル付与：\n",
    "        # - enumerate の整数（0..）をそのままクラスIDに採用。\n",
    "        # - CrossEntropyLoss を想定するなら long 整数であることが前提（この後の tensor 化で既定は int64=long）。\n",
    "        encoding[\"labels\"] = label\n",
    "\n",
    "        # テンソル化：\n",
    "        # - DataLoader で stack できるよう，各フィールドを torch.tensor に変換。\n",
    "        # - dtype は自動推論されるが，必要に応じて labels を明示的に dtype=torch.long にしても良い。\n",
    "        encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "\n",
    "        # データ配列へ格納：\n",
    "        # - 1 サンプル = 1 dict。後段で DataLoader に渡すとキーごとに stack されてミニバッチになる。\n",
    "        dataset_for_loader.append(encoding)\n",
    "\n",
    "# 補足（運用上の注意）：\n",
    "# - データ分割：本コードは全件を単一リストに溜める。**学習/検証/テストの分割**は別途必須（ランダム分割や時間分割等）。\n",
    "# - クラス不均衡：カテゴリ間でサンプル数が偏ると学習が偏る。重み付き損失やリサンプリングを検討。\n",
    "# - 文字コード：open(file, encoding='utf-8') などの明示が環境により必要。I/O エラー処理も実務では追加。\n",
    "# - 長文対策：128 トークンで情報が欠落する場合は，max_length を増やす／スライディングウィンドウで分割／要約前処理を検討。\n",
    "# - 再現性：MODEL_NAME，トークナイザ・辞書バージョン，カテゴリ順序→ラベル対応をメタデータとして保存する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9788040e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2,   227, 28553,   227,   687,  4847,    80,     8,   858,    19,\n",
      "           40,  6309, 10598,    11,   654,  1174,    15,    16,    33,     5,\n",
      "            9, 21411, 15933,    14,  1460,    34, 18447,  9699, 18447,  9541,\n",
      "          725,  9878, 28511,     8,  4726,     7,  2575,  3290,  1624, 31041,\n",
      "         2612,    11,  2949,  5835,    16, 19199,    13,   969,   558,    40,\n",
      "         5847,    81,    18,  1913,    11,   489,    16,    33,     8, 15933,\n",
      "          811,   737,    12,     9,     5,  3892,     7, 24169,    15,    10,\n",
      "        10494,    13,    59,    73,  3062,     9,  2935,  1766,  8881, 28756,\n",
      "          235,    40,  8962,  1978,   155,     8,    73, 29928,    11,   454,\n",
      "        28470,   191,    16,  1096, 28555,  5370,     5, 18447,  9699, 18447,\n",
      "         9541,   725,  9878, 28511,    11,  3111,    10,     8, 20964,     9,\n",
      "          259, 29064,    13,   625,  3005,    11,  4748, 31260,    34,  1101,\n",
      "        14974,    16,    73, 28363,    75,     8,  4947,     3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "# 6-12\n",
    "print(dataset_for_loader[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b08a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-13\n",
    "# =========================================================\n",
    "# 説明のコメント付き：学習/検証/テスト分割と DataLoader 構築\n",
    "# ---------------------------------------------------------\n",
    "# 目的：\n",
    "#  - 事前に前処理済み（dict: input_ids/attention_mask/token_type_ids/labels …）の\n",
    "#    dataset_for_loader を 60/20/20 に分割し、学習・検証・テスト用の DataLoader を作成する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - ランダム分割は単純だが、**クラス不均衡**や**時系列依存**がある場合には性能推定が歪む。\n",
    "#    → 分類では本来 **層化分割（stratified split）** が望ましい（各クラス比率を保つ）。\n",
    "#  - 検証（val）はハイパーパラメータ・早期終了判定に使用、テスト（test）は最終一度だけ触れるのが原則。\n",
    "#  - DataLoader はミニバッチ化の責務：学習のみ shuffle=True（勾配推定のバイアス低減）。\n",
    "#    検証・テストは順序固定（再現性/デバッグ容易化）。\n",
    "#  - 再現性：random.shuffle は**シード未固定**だと毎回分割が変わる。必要なら外部で seed 固定。\n",
    "# =========================================================\n",
    "\n",
    "# データセットの分割\n",
    "random.shuffle(\n",
    "    dataset_for_loader\n",
    ")  # ランダムにシャッフル（※再現性が必要なら事前に seed 固定）\n",
    "n = len(dataset_for_loader)  # 全サンプル数\n",
    "n_train = int(0.6 * n)  # 60% を学習に\n",
    "n_val = int(0.2 * n)  # 20% を検証に\n",
    "# 残り（約20%）はテストへ（端数は test に回る）\n",
    "dataset_train = dataset_for_loader[:n_train]  # 学習データ\n",
    "dataset_val = dataset_for_loader[n_train : n_train + n_val]  # 検証データ\n",
    "dataset_test = dataset_for_loader[n_train + n_val :]  # テストデータ\n",
    "# ＊注意：分類タスクでクラス比率を保ちたい場合は、このランダム分割ではなく層化分割を用いる。\n",
    "#   （例：各クラスごとに同割合で train/val/test に落とし込む）\n",
    "\n",
    "# データセットからデータローダを作成\n",
    "# - DataLoader は dict のキーごとにテンソルを stack してミニバッチを作る（デフォルト collate）。\n",
    "# - 学習データは shuffle=True（各エポックで順序を無作為化して汎化性能向上を狙う）。\n",
    "# - バッチサイズ設計：\n",
    "#   * train は 32（学習安定と計算資源のバランス）。VRAM とスループットに応じて調整。\n",
    "#   * val/test は大きめ（256）で高速化（勾配計算なし前提）。VRAM に収まらない場合は小さく。\n",
    "# - drop_last：端数バッチを落としたい（BN等）場合に True を検討。BERT 文分類では通常 False で可。\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val, batch_size=256\n",
    ")  # 検証は順序固定（shuffle=False 既定）\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=256)  # テストも順序固定\n",
    "\n",
    "# 補足：\n",
    "# - 乱数一貫性：エポック間・実行間の再現性が必要なら、分割前に random.seed(...) を設定。\n",
    "#   DataLoader のシャッフル側は generator=... で seed を渡す方法もある。\n",
    "# - 大規模データ：num_workers, pin_memory（CUDAのみ）, persistent_workers などの I/O 最適化は学習時に検討。\n",
    "# - ラベル整合：クラス名 ↔ ラベルID（category_list の順序）対応表を必ず保存しておく（評価・可視化に必須）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01d08665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-14\n",
    "# =====================================================================\n",
    "# 説明のコメント付き：PyTorch Lightning を用いた BERT 文分類の学習ループ定義\n",
    "# ---------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - Transformers の BertForSequenceClassification を LightningModule に包み、\n",
    "#    学習・検証・テスト各フェーズの step を最小構成で定義する。\n",
    "#\n",
    "# 理論メモ（重要ポイント）：\n",
    "#  - BertForSequenceClassification は BERT 本体 + Dropout + Linear の「分類ヘッド」を持つ。\n",
    "#    forward(**batch) に labels（整数クラスID）を渡すと CrossEntropyLoss を**自動計算**して返す。\n",
    "#  - Lightning は training_step が返した loss を backward → optimizer.step する（標準挙動）。\n",
    "#    self.log(...) はメトリクスをロガー（TensorBoard等）に出し、on_step/on_epoch の集計方針を指定可能。\n",
    "#  - 本実装の optimizer は Adam。Transformer では通常 AdamW + weight decay + 学習率スケジューラ\n",
    "#    （linear warmup/decay）が一般的（改良余地としてコメントに示す）。\n",
    "#  - test_step の accuracy は「バッチ内平均」であり、エポック平均を厳密に取りたい場合は\n",
    "#    on_epoch=True で self.log するか、torchmetrics を使って集約するのが推奨。\n",
    "# =====================================================================\n",
    "\n",
    "\n",
    "class BertForSequenceClassification_pl(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        # model_name: Transformersのモデルの名前（例：'tohoku-nlp/bert-base-japanese-whole-word-masking'）\n",
    "        # num_labels: ラベルの数（多クラス分類ならクラス数、二値なら 2）\n",
    "        # lr        : 学習率（BERT 微調整の典型レンジは 2e-5〜5e-5 程度。層別LRやヘッド強めも有効）\n",
    "        super().__init__()\n",
    "\n",
    "        # save_hyperparameters():\n",
    "        # - __init__ の引数（ここでは model_name, num_labels, lr）を self.hparams に保存し、\n",
    "        #   チェックポイントにも自動で埋め込む（再現性・追跡性のために有用）。\n",
    "        # - 以後 self.hparams.lr などで参照できる。\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # BERTのロード：\n",
    "        # - 事前学習済み BERT + 分類ヘッド（Dropout→Linear）が読み込まれる。\n",
    "        # - num_labels により最終 Linear 出力次元が [hidden_size → num_labels] になる。\n",
    "        # - 学習時、labels（long 整数）を渡すと CrossEntropyLoss が内部で適用される（平均リダクション）。\n",
    "        self.bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=num_labels\n",
    "        )\n",
    "        # 参考（改善の余地・理論メモ）：\n",
    "        # - 転移学習の安定化：最初は分類ヘッドのみ学習→徐々にBERT本体を解凍（層別LR）\n",
    "        # - 正則化：weight decay（AdamW）や勾配クリップ、ドロップアウト率の調整が一般的\n",
    "        # - スケジューラ：線形ウォームアップ→線形減衰は Transformer 微調整のデファクト\n",
    "\n",
    "    # 学習データのミニバッチ(`batch`)が与えられた時に損失を出力する関数。\n",
    "    # Lightning は戻り値の loss に対して backward を実行する（標準設定時）。\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # batch は DataLoader からの dict（input_ids, attention_mask, token_type_ids, labels など）\n",
    "        # labels が含まれていれば、bert_sc(**batch) は loss（CE）を内部計算して返す。\n",
    "        output = self.bert_sc(**batch)\n",
    "        loss = output.loss\n",
    "        # ロギング：\n",
    "        # - 'train_loss' をログ。必要に応じて on_step=True/on_epoch=True, prog_bar=True 等を指定可能。\n",
    "        # - 例：self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    # 検証データのミニバッチが与えられた時の評価指標を計算する関数。\n",
    "    # ここでは損失（CE）のみをログしている。精度やF1が必要なら logits から算出して self.log する。\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        # バリデーション損失をログ。エポック平均を取りたい場合は on_epoch=True を明示するのが推奨。\n",
    "        self.log(\n",
    "            \"val_loss\", val_loss\n",
    "        )  # 例：self.log('val_loss', val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    # テストデータのミニバッチが与えられた時の評価指標を計算する関数。\n",
    "    # ここでは accuracy を「バッチごと」に計算・ログしている（エポック平均の厳密な集計は別設定が必要）。\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # 注意：batch.pop('labels') は batch を破壊的に変更する。再利用があり得る場合はコピー推奨。\n",
    "        labels = batch.pop(\"labels\")  # バッチからラベルを取り出し、モデル入力から除外\n",
    "        output = self.bert_sc(**batch)\n",
    "        # 予測クラス：logits の argmax。softmax 不要（順位のみ使うため）。\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        num_correct = (labels_predicted == labels).sum().item()\n",
    "        accuracy = num_correct / labels.size(0)  # バッチ内精度\n",
    "        # ロギング：エポック平均を取りたい場合は on_epoch=True を推奨（分散学習では sync_dist=True も）。\n",
    "        self.log(\n",
    "            \"accuracy\", accuracy\n",
    "        )  # 例：self.log('test_acc', accuracy, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    # 学習に用いるオプティマイザを返す関数。\n",
    "    # 現状は Adam（weight decay 無し）。Transformer の慣例では AdamW + weight_decay + スケジューラが一般的。\n",
    "    def configure_optimizers(self):\n",
    "        # 改良案（コメント）：\n",
    "        # - AdamW に変更：torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=0.01)\n",
    "        # - スケジューラ：linear warmup (数百〜数千ステップ) → linear decay\n",
    "        # - Lightning では {\"optimizer\": opt, \"lr_scheduler\": sched, \"monitor\": \"val_loss\"} の dict を返せる\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eaeeccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "# 6-15\n",
    "# =====================================================================\n",
    "# 説明のコメント付き：学習中のチェックポイント保存と Trainer 設定（PL 2.x 準拠）\n",
    "# ---------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - 検証指標（val_loss）を監視し、最良モデルの重みを保存する。\n",
    "#  - 旧API（gpus=...）を廃し、accelerator/devices でデバイスを指定（2.x推奨）。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - ModelCheckpoint は monitor と mode に従い、より良いモデルのみ保存（save_top_k=1）。\n",
    "#  - save_weights_only=True は版差で挙動が異なることがあるため、必要に応じ state_dict の別保存も検討。\n",
    "#  - Lightning 2.x ではデバイス指定は accelerator (\"cpu\"/\"gpu\"/\"mps\"/\"auto\") と devices（数 or リスト）。\n",
    "#  - Mac(Apple Silicon) は MPS、NVIDIA 環境は GPU、その他は CPU を使うのが自然。\n",
    "# =====================================================================\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 学習時にモデルの重みを保存する条件を指定\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",  # 監視対象メトリクス（validation_step で self.log される前提）\n",
    "    mode=\"min\",  # 最小化が良い（損失）\n",
    "    save_top_k=1,  # ベスト1件のみ保持\n",
    "    save_weights_only=True,  # 版差に注意（必要なら別途 state_dict も保存）\n",
    "    dirpath=\"model/\",  # 出力先ディレクトリ\n",
    "    filename=\"epoch={epoch}-val_loss={val_loss:.4f}\",  # 保存ファイル名に指標を埋め込む\n",
    ")\n",
    "\n",
    "\n",
    "# デバイス自動選択（PL 2.x 推奨の指定方法）\n",
    "def pick_accelerator_and_devices():\n",
    "    # Apple Silicon (Metal Performance Shaders)\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return \"mps\", 1\n",
    "    # CUDA GPU\n",
    "    if torch.cuda.is_available():\n",
    "        return \"gpu\", 1\n",
    "    # フォールバック：CPU\n",
    "    return \"cpu\", 1\n",
    "\n",
    "\n",
    "accelerator, devices = pick_accelerator_and_devices()\n",
    "\n",
    "# （任意）早期終了で過学習を抑制：val_loss が改善しなければ打ち切る\n",
    "early_stopping = pl.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "\n",
    "# 学習の方法を指定（PL 2.x 準拠）\n",
    "# - max_epochs=10：総エポック数（EarlyStopping と併用で実質の学習長を自動調整）\n",
    "# - callbacks：チェックポイントと早期終了を適用\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=accelerator,  # \"mps\"（Mac）/ \"gpu\"（NVIDIA）/ \"cpu\"\n",
    "    devices=devices,  # 1（単一デバイス）\n",
    "    max_epochs=10,\n",
    "    callbacks=[checkpoint, early_stopping],\n",
    "    # （必要に応じて）deterministic=True, gradient_clip_val=1.0, precision=16 等を追加\n",
    ")\n",
    "\n",
    "# 参考：学習呼び出し例（model は LightningModule、dataloader_* は 6-13 で作成）\n",
    "# trainer.fit(model, dataloader_train, dataloader_val)\n",
    "\n",
    "# 参考：学習後のロード\n",
    "# 最良 ckpt を LightningModule としてロード\n",
    "# model = BertForSequenceClassification_pl.load_from_checkpoint(\"model/epoch=..-val_loss=....ckpt\")\n",
    "# あるいは重みのみを別途保存・復元（版差対策）\n",
    "# torch.save(model.bert_sc.state_dict(), \"best_weights.pt\")\n",
    "# model.bert_sc.load_state_dict(torch.load(\"best_weights.pt\", map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678bab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-16\n",
    "# =====================================================================\n",
    "# 説明のコメント付き：LightningModule のインスタンス化と学習実行\n",
    "# ---------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - 6-14 で定義した LightningModule（BERT 文分類）を生成し，\n",
    "#    6-13 の DataLoader（train/val）で学習を走らせる。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - num_labels=9 は 6-11 の category_list の長さに一致（多クラス分類）。\n",
    "#  - lr=1e-5 は BERT 微調整としては保守的（2e-5〜5e-5 も一般的）。学習が進まない場合は微調整。\n",
    "#  - Lightning は Trainer 側の accelerator/devices 設定に基づき，テンソルを自動でデバイス転送する。\n",
    "#  - 6-15 の ModelCheckpoint / EarlyStopping を callbacks に渡していれば，\n",
    "#    val_loss 最小の ckpt が model/ に保存される（filename に val_loss を埋め込む設定にしておくと追跡が容易）。\n",
    "# =====================================================================\n",
    "\n",
    "# PyTorch Lightningモデルのロード（BERT + 分類ヘッド）\n",
    "# - MODEL_NAME：トークナイザと同一モデル名を使用（語彙IDの不一致を防ぐ）。\n",
    "# - num_labels=9：9カテゴリ分類（category_list と整合）。\n",
    "# - lr=1e-5：安定寄り。過学習/収束速度を見て調整（ヘッド大きめ，BERT小さめの層別LRも有効）。\n",
    "model = BertForSequenceClassification_pl(MODEL_NAME, num_labels=9, lr=1e-5)\n",
    "\n",
    "# ファインチューニングを実行\n",
    "# - trainer は 6-15 で PL 2.x 準拠（accelerator/devices）で構築済みの前提。\n",
    "# - fit() は（学習）→（検証）を各エポックで実行。validation_step の self.log('val_loss', ...) が\n",
    "#   ModelCheckpoint/EarlyStopping の monitor と一致していることが前提。\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "\n",
    "# （任意・推奨）学習後のベストモデルでテスト評価を実行する場合：\n",
    "# - 6-15 で EarlyStopping/Checkpoint を設定済みなら，ckpt_path=\"best\" で自動的に最良重みをロードして評価できる。\n",
    "# - dataloader_test は 6-13 で構築済みの前提。\n",
    "# trainer.test(model, dataloader_test, ckpt_path=\"best\")\n",
    "\n",
    "# （任意）学習後に重みを別途保存したい場合（バージョン差異対策）：\n",
    "# torch.save(model.bert_sc.state_dict(), \"best_weights.pt\")\n",
    "# 復元：model.bert_sc.load_state_dict(torch.load(\"best_weights.pt\", map_location=\"cpu\"))\n",
    "\n",
    "# 運用上の注意：\n",
    "# - 早期終了を入れている場合，max_epochs は上限としての意味（実際のエポック数は短くなる）。\n",
    "# - 指標は accuracy だけでなく macro F1 なども追加すると多クラスでの挙動を把握しやすい。\n",
    "# - 再現性が必要なら，分割 seed（6-13），Trainer(deterministic=True)，各種乱数 seed の固定を徹底。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95643ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-17\n",
    "best_model_path = checkpoint.best_model_path  # ベストモデルのファイル\n",
    "print(\"ベストモデルのファイル: \", checkpoint.best_model_path)\n",
    "print(\"ベストモデルの検証データに対する損失: \", checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3be11d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b9a4a928c71c0f00\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b9a4a928c71c0f00\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6-18\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-19\n",
    "test = trainer.test(dataloaders=dataloader_test)\n",
    "print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7270a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 6-20\n",
    "# PyTorch Lightningモデルのロード\n",
    "model = BertForSequenceClassification_pl.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# Transformers対応のモデルを./model_transformesに保存\n",
    "model.bert_sc.save_pretrained(\"./model_transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e9e8463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-21\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(\"./model_transformers\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
