{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d2277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-1\n",
    "!mkdir chap7\n",
    "%cd ./chap7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "062b1b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-3\n",
    "import random\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 日本語の事前学習モデル\n",
    "MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a2a5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-4（説明コメント付き：BERTを用いたマルチラベル文分類の最小実装）\n",
    "# --------------------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - 事前学習済み BERT（エンコーダ本体：BertModel）の出力（最終層隠れ状態）から\n",
    "#    「[PAD] を除いたトークン平均（mean pooling）」で文ベクトルを作り、\n",
    "#    線形層でラベル数（num_labels）次元に写像して **マルチラベル**分類を行う。\n",
    "#\n",
    "# 理論メモ（マルチラベル vs マルチクラス）：\n",
    "#  - マルチクラス（排他的1クラス）では softmax + CrossEntropyLoss を用いるが、\n",
    "#  - マルチラベル（複数ラベル同時に1の可能性）では **各ラベルを独立なベルヌーイ**として扱い、\n",
    "#    出力は raw logits（活性前）→ **BCEWithLogitsLoss**（内部で sigmoid + BCE を数値安定に計算）を用いる。\n",
    "#  - 教師ラベルの形：shape [B, C]、各要素は {0,1}（float型推奨）。しきい値 0.5 などで陽性判定を後段で行う。\n",
    "#\n",
    "# プーリング設計（mean pooling の意図）：\n",
    "#  - [CLS] ベクトル単独の利用に比べ、文全体の情報を平均で取り入れやすい。短文～中程度の長さで堅実なベースライン。\n",
    "#  - attention_mask により [PAD] 位置は平均から除外（分母は非PADトークン数）。\n",
    "#  - 分母ゼロ（全PAD）対策：学習データ生成で通常発生しない想定だが、実務では clamp などで保険をかけると安全。\n",
    "#\n",
    "# そのほか設計の注意：\n",
    "#  - token_type_ids（= segment ids）は文対（sentence pair）で区別に使うが、単文では 0 固定でもよい。\n",
    "#  - 出力は慣例的には transformers の `SequenceClassifierOutput` を使うが、\n",
    "#    ここでは attributes にアクセスできる簡易オブジェクトを返す（互換性が必要なら差し替え推奨）。\n",
    "#  - クラス不均衡が強い場合は BCEWithLogitsLoss の `pos_weight` を利用すると勾配が安定する。\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class BertForSequenceClassificationMultiLabel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        # BertModel のロード（分類ヘッドなしの本体のみ）。\n",
    "        # - 入力：input_ids, attention_mask, token_type_ids\n",
    "        # - 出力：last_hidden_state（[B, L, H]）など\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "        # 文ベクトル（プーリング後の [B, H]）から各ラベルのロジット [B, C] を直線写像する層。\n",
    "        # - H: 隠れ次元（bert.config.hidden_size）、C: ラベル数（num_labels）\n",
    "        self.linear = torch.nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None\n",
    "    ):\n",
    "        # 1) BERT でトークン列を符号化し、最終層の隠れ状態を得る。\n",
    "        #    last_hidden_state の形は [B, L, H]（B:バッチ、L:系列長、H:隠れ次元）\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        last_hidden_state = bert_output.last_hidden_state\n",
    "\n",
    "        # 2) [PAD] 以外の位置のみで平均プーリングして文ベクトルを作る。\n",
    "        #    - attention_mask は [B, L] の {0,1}。unsqueeze(-1) で [B, L, 1] に拡張し、\n",
    "        #      last_hidden_state（float）にマスク乗算→ PAD 位置を 0 化。\n",
    "        #    - 分母は非PADトークン数（attention_mask.sum(dim=1)）。\n",
    "        #      典型データでは 0 になることはないが、実運用では clamp_min(1e-9) などで安全策も検討。\n",
    "        averaged_hidden_state = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(\n",
    "            1\n",
    "        ) / attention_mask.sum(1, keepdim=True)\n",
    "        # 参考（安全策の例、実装切替の際に利用）：\n",
    "        # denom = attention_mask.sum(1, keepdim=True).clamp_min(1e-9).float()\n",
    "        # masked = last_hidden_state * attention_mask.unsqueeze(-1).float()\n",
    "        # averaged_hidden_state = masked.sum(1) / denom\n",
    "\n",
    "        # 3) 線形層でラベル数 C 次元のロジットへ（活性はかけない：BCEWithLogitsLoss を使うため）\n",
    "        scores = self.linear(averaged_hidden_state)  # shape: [B, C]\n",
    "\n",
    "        # 4) 出力を dict 形式で用意（logits を必須、labels が来ていれば loss も計算）\n",
    "        output = {\"logits\": scores}\n",
    "\n",
    "        # 5) 教師ラベルが与えられた場合は損失を計算して返す。\n",
    "        #    - BCEWithLogitsLoss は内部で sigmoid を組み合わせた数値安定版。\n",
    "        #    - labels 形状は [B, C]、型は float（{0.0, 1.0}）を想定。\n",
    "        #    - クラス不均衡が強ければ pos_weight（shape [C]）の指定を検討。\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(scores, labels.float())\n",
    "            output[\"loss\"] = loss\n",
    "\n",
    "        # 6) 呼び出し側が `output.logits` / `output.loss` で参照できるよう簡易オブジェクト化。\n",
    "        #    transformers の標準 `SequenceClassifierOutput` を使う場合は差し替え可。\n",
    "        output = type(\"bert_output\", (object,), output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b11ff6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using device = mps\n"
     ]
    }
   ],
   "source": [
    "# 7-5（説明コメント付き：マルチラベル文分類モデルのロードとデバイス配置）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - 日本語BERTと同一語彙のトークナイザをロードし、\n",
    "#  - 7-4で定義した「BertForSequenceClassificationMultiLabel」を初期化、\n",
    "#  - 環境に応じた最適デバイス（MPS/CUDA/CPU）へ安全に配置する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - 本モデルは出力が各ラベルの「ロジット」（活性前）で、マルチラベル想定（独立ベルヌーイ）。\n",
    "#    学習時は BCEWithLogitsLoss（sigmoid + BCE の安定版）を用いる。\n",
    "#  - num_labels=2 の場合、2つのラベルそれぞれを {0,1} で同時に予測する（softmaxではない）。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer\n",
    "\n",
    "# --- MODEL_NAME が未定義でも動くように保険を入れる ---\n",
    "try:\n",
    "    MODEL_NAME\n",
    "except NameError:\n",
    "    MODEL_NAME = (\n",
    "        \"tohoku-nlp/bert-base-japanese-whole-word-masking\"  # 東北大版日本語BERT（WWM）\n",
    "    )\n",
    "\n",
    "\n",
    "# --- デバイス選択：Mac(MPS) → CUDA → CPU の順で自動選択 ---\n",
    "def get_best_device() -> torch.device:\n",
    "    # Apple Silicon + macOS Metal\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    # NVIDIA GPU（Linux/Windowsなど）\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # フォールバック：CPU\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"[info] using device = {device}\")\n",
    "\n",
    "# --- トークナイザのロード（モデルと同じ語彙でないとIDがずれて壊れる） ---\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- マルチラベル分類モデルの構築 ---\n",
    "# num_labels=2：2つのラベルを独立に予測（例：{スポーツ, IT} の属否を同時に判定 など）\n",
    "bert_scml = BertForSequenceClassificationMultiLabel(MODEL_NAME, num_labels=2)\n",
    "\n",
    "# --- 最適デバイスへ配置（.cuda()固定はMacで失敗するため .to(device) を統一使用） ---\n",
    "bert_scml = bert_scml.to(device)\n",
    "\n",
    "# 推論時は Dropout を止めたいので eval()、学習開始時は train() を呼ぶ。\n",
    "# bert_scml.eval()   # 例：推論前\n",
    "# bert_scml.train()  # 例：学習ループ開始時\n",
    "\n",
    "# （参考）出力の扱い：\n",
    "# - forward は {'logits': Tensor[B, C], 'loss': Tensor[]} 相当を返す簡易オブジェクト。\n",
    "# - 学習時は labels=[B, C] を与えると BCEWithLogitsLoss を内部計算し output.loss に入る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "037ea7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# scores (logits): tensor([[0.2937, 0.1129],\n",
      "        [0.2349, 0.0266]], device='mps:0')\n",
      "# probs (sigmoid): tensor([[0.5729, 0.5282],\n",
      "        [0.5585, 0.5066]], device='mps:0')\n",
      "# predicted labels: tensor([[1, 1],\n",
      "        [1, 1]], device='mps:0')\n",
      "# subset accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# 7-6（説明コメント付き：マルチラベル推論・しきい値判定・サブセット精度）\n",
    "import torch\n",
    "\n",
    "# 入力テキスト：各サンプルに複数ラベルが立ち得る（マルチラベル）\n",
    "text_list = [\"今日の仕事はうまくいったが、体調があまり良くない。\", \"昨日は楽しかった。\"]\n",
    "\n",
    "# 教師ラベル（shape=[B, C]）：\n",
    "#  - マルチラベルなので各列（ラベル）を独立に {0,1} で表す\n",
    "#  - 例：C=2 のとき [仕事/体調] のような2軸を同時に予測するイメージ\n",
    "labels_list = [[1, 1], [0, 1]]\n",
    "\n",
    "# --- デバイス整合：モデル(bert_scml)の実デバイスに入力を合わせる ---\n",
    "device = next(bert_scml.parameters()).device\n",
    "\n",
    "# データの符号化\n",
    "#  - padding='longest' はバッチ内の最長系列に合わせて動的にPADを付与\n",
    "#  - 実運用では max_length+truncation=True+padding='max_length' で固定長化すると計算が安定\n",
    "encoding = tokenizer(text_list, padding=\"longest\", return_tensors=\"pt\")\n",
    "encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "# 教師ラベルテンソル\n",
    "#  - 学習で BCEWithLogitsLoss を使う場合は float（{0.0,1.0}）が望ましい\n",
    "#  - ここでは評価用に int 版も用意（比較演算時に便利）\n",
    "labels_float = torch.tensor(labels_list, dtype=torch.float32, device=device)\n",
    "labels_int = labels_float.to(torch.int64)\n",
    "\n",
    "# --- 推論：本モデルは logits（活性前スコア）を返す ---\n",
    "with torch.no_grad():\n",
    "    output = bert_scml(**encoding)\n",
    "scores = output.logits  # shape: [B, C]，各ラベルのロジット\n",
    "\n",
    "# --- 判定（しきい値） ---\n",
    "# 方式A：確率で判定（推奨。しきい値を柔軟に最適化できる）\n",
    "probs = scores.sigmoid()  # p(y_c=1|x)\n",
    "threshold = 0.5  # 必要に応じて ROC/PR で最適化\n",
    "labels_predicted = (probs >= threshold).to(torch.int64)\n",
    "\n",
    "# 方式B：ロジットの符号で判定（sigmoid≥0.5 と等価だが明示性に欠ける）\n",
    "# labels_predicted = (scores > 0).to(torch.int64)\n",
    "\n",
    "# --- 精度計算（subset accuracy：全ラベル一致率） ---\n",
    "#  - サンプルごとに全てのラベルが正解しているかを判定し、その平均をとる厳しめの指標\n",
    "num_correct = (labels_predicted == labels_int).all(dim=-1).sum().item()\n",
    "accuracy = num_correct / labels_int.size(0)\n",
    "\n",
    "# 参考：他の評価指標（マルチラベルで一般的）\n",
    "#  - example-based F1（サンプル単位のF1を平均）\n",
    "#  - micro/macro-F1（ラベル軸で集計）\n",
    "#  - Hamming loss（ラベル単位の誤り率）\n",
    "# これらは torchmetrics（MultilabelF1Score 等）で容易に算出可能\n",
    "\n",
    "print(\"# scores (logits):\", scores)\n",
    "print(\"# probs (sigmoid):\", probs)\n",
    "print(\"# predicted labels:\", labels_predicted)\n",
    "print(\"# subset accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a26f061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# loss: 0.671375036239624\n"
     ]
    }
   ],
   "source": [
    "# 7-7（説明コメント付き：マルチラベル損失（BCEWithLogitsLoss）の計算）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - トークナイズ結果にマルチラベル教師（[B, C]）を同梱し、モデルの forward で\n",
    "#    BCEWithLogitsLoss（sigmoid + BCE の数値安定版）を自動計算させて loss を得る。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - マルチラベルでは各クラスを独立なベルヌーイとみなし、出力はロジット（活性前）。\n",
    "#    教師は float 型の {0.0, 1.0} 行列（shape=[B, C]）。softmax は使わない。\n",
    "#  - BCEWithLogitsLoss は内部で sigmoid を合成するため、出力に sigmoid をかけずに\n",
    "#    生のロジットをそのまま渡すのが正しい。\n",
    "#  - クラス不均衡が強い場合は pos_weight（shape=[C]）を指定すると陽性側の損失寄与を補正できる。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "\n",
    "# --- デバイス整合：モデル(bert_scml)の実デバイスに入力を合わせる ---\n",
    "device = next(bert_scml.parameters()).device\n",
    "\n",
    "# --- データの符号化（バッチ内の最長に合わせて動的PAD） ---\n",
    "# 実運用では max_length+truncation=True+padding=\"max_length\" で固定長化すると計算が安定。\n",
    "encoding = tokenizer(text_list, padding=\"longest\", return_tensors=\"pt\")\n",
    "encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "# --- ラベルを同梱（[B, C]、float 型） ---\n",
    "# BCEWithLogitsLoss は float を前提。int のままでも内部で float 化されるが明示が安全。\n",
    "labels = torch.tensor(labels_list, dtype=torch.float32, device=device)\n",
    "encoding[\"labels\"] = labels\n",
    "\n",
    "# --- 損失の計算 ---\n",
    "# 学習時は train() にして Dropout を有効、評価時は eval() + no_grad()。\n",
    "bert_scml.train()  # 例：学習フェーズを想定（評価なら bert_scml.eval() と no_grad() を併用）\n",
    "output = bert_scml(**encoding)\n",
    "loss = output.loss  # BCEWithLogitsLoss（バッチ平均）\n",
    "\n",
    "print(\"# loss:\", float(loss.detach().cpu()))\n",
    "\n",
    "# （参考）不均衡対策：pos_weight を使う例（実装差し替え時）\n",
    "#   C = labels.size(1)\n",
    "#   pos_weight = torch.tensor([...], dtype=torch.float32, device=device)  # shape=[C]\n",
    "#   criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#   loss = criterion(output.logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d5d562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence_id': 0, 'sentence': '当期におけるわが国経済は、景気は緩やかな回復基調が続き、設備投資の持ち直し等を背景に企業収益は改善しているものの、海外では、資源国等を中心に不透明な状況が続き、為替が急激に変動するなど、依然として先行きが見通せない状況で推移した', 'opinions': [{'target': 'わが国経済', 'category': 'OOD#general', 'polarity': 'neutral', 'from': 6, 'to': 11}, {'target': '景気', 'category': 'OOD#general', 'polarity': 'positive', 'from': 13, 'to': 15}, {'target': '設備投資', 'category': 'OOD#general', 'polarity': 'positive', 'from': 28, 'to': 32}, {'target': '企業収益', 'category': 'OOD#general', 'polarity': 'positive', 'from': 42, 'to': 46}, {'target': '資源国等', 'category': 'OOD#general', 'polarity': 'neutral', 'from': 62, 'to': 66}, {'target': '為替', 'category': 'OOD#general', 'polarity': 'negative', 'from': 80, 'to': 82}]}\n"
     ]
    }
   ],
   "source": [
    "# 7-9\n",
    "data = json.load(open(\"chABSA-dataset/e00030_ann.json\"))\n",
    "print(data[\"sentences\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c030f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 6119\n",
      "# positives per class: {'neutral': 230, 'positive': 2210, 'negative': 1746}\n",
      "# example: {'text': '当連結会計年度におけるわが国経済は、政府の経済政策や日銀の金融緩和策により、企業業績、雇用・所得環境は改善し、景気も緩やかな回復基調のうちに推移いたしましたが、中国をはじめとするアジア新興国経済の減速懸念や、英国の欧州連合（ＥＵ）離脱決定、米国新政権への移行など、引き続き先行きは不透明な状況となっております', 'labels': [0, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# 7-10（説明コメント付き：chABSA からマルチラベル（negative/neutral/positive）データセットを構築）\n",
    "# --------------------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - chABSA の JSON 群（各ファイルに sentences が入っている）から、\n",
    "#    文テキストと極性ラベル（3値：negative/neutral/positive）を **マルチラベル** 形式で取り出す。\n",
    "#    * ABSA は 1 文中に複数の「意見（aspect-opinion）」が存在し得るため、\n",
    "#      文レベルでは negative/neutral/positive が**同時に立つ**可能性を想定（= マルチラベル）。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - ここで作る labels は長さ 3 の one-hot ではなく **multi-hot**（例： [1,0,1]）。\n",
    "#    後段の学習では BCEWithLogitsLoss（独立ベルヌーイ）を用いるのが標準。\n",
    "#  - マルチクラス（排他的 1 ラベル）で扱いたい場合は、「優先順位ルール」や\n",
    "#    「確率最大クラスを1つ選ぶ」などの単一化手続きを別途設ける。\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# 極性 → ラベルID の対応（ID順は学習・評価で再利用するため**固定**しておく）\n",
    "category_id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "id_to_category = {v: k for k, v in category_id.items()}  # デバッグ・可視化用の逆引き\n",
    "\n",
    "dataset = []  # 出力：[{ 'text': <str>, 'labels': [neg, neu, pos] }, ...] の配列\n",
    "\n",
    "# chABSA の JSON を列挙（ソートしておくと再現性が増す）\n",
    "for file in sorted(glob.glob(\"chABSA-dataset/*.json\")):\n",
    "    # JSON をロード（UTF-8 を想定）\n",
    "    with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 各ファイルは {\"sentences\": [...]} の構造を想定\n",
    "    # sentence 例：\n",
    "    #   {\n",
    "    #     \"sentence\": \"・・・\",\n",
    "    #     \"opinions\": [\n",
    "    #        {\"polarity\": \"positive\", ...},\n",
    "    #        {\"polarity\": \"negative\", ...},\n",
    "    #        ...\n",
    "    #     ]\n",
    "    #   }\n",
    "    for sentence in data.get(\"sentences\", []):\n",
    "        text = sentence.get(\"sentence\", \"\")\n",
    "\n",
    "        # 文レベルのマルチラベルベクトル（[neg, neu, pos]）\n",
    "        # - 同一文に複数 opinion がある場合、それぞれの極性に 1 を立てる（重複は 1 のまま）\n",
    "        labels = [0, 0, 0]\n",
    "        for opinion in sentence.get(\"opinions\", []):\n",
    "            pol = opinion.get(\"polarity\")\n",
    "            if pol in category_id:\n",
    "                labels[category_id[pol]] = 1\n",
    "            else:\n",
    "                # 未知の極性が来た場合の保険（データ品質に依存）\n",
    "                # 実務ではログ出力・スキップ数カウント等の監視を入れるとよい\n",
    "                pass\n",
    "\n",
    "        # 1 サンプル（テキスト + マルチラベル）を追加\n",
    "        sample = {\"text\": text, \"labels\": labels}\n",
    "        dataset.append(sample)\n",
    "\n",
    "# ------------------（任意）簡易サニティチェックと統計------------------\n",
    "# ラベル出現数（クラス別の陽性総数）を集計してみる\n",
    "label_totals = Counter()\n",
    "for s in dataset:\n",
    "    for i, bit in enumerate(s[\"labels\"]):\n",
    "        if bit:\n",
    "            label_totals[id_to_category[i]] += 1\n",
    "\n",
    "print(f\"# samples: {len(dataset)}\")\n",
    "print(\"# positives per class:\", dict(label_totals))\n",
    "print(\"# example:\", dataset[0] if dataset else None)\n",
    "\n",
    "# ここで得られた `dataset` は、後段の符号化フェーズで\n",
    "# tokenizer(text, ...) → input_ids/attention_mask/... を作り、\n",
    "# labels を torch.tensor([B, 3]) の float 型（{0.0,1.0}）にして BCEWithLogitsLoss へ渡す想定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd2270d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '当連結会計年度におけるわが国経済は、政府の経済政策や日銀の金融緩和策により、企業業績、雇用・所得環境は改善し、景気も緩やかな回復基調のうちに推移いたしましたが、中国をはじめとするアジア新興国経済の減速懸念や、英国の欧州連合（ＥＵ）離脱決定、米国新政権への移行など、引き続き先行きは不透明な状況となっております', 'labels': [0, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# 7-11\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d94a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-12\n",
    "# =============================================================================\n",
    "# 説明コメント付き：chABSA 由来データの符号化（固定長化）→ テンソル化 → 分割 → DataLoader 作成\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - 文テキストをトークナイズし、BERT へ入力可能な辞書（input_ids/attention_mask/...）＋ labels を作る\n",
    "#  - 学習/検証/テストに 60/20/20 で分割し、ミニバッチを供給する DataLoader を用意する\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - BERT 系の学習は「**固定長化**（max_length + truncation + padding='max_length'）」で\n",
    "#    バッチ形状を揃えると、スループットと再現性が安定しやすい\n",
    "#  - 本タスクは**マルチラベル**（multi-hot）想定：後段は BCEWithLogitsLoss（内部で sigmoid 合成）\n",
    "#    → labels は float（{0.0, 1.0}）が望ましい\n",
    "#  - 分割は単純ランダムだとクラス/マルチラベル分布が歪むことがある\n",
    "#    → 研究では「層化（Stratified / Iterative Stratification）」や\n",
    "#       「グループ分割（ファイル単位）」が推奨（ここでは最小構成としてランダム）\n",
    "#  - 再現性が必要なら、事前に random.seed(SEED) を固定しておく\n",
    "# =============================================================================\n",
    "\n",
    "# トークナイザのロード（MODEL_NAME は 7-5 等で定義済み：語彙ID整合のため同一モデル名を使う）\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 各データの形式を整える（固定長化）\n",
    "max_length = 128\n",
    "dataset_for_loader = []\n",
    "for sample in dataset:\n",
    "    text = sample[\"text\"]  # 文テキスト\n",
    "    labels = sample[\"labels\"]  # multi-hot（例：[neg, neu, pos]）\n",
    "\n",
    "    # 1) トークナイズ：固定長化（長文は切り詰め、短文は PAD で埋める）\n",
    "    encoding = tokenizer(\n",
    "        text, max_length=max_length, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "\n",
    "    # 2) ラベル付与（BCEWithLogitsLoss を前提に float 化するのが望ましい）\n",
    "    encoding[\"labels\"] = labels\n",
    "\n",
    "    # 3) テンソル化\n",
    "    #  - tokenizer の戻り値（list[int]）→ torch.long（int64）へ\n",
    "    #  - labels は明示的に float32（{0.0, 1.0}）へ（int でも後段で float 化されるが、ここで揃えると安全）\n",
    "    encoding = {k: torch.tensor(v) for k, v in encoding.items() if k != \"labels\"}\n",
    "    encoding[\"labels\"] = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    dataset_for_loader.append(encoding)\n",
    "\n",
    "# データセットの分割（60/20/20）\n",
    "# - 単純ランダム：クラス不均衡やデータリークに注意（必要に応じて層化/グループ分割に置換）\n",
    "random.shuffle(dataset_for_loader)\n",
    "n = len(dataset_for_loader)\n",
    "n_train = int(0.6 * n)\n",
    "n_val = int(0.2 * n)\n",
    "dataset_train = dataset_for_loader[:n_train]  # 学習データ\n",
    "dataset_val = dataset_for_loader[n_train : n_train + n_val]  # 検証データ\n",
    "dataset_test = dataset_for_loader[n_train + n_val :]  # テストデータ\n",
    "\n",
    "# データローダを作成\n",
    "# - 学習のみ shuffle=True（SGD のバイアス低減）\n",
    "# - val/test は順序固定で OK（再現性・デバッグのため）\n",
    "# - バッチサイズは VRAM/メモリと相談（val/test は no_grad 前提で大きく取りやすい）\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=256, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=256, shuffle=False)\n",
    "\n",
    "# 補足：\n",
    "# - DataLoader は辞書の各キーごとにテンソルをバッチ結合（collate_fn 既定）するため、\n",
    "#   上記の dict 形式（input_ids/attention_mask/token_type_ids/labels）のままで学習に投入可能\n",
    "# - BCEWithLogitsLoss の pos_weight（shape=[C]）で不均衡補正を入れると安定しやすい\n",
    "# - グループ分割（例：source_file 単位）を採用するなら、7-10 で sample にメタを持たせるとよい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1303b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-13（修正＆説明コメント付き：PL 2.x の fit 引数名を正しく指定）\n",
    "# =====================================================================\n",
    "# エラー原因：\n",
    "# - PyTorch Lightning 2.x の Trainer.fit は `dataloaders=` を受け付けず、\n",
    "#   学習データは `train_dataloaders=...`、検証データは `val_dataloaders=...` を使う。\n",
    "#   → `TypeError: Trainer.fit() got an unexpected keyword argument 'dataloaders'`\n",
    "#     はこの非対応引数名が原因。\n",
    "# 対処：\n",
    "# - `trainer.fit(model, train_dataloaders=dataloader_train, val_dataloaders=dataloader_val)`\n",
    "#   に修正する。\n",
    "# ついでに：\n",
    "# - Mac(MPS)/CUDA/CPU 自動選択（accelerator/devices）\n",
    "# - ログの on_epoch 明示化\n",
    "# - test_step で batch を破壊しないようにコピーして使用\n",
    "# =====================================================================\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# --- 7-4で定義済み ---\n",
    "# class BertForSequenceClassificationMultiLabel(torch.nn.Module): ...\n",
    "\n",
    "\n",
    "class BertForSequenceClassificationMultiLabel_pl(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        \"\"\"\n",
    "        model_name: 事前学習BERT名（tokenizer と同一語彙を使うこと）\n",
    "        num_labels: マルチラベルの次元数（例：3 → [neg, neu, pos]）\n",
    "        lr       : 学習率（BERT微調整の一般的レンジは 1e-5〜5e-5）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()  # ハイパーパラメータを保存（再現性・ckpt再現に有用）\n",
    "\n",
    "        # BERT本体 + mean-pooling + Linear → 各ラベルのロジット（活性前）\n",
    "        self.bert_scml = BertForSequenceClassificationMultiLabel(\n",
    "            model_name, num_labels=num_labels\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # labels を含む dict をそのまま渡すと BCEWithLogitsLoss を内部計算（7-4のforward）\n",
    "        output = self.bert_scml(**batch)\n",
    "        loss = output.loss\n",
    "        # エポック平均・プログレスバー表示を明示\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_scml(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # 破壊的に pop しない（後段のフックや再利用に安全）\n",
    "        labels = batch[\"labels\"]  # [B, C]（float 0/1 想定）\n",
    "        inputs = {k: v for k, v in batch.items() if k != \"labels\"}\n",
    "        output = self.bert_scml(**inputs)\n",
    "        scores = output.logits  # [B, C] ロジット\n",
    "        probs = scores.sigmoid()  # p(y_c=1|x)\n",
    "        labels_pred = (probs >= 0.5).to(torch.int64)  # 閾値 τ=0.5\n",
    "        labels_int = labels.to(torch.int64)\n",
    "        # subset accuracy（全ラベル一致率：厳しめの指標）\n",
    "        num_correct = (labels_pred == labels_int).all(dim=-1).sum().item()\n",
    "        accuracy = num_correct / scores.size(0)\n",
    "        self.log(\"accuracy\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # 最小構成：Adam（実務は AdamW + weight_decay + scheduler 推奨）\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "\n",
    "# ============================== コールバック & Trainer ==============================\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,  # 版差あり。必要なら state_dict で別保存も併用\n",
    "    dirpath=\"model/\",\n",
    "    filename=\"epoch={epoch}-val_loss={val_loss:.4f}\",\n",
    ")\n",
    "\n",
    "early_stopping = pl.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\n",
    "\n",
    "\n",
    "def pick_accelerator_and_devices():\n",
    "    # Mac(Apple Silicon) 優先 → CUDA → CPU\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return \"mps\", 1\n",
    "    if torch.cuda.is_available():\n",
    "        return \"gpu\", 1\n",
    "    return \"cpu\", 1\n",
    "\n",
    "\n",
    "accelerator, devices = pick_accelerator_and_devices()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=accelerator,  # \"mps\"/\"gpu\"/\"cpu\"（または \"auto\" でも可）\n",
    "    devices=devices,  # 単一デバイス\n",
    "    max_epochs=5,\n",
    "    callbacks=[checkpoint, early_stopping],\n",
    "    # 任意：deterministic=True, gradient_clip_val=1.0, precision=16 など\n",
    ")\n",
    "\n",
    "# ============================== 学習・テスト実行 ==============================\n",
    "\n",
    "model = BertForSequenceClassificationMultiLabel_pl(\n",
    "    MODEL_NAME, num_labels=3, lr=1e-5  # [neg, neu, pos]\n",
    ")\n",
    "\n",
    "# 【FIX】PL 2.x の正しい引数名：train_dataloaders / val_dataloaders\n",
    "trainer.fit(model, train_dataloaders=dataloader_train, val_dataloaders=dataloader_val)\n",
    "\n",
    "# テストは dataloaders=... でOK（PL 2.x 仕様）\n",
    "test = trainer.test(model, dataloaders=dataloader_test, ckpt_path=\"best\")\n",
    "print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c806e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-14（説明コメント付き：学習済みマルチラベルBERTで新規テキストを推論）\n",
    "# =============================================================================\n",
    "# 目的：\n",
    "#  - 7-13 の学習で保存された「最良 ckpt」をロードし、任意の文章に対して\n",
    "#    マルチラベル（例： [neg, neu, pos] ）の予測を行う。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - 本モデルは logits（活性前スコア）を出力する。確率化には sigmoid を用い、\n",
    "#    しきい値 τ（既定 0.5）で各ラベルの 0/1 を決める（独立ベルヌーイ仮定）。\n",
    "#  - 推論時は Dropout を停止するため eval()、かつ勾配不要の no_grad() を使う。\n",
    "#  - デバイスは MPS（Mac）→ CUDA → CPU の順で自動選択し、入出力テンソルとモデルを揃える。\n",
    "#  - トークナイズは本来、学習時と同じ固定長化（max_length + truncation + padding='max_length'）\n",
    "#    が望ましいが、ここでは可読性を優先して padding='longest' を利用（小規模推論向け）。\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer\n",
    "\n",
    "# --- ラベルIDの定義（7-10 と整合） ---\n",
    "category_id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "id_to_category = {v: k for k, v in category_id.items()}\n",
    "\n",
    "# --- 入力する文章 ---\n",
    "text_list = [\n",
    "    \"今期は売り上げが順調に推移したが、株価は低迷の一途を辿っている。\",\n",
    "    \"昨年から黒字が減少した。\",\n",
    "    \"今日の飲み会は楽しかった。\",\n",
    "]\n",
    "\n",
    "\n",
    "# --- デバイス選択：Mac(MPS) → CUDA → CPU の順 ---\n",
    "def get_best_device() -> torch.device:\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"[info] device = {device}\")\n",
    "\n",
    "# --- トークナイザのロード（学習時と同一 MODEL_NAME を使用） ---\n",
    "try:\n",
    "    MODEL_NAME\n",
    "except NameError:\n",
    "    MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "# --- 最良 ckpt のパスを取得 ---\n",
    "# 7-13 の checkpoint.best_model_path が生きていればそれを使い、\n",
    "# 無ければ 'model/' ディレクトリから新しい .ckpt を探索して拾う。\n",
    "def resolve_best_ckpt():\n",
    "    # 1) checkpoint.best_model_path がグローバルに存在する場合\n",
    "    if \"checkpoint\" in globals():\n",
    "        try:\n",
    "            p = checkpoint.best_model_path\n",
    "            if isinstance(p, str) and os.path.exists(p):\n",
    "                return p\n",
    "        except Exception:\n",
    "            pass\n",
    "    # 2) フォールバック：model/ 以下の .ckpt を更新時刻順で最後の1件\n",
    "    cks = sorted(glob.glob(\"model/*.ckpt\"), key=os.path.getmtime)\n",
    "    if not cks:\n",
    "        raise FileNotFoundError(\n",
    "            \"最良 ckpt が見つかりません（model/*.ckpt が存在しません）。\"\n",
    "        )\n",
    "    return cks[-1]\n",
    "\n",
    "\n",
    "best_model_path = resolve_best_ckpt()\n",
    "print(f\"[info] best ckpt = {best_model_path}\")\n",
    "\n",
    "# --- LightningModule を ckpt から復元し、中の bert_scml（純PyTorchモジュール）を取り出す ---\n",
    "# 7-13 の LightningModule は self.save_hyperparameters() 済みのため、追加引数なしで復元可。\n",
    "model = BertForSequenceClassificationMultiLabel_pl.load_from_checkpoint(best_model_path)\n",
    "bert_scml = model.bert_scml.to(device).eval()  # 推論モードに切替\n",
    "\n",
    "# --- データの符号化 ---\n",
    "# 小規模推論のため padding='longest' を使う（大量推論は固定長化の方が安定）。\n",
    "encoding = tokenizer(text_list, padding=\"longest\", return_tensors=\"pt\")\n",
    "encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "# --- 予測（logits → sigmoid → 閾値判定） ---\n",
    "with torch.no_grad():\n",
    "    output = bert_scml(**encoding)\n",
    "scores = output.logits  # [B, C]（活性前）\n",
    "probs = scores.sigmoid()  # [B, C]（各ラベルの確率）\n",
    "threshold = 0.5\n",
    "labels_predicted = (probs >= threshold).to(torch.int64).cpu().tolist()\n",
    "\n",
    "# --- 結果を表示（ラベル名に変換） ---\n",
    "print(\"\\n# 推論結果\")\n",
    "for text, label_vec in zip(text_list, labels_predicted):\n",
    "    active = [id_to_category[i] for i, v in enumerate(label_vec) if v == 1]\n",
    "    print(\"--\")\n",
    "    print(f\"入力：{text}\")\n",
    "    print(f\"出力（multi-hot） ：{label_vec}\")\n",
    "    print(f\"出力（ラベル名） ：{active}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 補足（実務向けの推奨）：\n",
    "#  - 学習と推論でトークナイズ設定（max_length など）を揃える。\n",
    "#  - 閾値 0.5 は便宜的。検証セットで PR/ROC に基づき τ を最適化するか、\n",
    "#    ラベル別に τ_c を持たせると性能が上がりやすい。\n",
    "#  - クラス不均衡が強い場合、学習時に BCEWithLogitsLoss(pos_weight=...) を導入。\n",
    "#  - しきい値最適化後は、micro/macro-F1・Hamming loss などを併記すると挙動把握が容易。\n",
    "# ============================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
