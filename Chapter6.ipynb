{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6cba954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maton/BERT_Practice/chap6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maton/.pyenv/versions/3.10.13/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# 6-1\n",
    "!mkdir chap6\n",
    "%cd ./chap6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a66273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-3\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 日本語の事前学習モデル\n",
    "MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "331e402d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using device = mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6-4（説明コメント付き：日本語BERTによる二値分類のセットアップ）\n",
    "# =====================================================================\n",
    "# 目的：\n",
    "#  - 事前学習済みの日本語BERT（MODEL_NAME）を、Sequence Classification（文分類）タスク用ヘッド付きでロードし、\n",
    "#    デバイス（MacのMPS / CUDA / CPU）に配置する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - BertForSequenceClassification は BERT 本体の上に「分類ヘッド（Dropout→Linear）」を載せたモデルである。\n",
    "#  - 推論時の出力 logits は形状 [B, num_labels]。num_labels=2 のとき、\n",
    "#    softmax(logits) によりクラス確率（2クラス）が得られる（argmax で予測ラベル）。\n",
    "#  - 学習時の損失は入力ラベルと num_labels により自動選択される（HFの既定）：\n",
    "#      * num_labels == 1 かつラベルが float       → 回帰（MSELoss）\n",
    "#      * num_labels > 1 かつラベルが整数カテゴリ → 単一ラベル分類（CrossEntropyLoss）\n",
    "#      * それ以外（ラベルが multi-hot 等）      → マルチラベル（BCEWithLogitsLoss）\n",
    "#  - BERT の文表現は概ね [CLS] トークンの隠れ状態（最終層）を用い、分類ヘッドで写像する設計（プール層の出力を使う実装もある）。\n",
    "#  - トークナイザは [CLS]/[SEP]/[PAD] を自動付与（既定）し、attention_mask でPADを無視する。\n",
    "# =====================================================================\n",
    "\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "\n",
    "\n",
    "# --- デバイス選択（Mac優先：MPS → CUDA → CPU） ---\n",
    "def get_best_device() -> torch.device:\n",
    "    # Apple Silicon + macOS Metal（CUDAは不可）\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    # 他環境（Linux/Windows等）ではCUDAを優先\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # それ以外はCPU\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"[info] using device = {device}\")\n",
    "\n",
    "# --- モデル名の決定（未定義時の保険） ---\n",
    "try:\n",
    "    MODEL_NAME\n",
    "except NameError:\n",
    "    # WWM版：語単位での一貫性を意識したマスキングで事前学習（日本語に適した設定）\n",
    "    MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "\n",
    "# --- トークナイザのロード ---\n",
    "# 形態素解析→WordPieceの二段分割。語彙はモデルとペアで一致させる（IDずれ防止）。\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- 文分類モデル（2クラス）をロード ---\n",
    "# num_labels=2：二値分類（例：ネガ/ポジ、真/偽 など）\n",
    "# 出力 logits 形状：[B, 2]。学習時にラベルが整数（0/1）なら CrossEntropyLoss が自動選択される。\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "# --- モデルを最適デバイスへ配置 ---\n",
    "# 元コード：bert_sc = bert_sc.cuda()\n",
    "# → Mac の場合 .cuda() は不可。統一して .to(device) を用いる。\n",
    "bert_sc = bert_sc.to(device)\n",
    "\n",
    "# --- 推論時の基本姿勢 ---\n",
    "# 評価モードにして Dropout を停止（推論の安定化）。学習時は bert_sc.train() に切り替える。\n",
    "bert_sc.eval()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 参考：推論の最小例（コメント解除で動作確認）\n",
    "# ---------------------------------------------------------------------\n",
    "# texts = [\"今日は自然言語処理を学ぶ。\", \"この映画は退屈だった。\"]\n",
    "# enc = tokenizer(\n",
    "#     texts,\n",
    "#     max_length=128,           # 学習・評価で固定長を揃えると効率的（pad_to_max_length 相当）\n",
    "#     padding=\"max_length\",\n",
    "#     truncation=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# )\n",
    "# enc = {k: v.to(device) for k, v in enc.items()}\n",
    "# with torch.no_grad():\n",
    "#     logits = bert_sc(**enc).logits           # [B, 2]\n",
    "#     probs  = logits.softmax(dim=-1)          # クラス確率\n",
    "#     preds  = probs.argmax(dim=-1)            # 予測クラスID（0/1）\n",
    "# print(\"probs:\", probs.detach().cpu().numpy())\n",
    "# print(\"preds:\", preds.detach().cpu().tolist())\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 運用上の注意（理論と実務の橋渡し）\n",
    "# ---------------------------------------------------------------------\n",
    "# - 前処理の一貫性：学習・評価で tokenizer の設定（max_length, padding, truncation）が一致していること。\n",
    "# - ラベル整合性：データセット内で 0/1 とクラス名のマッピングを厳密に固定（モデル保存時に一緒に管理）。\n",
    "# - 不均衡データ：クラス不均衡が強い場合は、重み付き損失（class_weight）や閾値最適化で改善。\n",
    "# - 再現性：random seed、モデルバージョン、辞書バージョンを固定。デバイス差（MPS/CUDA/CPU）で微差が出る点は許容範囲で管理。\n",
    "# - 転移学習：タスクに合わせて最終ヘッドのみならずBERT側も微調整（学習率は本体を小さく、ヘッドを大きくするのが通例）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2330781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# scores (shape):\n",
      "torch.Size([3, 2])\n",
      "# predicted labels:\n",
      "tensor([1, 1, 1], device='mps:0')\n",
      "# accuracy:\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# 6-5（説明コメント付き：二値文分類のバッチ推論と精度計算）\n",
    "# =====================================================================\n",
    "# 目的：\n",
    "#  - 日本語BERTの文分類モデル（bert_sc; num_labels=2）を用いて、複数文の推論を一括で行い、\n",
    "#    予測ラベルと単純精度（accuracy）を計算・表示する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - BertForSequenceClassification の出力 logits 形状は [B, num_labels]。\n",
    "#    num_labels=2 のとき argmax(logits) は「2クラスsoftmaxの最大事後確率クラス」と一致する。\n",
    "#  - tokenizer(..., padding='longest') はバッチ内の最長系列に合わせてPADを付与する\n",
    "#    “動的パディング”。attention_mask により PAD 位置は Self-Attention から無視される。\n",
    "#  - 学習時はラベル（0/1）が整数クラスなら CrossEntropyLoss が自動選択される（HFの既定）。\n",
    "#    本セルは推論のみ（no_grad & eval）。\n",
    "#  - デバイス（MPS/CUDA/CPU）は“モデルが現在置かれている device”に合わせて入力テンソルを移動する。\n",
    "#    （Mac は通常 MPS。`.cuda()` は不可なので `.to(device)` を一貫して用いる。）\n",
    "# =====================================================================\n",
    "\n",
    "text_list = [\n",
    "    \"この映画は面白かった。\",  # 正例のつもり（1）\n",
    "    \"この映画の最後にはがっかりさせられた。\",  # 負例のつもり（0）\n",
    "    \"この映画を見て幸せな気持ちになった。\",  # 正例のつもり（1）\n",
    "]\n",
    "label_list = [1, 0, 1]  # 教師ラベル（0=negative, 1=positive の想定）\n",
    "\n",
    "# --- データの符号化（トークン化→ID化） ---\n",
    "# padding='longest'：バッチ内の最長文の長さに合わせてPADを付与（動的パディング）。\n",
    "# return_tensors='pt'：PyTorchテンソルで返す（input_ids/attention_mask/token_type_ids など）。\n",
    "# ＊長文や学習時は max_length と truncation=True を併用して長さ上限を明示するのが安定。\n",
    "encoding = tokenizer(text_list, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "# --- デバイスをモデルに揃える（MPS/CUDA/CPU いずれでも動く書き方） ---\n",
    "# bert_sc は 6-4 で .to(device) 済みの前提。ここではその device を参照して入力を移す。\n",
    "device = next(bert_sc.parameters()).device\n",
    "encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "# ラベルテンソル作成\n",
    "# dtype=torch.long：分類（CE Loss）のクラスIDは long 整数で表現するのが通例。\n",
    "labels = torch.tensor(label_list, dtype=torch.long, device=device)\n",
    "\n",
    "# --- 推論（評価モード＋勾配無効） ---\n",
    "# eval()：Dropout等を停止して推論安定化。6-4で eval 済みでも冪等。\n",
    "bert_sc.eval()\n",
    "with torch.no_grad():\n",
    "    # モデル呼び出し：.forward(**encoding) と bert_sc(**encoding) は等価（後者が慣用的）\n",
    "    output = bert_sc(**encoding)\n",
    "\n",
    "# --- 出力の解釈と評価 ---\n",
    "scores = output.logits  # 形状 [B, 2]：各クラスのスコア（ロジット）\n",
    "labels_predicted = scores.argmax(-1)  # 予測クラスID（最尤クラス）\n",
    "num_correct = (labels_predicted == labels).sum().item()  # 正解数（スカラー）\n",
    "accuracy = num_correct / labels.size(0)  # 単純精度（micro accuracy）\n",
    "\n",
    "# --- 可視出力（形状と予測・精度） ---\n",
    "print(\"# scores (shape):\")\n",
    "print(scores.size())  # 例：torch.Size([3, 2])\n",
    "print(\"# predicted labels:\")\n",
    "print(labels_predicted)  # 例：tensor([1, 0, 1], device=..., dtype=torch.int64)\n",
    "print(\"# accuracy:\")\n",
    "print(accuracy)  # 例：1.0\n",
    "\n",
    "# 運用上の注意：\n",
    "# - データ分布が偏っている場合、単純精度だけでは不十分。適宜、適合率/再現率/F1、混同行列を併用する。\n",
    "# - 推論の閾値調整が必要な場合（特にコスト非対称なタスク）は、2クラスsoftmaxの確率を取得して閾値最適化を行う。\n",
    "# - 学習と評価で tokenizer の前処理（max_length/padding/truncation）を一致させること。\n",
    "# - デバイス差（MPS/CUDA/CPU）でごく僅かな数値差が出る場合があるが、多くは統計誤差の範囲。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e75180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5852, device='mps:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 6-6（説明コメント付き：ラベル付き入力で損失を計算）\n",
    "# =====================================================================\n",
    "# 目的：\n",
    "#  - 3本のテキストを一括でトークン化し、ラベル（0/1）を入力に同梱して\n",
    "#    BertForSequenceClassification が内部で計算する損失（loss）を取得して表示する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - Hugging Face の BertForSequenceClassification は、forward(**batch) に\n",
    "#    'labels' を渡すと自動で損失を計算して返す。\n",
    "#    * num_labels=2 かつ labels が整数クラス（dtype: long）の場合 → CrossEntropyLoss\n",
    "#    * 出力 logits の形状は [B, num_labels]。loss はバッチ平均のスカラー。\n",
    "#  - tokenizer(..., padding='longest') は“動的パディング”（バッチ内の最長系列に合わせてPAD）。\n",
    "#    attention_mask により PAD 位置の注意は抑制される。\n",
    "#  - デバイス整合：モデル（bert_sc）の置かれている device（MPS/CUDA/CPU）に\n",
    "#    入力テンソルを合わせること（.to(device)）。Mac では .cuda() は不可。\n",
    "# =====================================================================\n",
    "\n",
    "# 符号化（トークン化→ID化）。長さはバッチ内最長に合わせてPAD（学習では max_length+truncation推奨）。\n",
    "encoding = tokenizer(text_list, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "# ラベルを入力 dict に追加。\n",
    "# - CrossEntropyLoss の前提として dtype は long（整数クラスID）。\n",
    "# - デバイス移動はこの後まとめて行うので、ここではCPUでOK。\n",
    "encoding[\"labels\"] = torch.tensor(label_list, dtype=torch.long)\n",
    "\n",
    "# デバイス整合：モデルの実デバイスへ（Macは多くの場合 'mps'）。\n",
    "device = next(bert_sc.parameters()).device\n",
    "encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "# ロスの計算\n",
    "# - labels を渡しているため、出力に loss が含まれる。\n",
    "# - 学習（逆伝播）するなら bert_sc.train() にして、loss.backward() → optimizer.step() を行う。\n",
    "#   ここでは損失値の確認のみ。\n",
    "output = bert_sc(**encoding)\n",
    "loss = output.loss  # スカラー損失（バッチ平均）\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# 補足（運用上の注意）：\n",
    "# - 動的パディングはバッチごとに系列長が変わるため、学習ループでは DataCollator で揃えるか、\n",
    "#   max_length + truncation + padding='max_length' で固定長にすると効率が安定する。\n",
    "# - クラス不均衡が大きい場合、重み付き損失や閾値最適化、評価指標（F1等）の併用が有効。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d5bca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
