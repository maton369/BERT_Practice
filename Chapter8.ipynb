{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a1aa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maton/BERT_Practice/chap8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maton/.pyenv/versions/3.10.13/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# 8-1\n",
    "!mkdir chap8\n",
    "%cd ./chap8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba06fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-3\n",
    "import itertools\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForTokenClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 日本語学習済みモデル\n",
    "MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8cf15c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ＡＢＣ -> ABC\n",
      "ABC -> ABC\n",
      "１２３ -> 123\n",
      "123 -> 123\n",
      "アイウ -> アイウ\n",
      "ｱｲｳ -> アイウ\n"
     ]
    }
   ],
   "source": [
    "# 8-4（説明コメント付き：Unicode 正規化 NFKC による表記ゆれの吸収）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - Unicode の **互換性分解 + 合成**（NFKC）で、全角/半角・互換文字（例：半角ｶﾅ・ローマ数字・① 等）\n",
    "#    を正規化し、検索/重複排除/機械学習前処理での表記ゆれを減らす。\n",
    "#\n",
    "# 理論メモ（NFC/NFD/NFKC/NFKD の違い）：\n",
    "#  - NFC: 正規分解 → 合成（互換性は無視）。見た目を保ちながら正規の合成形式へ。\n",
    "#  - NFD: 正規分解のみ（結合文字にバラす）。検索・照合の下処理に使うことがある。\n",
    "#  - NFKC: **互換分解**（見た目は同じでも “意味的に同一視される” 文字を分解）→ 合成。\n",
    "#          例：全角英数/記号、半角ｶﾅ、ローマ数字 Ⅳ、丸数字 ① などを通常の文字に畳み込む。\n",
    "#  - NFKD: 互換分解のみ。\n",
    "# 互換分解は “情報の落ち” が起こり得る（例：①→1、㍍→メートル→m など）。監査用途では注意。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "# NFKC で正規化する関数\n",
    "# - 全角英数→半角、半角ｶﾅ→全角カタカナ、結合記号の統合 等を一括で行う\n",
    "normalize = lambda s: unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "# 動作例：全角/半角の統一（学習前の前処理・検索キー作成などで有効）\n",
    "print(f'ＡＢＣ -> {normalize(\"ＡＢＣ\")}')  # 全角アルファベット → 半角 \"ABC\"\n",
    "print(f'ABC -> {normalize(\"ABC\")}')  # 既に半角 → 変化なし\n",
    "print(f'１２３ -> {normalize(\"１２３\")}')  # 全角数字 → 半角 \"123\"\n",
    "print(f'123 -> {normalize(\"123\")}')  # 既に半角 → 変化なし\n",
    "print(f'アイウ -> {normalize(\"アイウ\")}')  # 全角カタカナ → 変化なし\n",
    "print(f'ｱｲｳ -> {normalize(\"ｱｲｳ\")}')  # 半角ｶﾅ → 全角カタカナ \"アイウ\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 追加の知見（必要ならテストして確認）：\n",
    "#  - 濁点付き半角ｶﾅ（例：ｶﾞ）も結合が解決され全角「ガ」に統一される：\n",
    "#      normalize(\"ｶﾞ\") == \"ガ\"\n",
    "#  - 互換文字の折り畳み：\n",
    "#      normalize(\"ⅠⅡⅢ\") -> \"III\"  （ローマ数字 → ラテン大文字）\n",
    "#      normalize(\"①②\")   -> \"12\"   （丸数字 → 通常数字）\n",
    "#  - 正規化は “可逆でない” ことがあるため、**原文は別項目で保存**しておくと安全。\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bea67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-5（説明コメント付き：日本語BERT用 NER 前処理ユーティリティ）\n",
    "# =============================================================================\n",
    "# 前提：\n",
    "#  - BertJapaneseTokenizer（transformers）を継承して、固有表現抽出（NER）向けの\n",
    "#    2種類のエンコード（教師あり/教師なし）と、モデル出力から固有表現スパンを復元する\n",
    "#    補助関数を提供するクラスである。\n",
    "#  - ラベリング方式は BIO ではなく「整数 ID によるスパン連結」方式（O=0, その他=タイプID）。\n",
    "#    → 連続する同一ラベルのトークン列を 1 つのエンティティにまとめる前提。\n",
    "#  - 重複・ネスト・オーバーラップするエンティティは表現できない（単純スパンのみ）。\n",
    "#  - prepare_for_model により [CLS]/[SEP] の特殊トークンが付与される前提で、ラベルを先頭/末尾0に整合。\n",
    "#  - max_length を超える部分は切り捨てられるため、切断されたエンティティは欠落し得る点に注意。\n",
    "#  - encode_plus_untagged では、MeCab 分かち書き（word_tokenizer）→ WordPiece（subword_tokenizer）\n",
    "#    で二段階トークナイズを行い、トークン表層を原文へシーケンシャルマッチしてスパンを得る。\n",
    "#    同一トークンの繰り返しが多い文でも、左から貪欲にマッチしていくため順次対応できる。\n",
    "#  - return_tensors='pt' 指定時、encoding のみ Tensor 化する（spans はリストのまま返る）。\n",
    "#    下流でテンソルを期待する場合は適宜変換すること。\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class NER_tokenizer(BertJapaneseTokenizer):\n",
    "\n",
    "    def encode_plus_tagged(self, text, entities, max_length):\n",
    "        \"\"\"\n",
    "        文章とそれに含まれる固有表現が与えられた時に、\n",
    "        符号化とラベル列の作成を行う。\n",
    "\n",
    "        引数:\n",
    "          text: str\n",
    "            元の文章（原文）。エンティティ抽出の対象。\n",
    "          entities: List[Dict]\n",
    "            [{'span': [start, end], 'type_id': int}, ...] の形を想定。\n",
    "            span は原文 text における [start, end) の半開区間、type_id は 0 以外の整数。\n",
    "          max_length: int\n",
    "            BERT 入力の最大系列長。prepare_for_model で [CLS]/[SEP] を含む長さに正規化される。\n",
    "\n",
    "        戻り値:\n",
    "          encoding: Dict[str, List[int]]\n",
    "            input_ids / attention_mask / token_type_ids / labels を含む辞書。\n",
    "            labels は [0]=CLS, 末尾 [0]=SEP/必要に応じ PAD を 0 で詰める。\n",
    "        \"\"\"\n",
    "        # --- 前処理: エンティティを開始位置で昇順ソート ---\n",
    "        entities = sorted(entities, key=lambda x: x[\"span\"][0])\n",
    "\n",
    "        # --- 原文を「非固有表現片(O=0)」「固有表現片(=type_id)」に分割して並べる ---\n",
    "        splitted = []  # 分割後の文字列片を順に蓄積\n",
    "        position = 0  # 直前に処理した末尾の次インデックス\n",
    "        for entity in entities:\n",
    "            start = entity[\"span\"][0]\n",
    "            end = entity[\"span\"][1]\n",
    "            label = entity[\"type_id\"]\n",
    "            # 非固有表現部分（直近の position からエンティティ開始まで）→ ラベル 0\n",
    "            splitted.append({\"text\": text[position:start], \"label\": 0})\n",
    "            # 固有表現部分（[start:end)）→ ラベル type_id\n",
    "            splitted.append({\"text\": text[start:end], \"label\": label})\n",
    "            position = end\n",
    "        # 最後のエンティティ以降の末尾テキストも非固有表現として追加\n",
    "        splitted.append({\"text\": text[position:], \"label\": 0})\n",
    "        # 空文字は除去（連続するエンティティで start==end 等により空が生じ得るため）\n",
    "        splitted = [s for s in splitted if s[\"text\"]]\n",
    "\n",
    "        # --- 片ごとにトークン化し、各トークンへ片に対応するラベルを付与 ---\n",
    "        tokens = []  # WordPiece トークン列\n",
    "        labels = []  # トークンに対するラベル（0 or type_id）\n",
    "        for text_splitted in splitted:\n",
    "            text = text_splitted[\"text\"]\n",
    "            label = text_splitted[\"label\"]\n",
    "            # BertJapaneseTokenizer.tokenize は日本語前処理（MeCab）+ WordPiece を内部で実施\n",
    "            tokens_splitted = self.tokenize(text)\n",
    "            labels_splitted = [label] * len(tokens_splitted)\n",
    "            tokens.extend(tokens_splitted)\n",
    "            labels.extend(labels_splitted)\n",
    "\n",
    "        # --- トークン列を ID 列に変換し、BERT 入力辞書へ整形 ---\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids, max_length=max_length, padding=\"max_length\", truncation=True\n",
    "        )  # prepare_for_model が [CLS]/[SEP] 等を付与し、長さを揃える\n",
    "\n",
    "        # --- ラベルの特殊トークン・PAD 整合 ---\n",
    "        # 先頭に CLS=0 を付与、本文ラベルは最大長-2（CLS/SEP）までに切詰め、末尾に SEP=0\n",
    "        labels = [0] + labels[: max_length - 2] + [0]\n",
    "        # さらに不足分（PAD 部分）を 0 で埋め、最終的に長さを max_length に一致させる\n",
    "        labels = labels + [0] * (max_length - len(labels))\n",
    "        encoding[\"labels\"] = labels\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def encode_plus_untagged(self, text, max_length=None, return_tensors=None):\n",
    "        \"\"\"\n",
    "        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
    "\n",
    "        引数:\n",
    "          text: str\n",
    "            元の文章。\n",
    "          max_length: Optional[int]\n",
    "            指定時は padding='max_length', truncation=True で固定長化。\n",
    "            未指定時は可変長（特殊トークン付与は維持）。\n",
    "          return_tensors: Optional[str]\n",
    "            'pt' を指定すると encoding の各値を torch.Tensor(batch次元つき) に変換。\n",
    "\n",
    "        戻り値:\n",
    "          encoding: Dict[str, List[int]] or Dict[str, torch.Tensor]\n",
    "            prepare_for_model の出力（[CLS]/[SEP] 付与後）。\n",
    "          spans: List[List[int]]\n",
    "            各トークンに対応する原文上の [start, end) 位置。\n",
    "            特殊トークン([CLS],[SEP],[PAD])の位置は [-1,-1] のダミーにする。\n",
    "        \"\"\"\n",
    "        # --- 形態素（MeCab）→ サブワード（WordPiece）の二段トークナイズ ---\n",
    "        tokens = []  # WordPiece トークン\n",
    "        tokens_original = (\n",
    "            []\n",
    "        )  # スパン計算用の「表層文字列」列（'##' 除去/UNK は単語そのまま）\n",
    "        words = self.word_tokenizer.tokenize(text)  # MeCab による単語列\n",
    "        for word in words:\n",
    "            # 単語を WordPiece に分割\n",
    "            tokens_word = self.subword_tokenizer.tokenize(word)\n",
    "            tokens.extend(tokens_word)\n",
    "            if (\n",
    "                tokens_word[0] == \"[UNK]\"\n",
    "            ):  # 未知語は WordPiece に分割できない → 原単語をそのまま使う\n",
    "                tokens_original.append(word)\n",
    "            else:\n",
    "                # '##' 接頭辞を削って表層形を復元（スパン同定に用いる）\n",
    "                tokens_original.extend(\n",
    "                    [token.replace(\"##\", \"\") for token in tokens_word]\n",
    "                )\n",
    "\n",
    "        # --- 原文上のスパンを左から順次マッチで同定（空白/記号も考慮して進める）---\n",
    "        position = 0  # 原文上の走査位置（左から前進のみ）\n",
    "        spans = []  # 各トークンの [start, end) を蓄積\n",
    "        for token in tokens_original:\n",
    "            l = len(token)\n",
    "            while 1:\n",
    "                # 現在位置から長さ l を切り出して一致判定（空白等を飛ばすため不一致なら1文字進める）\n",
    "                if token != text[position : position + l]:\n",
    "                    position += 1\n",
    "                else:\n",
    "                    spans.append([position, position + l])\n",
    "                    position += l\n",
    "                    break\n",
    "        # ここでの手法は「逐次マッチ」のため、同一部分文字列の繰り返しがあっても左から順に整合が取れる。\n",
    "        # ただし、原文編集（正規化や空白折り畳み）を別途行う場合は、同じ変換を token 側にも適用しておくこと。\n",
    "\n",
    "        # --- BERT 入力辞書へ整形（固定長/可変長を分岐）---\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\" if max_length else False,\n",
    "            truncation=True if max_length else False,\n",
    "        )\n",
    "        sequence_length = len(\n",
    "            encoding[\"input_ids\"]\n",
    "        )  # [CLS]/[SEP] を含む最終トークン列長\n",
    "\n",
    "        # --- 特殊トークン分のダミー span を付与し、長さを encoding に一致させる ---\n",
    "        # 先頭 [CLS] 用のダミー\n",
    "        spans = [[-1, -1]] + spans[: sequence_length - 2]\n",
    "        # 末尾 [SEP] と PAD 分をダミーで埋める（長さを sequence_length に揃える）\n",
    "        spans = spans + [[-1, -1]] * (sequence_length - len(spans))\n",
    "\n",
    "        # --- 必要なら Tensor 化（encoding のみ。spans はリストで返す） ---\n",
    "        if return_tensors == \"pt\":\n",
    "            encoding = {k: torch.tensor([v]) for k, v in encoding.items()}\n",
    "\n",
    "        return encoding, spans\n",
    "\n",
    "    def convert_bert_output_to_entities(self, text, labels, spans):\n",
    "        \"\"\"\n",
    "        文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n",
    "\n",
    "        引数:\n",
    "          text: str\n",
    "            原文。\n",
    "          labels: Sequence[int]\n",
    "            各トークンのラベル（O=0, その他は type_id）。[CLS]/[SEP]/[PAD] 位置の分も含む想定。\n",
    "          spans: Sequence[List[int]]\n",
    "            各トークンの [start, end)。特殊トークンは [-1,-1]。\n",
    "\n",
    "        戻り値:\n",
    "          entities: List[Dict]\n",
    "            {\"name\": 原文片, \"span\": [start,end], \"type_id\": ラベルID} の配列。\n",
    "        \"\"\"\n",
    "        # --- 特殊トークン（span=-1）に対応するラベル/スパンを除去して本文トークンに限定 ---\n",
    "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
    "        spans = [span for span in spans if span[0] != -1]\n",
    "\n",
    "        # --- 連続する同一ラベルをまとめて 1 スパンのエンティティに復元 ---\n",
    "        # itertools.groupby により、labels の連続区間ごとにグルーピングする。\n",
    "        # 例: labels=[0,0,2,2,0,3] → (0区間)(2区間)(0区間)(3区間)\n",
    "        # ラベル0（O）は無視し、非0の区間のみエンティティを生成する。\n",
    "        entities = []\n",
    "        for label, group in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
    "\n",
    "            group = list(group)  # group は [(idx,label), ...] の列\n",
    "            start = spans[group[0][0]][0]  # 区間先頭トークンの start\n",
    "            end = spans[group[-1][0]][1]  # 区間末尾トークンの end（半開区間の終端）\n",
    "\n",
    "            if label != 0:  # O 以外のラベルのみエンティティ化\n",
    "                entity = {\n",
    "                    \"name\": text[start:end],  # 原文断片（復元テキスト）\n",
    "                    \"span\": [start, end],  # 原文上の [start,end)\n",
    "                    \"type_id\": label,  # ラベル ID（BIO ではなく type_id そのもの）\n",
    "                }\n",
    "                entities.append(entity)\n",
    "\n",
    "        # 注意：\n",
    "        #  - BIO 方式ではないため、同一ラベルが連続すれば 1 エンティティに連結される。\n",
    "        #    同じ type_id が離れて複数回出現する場合は、それぞれ別エンティティとして復元される。\n",
    "        #  - ネスト/オーバーラップは扱えない（単純連続区間のみ）。\n",
    "        #  - max_length による切詰めでエンティティの一部が失われると、復元されない場合がある。\n",
    "\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f4f30b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 8-6\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c10f011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 10271, 28486, 5, 546, 3000, 1518, 233, 13, 5, 1878, 2682, 9, 10750, 308, 10, 8, 3, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], 'labels': [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# 8-7（説明コメント付き：NER_tokenizer.encode_plus_tagged の動作確認）\n",
    "# =============================================================================\n",
    "# 目的：\n",
    "#  - 原文 `text` と、原文上のスパン（[start, end) の半開区間）＋ラベルIDからなる `entities`\n",
    "#    を与え、BERT 入力（input_ids/attention_mask/token_type_ids）に整形すると同時に、\n",
    "#    各トークンに対応する **ラベル列**（O=0, エンティティは type_id）を作成する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - encode_plus_tagged は「BIO 方式」ではなく **連結方式**（同一ラベルが連続するトークン列を1つの\n",
    "#    エンティティとみなす）前提でラベルを作る。特殊トークン [CLS]/[SEP]/[PAD] は 0 として埋める。\n",
    "#  - `span=[start,end]` は **半開区間**（start 文字を含み end 文字を含まない）。インデックスは 0 始まり。\n",
    "#  - トークン化は WordPiece なので、1語が複数トークンに分割されても、分割前の片に付けたラベルが\n",
    "#    そのまま各トークンに複製される設計。\n",
    "#  - `max_length` 超過分は切り捨てられるため、末尾側のエンティティが途中で切れると、対応トークンに\n",
    "#    ラベルが乗らず欠落し得る点に注意（長さ設計 or スライディングで対処）。\n",
    "#  - 直前に正規化（例：NFKC）を行った場合、**スパンは正規化後の文字列基準**で与えること（前後で\n",
    "#    文字長が変わるとズレる）。\n",
    "# =============================================================================\n",
    "\n",
    "# （互換/保守）NER_tokenizer が未インポート/未定義でも動くように最小限のフォールバックを用意\n",
    "try:\n",
    "    NER_tokenizer\n",
    "except NameError:\n",
    "    # ここではクラス本体は 8-5 で定義済み想定。未定義なら明示エラーにする。\n",
    "    raise RuntimeError(\n",
    "        \"NER_tokenizer クラスが未定義です。先に 8-5 の定義を実行してください。\"\n",
    "    )\n",
    "\n",
    "# 既存の `tokenizer` が NER_tokenizer でない場合は、同名で置き換えても構わない運用なら差し替える\n",
    "try:\n",
    "    tokenizer\n",
    "    has_encode_tagged = hasattr(tokenizer, \"encode_plus_tagged\")\n",
    "except NameError:\n",
    "    has_encode_tagged = False\n",
    "\n",
    "if not has_encode_tagged:\n",
    "    # 学習時と同じ語彙を使う（未定義なら東北大BERTにフォールバック）\n",
    "    try:\n",
    "        MODEL_NAME\n",
    "    except NameError:\n",
    "        MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --------------------------- 元のコード（＋説明コメント） ---------------------------\n",
    "\n",
    "# 入力テキスト（インデックスは 0 始まり）\n",
    "# 文字位置: 0:昨,1:日,2:の,3:み,4:ら,5:い,6:事,7:務,8:所,9:と,10:の,11:打,12:ち,13:合,14:わ,15:せ,16:は,17:順,18:調,19:だ,20:っ,21:た,22:。\n",
    "text = \"昨日のみらい事務所との打ち合わせは順調だった。\"\n",
    "\n",
    "# エンティティの指定：\n",
    "#  - name は人間可読の補助で、実際のラベル付与は span と type_id によって行われる\n",
    "#  - span=[3,9) → 原文中の「みらい事務所」（3〜8文字目）を指定（end=9 は「と」なので非包含）\n",
    "#  - type_id=1 → O（背景）を 0 とするクラス設計の「1番」カテゴリ\n",
    "entities = [{\"name\": \"みらい事務所\", \"span\": [3, 9], \"type_id\": 1}]\n",
    "\n",
    "# ラベル列作成つきの符号化（最大長は [CLS]/[SEP] を含むトークン長に正規化される）\n",
    "# labels は先頭[CLS]=0、本文（max_length-2 まで）、末尾[SEP]=0、残りPAD=0 で埋められる。\n",
    "encoding = tokenizer.encode_plus_tagged(text, entities, max_length=20)\n",
    "\n",
    "# 出力の中身は Hugging Face の標準キー（input_ids, attention_mask, token_type_ids）に加え、\n",
    "# 1:1 に対応する labels が入っている。確認用にそのまま表示。\n",
    "print(encoding)\n",
    "\n",
    "# 追加の確認（任意）：\n",
    "# - トークン列長とラベル列長が一致していること\n",
    "# - ラベル中の 1 の連続区間が「みらい事務所」の WordPiece 分割数に一致していること\n",
    "# 例：\n",
    "# tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "# print(tokens)\n",
    "# print(encoding[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85a51968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# encoding\n",
      "{'input_ids': tensor([[    2,     1,     5,  1543,   125,     9,  6749, 28550,  2953, 28550,\n",
      "         28566, 21202, 28683, 14050, 12475,    12,    31,     8,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "# spans\n",
      "[[-1, -1], [0, 2], [2, 3], [3, 5], [5, 6], [6, 7], [7, 9], [9, 10], [10, 12], [12, 13], [13, 14], [15, 18], [18, 19], [19, 23], [24, 27], [27, 28], [28, 30], [30, 31], [-1, -1]]\n"
     ]
    }
   ],
   "source": [
    "# 8-8\n",
    "text = \"騰訊の英語名はTencent Holdings Ltdである。\"\n",
    "encoding, spans = tokenizer.encode_plus_untagged(text, return_tensors=\"pt\")\n",
    "print(\"# encoding\")\n",
    "print(encoding)\n",
    "print(\"# spans\")\n",
    "print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55cb9eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': '騰訊の', 'span': [0, 3], 'type_id': 1}, {'name': 'ncent Holdings Ltdで', 'span': [9, 28], 'type_id': 1}]\n"
     ]
    }
   ],
   "source": [
    "# 8-9\n",
    "labels_predicted = [0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "entities = tokenizer.convert_bert_output_to_entities(text, labels_predicted, spans)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb0ff5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] device = mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 8-10（説明コメント付き：日本語BERTをトークン分類（NER）用に初期化）\n",
    "# =============================================================================\n",
    "# 目的：\n",
    "#  - 日本語BERTの語彙・前処理（MeCab + WordPiece）に対応した **NER_tokenizer** を読み込み、\n",
    "#    事前学習モデルの上に **BertForTokenClassification** ヘッド（num_labels=4）を載せる。\n",
    "#  - MacBook 環境（Apple Silicon/MPS）・CUDA・CPU のいずれでも動くように **デバイス自動選択**で配置。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - Token Classification は各トークン t に対してクラスのロジット z_{t,c} を出力し、\n",
    "#    学習時は通常 CrossEntropyLoss（各位置の多クラス）を用いる。\n",
    "#  - NER ではしばしば **BIO/IOBES** を採用するが、本書式は「O=0, エンティティ種別ID>0」\n",
    "#    のシンプル方式（連結方式）も可能。`num_labels=4` は例として\n",
    "#       0: \"O\", 1: \"ORG\", 2: \"PER\", 3: \"LOC\"\n",
    "#    のように解釈できる（実際の定義はデータに合わせて固定・共有すること）。\n",
    "#  - 学習時に `[CLS]/[SEP]/[PAD]` 位置のラベルを損失から除外するには、\n",
    "#    データ側のラベルを **ignore_index（例：-100）** にしておくのが Hugging Face の定石。\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "from transformers import BertForTokenClassification\n",
    "\n",
    "# --- （参考）元のコード（実行はしない。可搬性の観点で .cuda() 固定は避けるため） ---\n",
    "# tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "# bert_tc = BertForTokenClassification.from_pretrained(\n",
    "#     MODEL_NAME, num_labels=4\n",
    "# )\n",
    "# bert_tc = bert_tc.cuda()\n",
    "\n",
    "# --- MODEL_NAME が未定義なら東北大BERTにフォールバック ---\n",
    "try:\n",
    "    MODEL_NAME\n",
    "except NameError:\n",
    "    MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "\n",
    "# --- NER_tokenizer クラスの存在確認（8-5 で定義済み想定） ---\n",
    "try:\n",
    "    NER_tokenizer\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\n",
    "        \"NER_tokenizer クラスが未定義です。先に 8-5 の定義を実行してください。\"\n",
    "    ) from e\n",
    "\n",
    "# --- ラベル定義（例）。実データに合わせて **固定・共有** することが重要 ---\n",
    "num_labels = 4\n",
    "id2label = {0: \"O\", 1: \"ORG\", 2: \"PER\", 3: \"LOC\"}  # 例：組織/人/場所\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "\n",
    "# --- デバイス自動選択（Mac MPS → CUDA → CPU） ---\n",
    "def pick_device() -> torch.device:\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = pick_device()\n",
    "print(f\"[info] device = {device}\")\n",
    "\n",
    "# --- トークナイザ：日本語用の NER_tokenizer（MeCab + WordPiece 連携） ---\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- モデル本体：事前学習BERT + Token Classification ヘッド ---\n",
    "bert_tc = (\n",
    "    BertForTokenClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,  # 推論・可視化で役立つ（config に保存され ckpt へも残る）\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    .to(device)\n",
    "    .eval()\n",
    ")  # 推論時は eval()。学習時は train() に切替。\n",
    "\n",
    "# =============================================================================\n",
    "# 補足（実務向け）：\n",
    "#  - 学習：\n",
    "#      - ラベル列の特殊トークン位置は `-100`（ignore_index）にして DataCollator などで結合。\n",
    "#      - Optimizer は AdamW、Scheduler は linear/warmup を推奨。\n",
    "#  - 推論：\n",
    "#      - `outputs = bert_tc(**batch)` → `logits.argmax(-1)` でラベルID列。\n",
    "#      - `tokenizer` から `offset_mapping`（Fast版）を使える場合、原文スパン復元が堅牢。\n",
    "#  - 再現性：\n",
    "#      - `MODEL_NAME`、`id2label`、`max_length`、正規化の有無（NFKC）を **明示的に固定**・記録。\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29c545c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.4404\n"
     ]
    }
   ],
   "source": [
    "# 8-12（修正版：CUDA 未有効エラーの解消と実務的ガードを追加）\n",
    "# =============================================================================\n",
    "# エラー原因：\n",
    "#   AssertionError: Torch not compiled with CUDA enabled\n",
    "# → 環境が CUDA 非対応なのに `.cuda()` を呼んだため。\n",
    "#\n",
    "# 対処方針：\n",
    "#  1) モデルが載っている実デバイスに合わせて **.to(device)** に統一（MPS/CUDA/CPU どれでも可）。\n",
    "#  2) （任意だが推奨）特殊トークン([CLS]/[SEP]/[PAD])と PAD 部分のラベルを **-100** にして\n",
    "#     CrossEntropyLoss の ignore_index に合わせ、損失に寄与させない。\n",
    "#     ※ 8-5 実装はこれらを 0=O にしているため、何もしないと O クラス過多で学習が歪む。\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- 手作り NER データ（元コードそのまま） ---\n",
    "data = [\n",
    "    {\n",
    "        \"text\": \"AさんはB大学に入学した。\",\n",
    "        \"entities\": [\n",
    "            {\"name\": \"A\", \"span\": [0, 1], \"type_id\": 2},\n",
    "            {\"name\": \"B大学\", \"span\": [4, 7], \"type_id\": 1},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"CDE株式会社は新製品「E」を販売する。\",\n",
    "        \"entities\": [\n",
    "            {\"name\": \"CDE株式会社\", \"span\": [0, 7], \"type_id\": 1},\n",
    "            {\"name\": \"E\", \"span\": [12, 13], \"type_id\": 3},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# --- 符号化：8-5 の encode_plus_tagged を使用（元コードそのまま） ---\n",
    "max_length = 32\n",
    "dataset_for_loader = []\n",
    "for sample in data:\n",
    "    text = sample[\"text\"]\n",
    "    entities = sample[\"entities\"]\n",
    "    encoding = tokenizer.encode_plus_tagged(text, entities, max_length=max_length)\n",
    "    # HF の TokenClassification は labels を long テンソル（クラスID）で受けるのが定石\n",
    "    encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "    dataset_for_loader.append(encoding)\n",
    "\n",
    "# --- DataLoader（元コードの方針：全件を 1 ミニバッチに） ---\n",
    "dataloader = DataLoader(dataset_for_loader, batch_size=len(data))\n",
    "\n",
    "# --- デバイス：モデル側に合わせて自動決定（.cuda() を撤廃） ---\n",
    "device = next(bert_tc.parameters()).device  # 例：mps/cuda/cpu のいずれか\n",
    "\n",
    "\n",
    "# --- 推奨：特殊トークンと PAD を損失から除外（ignore_index=-100）に置換する関数 ---\n",
    "def mask_special_positions_to_ignore_index(batch_dict, tokenizer, ignore_index=-100):\n",
    "    \"\"\"\n",
    "    TokenClassification の損失から除外したい位置（[CLS]/[SEP]/[PAD] と attention_mask=0）を\n",
    "    ラベル上で ignore_index に置き換える。\n",
    "    \"\"\"\n",
    "    input_ids = batch_dict[\"input_ids\"]\n",
    "    attention_mask = batch_dict[\"attention_mask\"]\n",
    "    labels = batch_dict[\"labels\"]\n",
    "\n",
    "    # PAD 位置（attention_mask==0）を -100 に\n",
    "    labels = labels.masked_fill(attention_mask.eq(0), ignore_index)\n",
    "\n",
    "    # CLS/SEP 位置を -100 に（存在チェック込み）\n",
    "    cls_id = tokenizer.cls_token_id\n",
    "    sep_id = tokenizer.sep_token_id\n",
    "    for i in range(labels.size(0)):\n",
    "        cls_pos = (input_ids[i] == cls_id).nonzero(as_tuple=True)[0]\n",
    "        sep_pos = (input_ids[i] == sep_id).nonzero(as_tuple=True)[0]\n",
    "        if cls_pos.numel() > 0:\n",
    "            labels[i, cls_pos] = ignore_index\n",
    "        if sep_pos.numel() > 0:\n",
    "            labels[i, sep_pos] = ignore_index\n",
    "\n",
    "    batch_dict[\"labels\"] = labels\n",
    "    return batch_dict\n",
    "\n",
    "\n",
    "# --- ミニバッチで損失を計算（.to(device) で可搬化） ---\n",
    "for batch in dataloader:\n",
    "    # 元コードの `.cuda()` を **.to(device)** に差し替え（MPS/CPU でも動く）\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    # （任意・推奨）損失に含めない位置を ignore_index=-100 に置換\n",
    "    batch = mask_special_positions_to_ignore_index(batch, tokenizer, ignore_index=-100)\n",
    "\n",
    "    # 推論または学習（ここでは損失取得のみ）\n",
    "    output = bert_tc(**batch)  # labels が含まれているため内部で CrossEntropyLoss を計算\n",
    "    loss = output.loss  # 平均化されたトークン単位 CE 損失\n",
    "    print(f\"loss = {loss.item():.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 理論補足：\n",
    "#  - CrossEntropyLoss はクラス ID（long）を取り、ignore_index（既定 -100）位置は損失計算から除外。\n",
    "#  - ignore_index を適用すると、PAD や特殊トークンの頻度に損失が引っぱられず、学習が安定。\n",
    "#  - 連結方式（O=0, type_id>0）でも動作するが、隣接同タイプを分離したいなら BIO/IOBES へ拡張。\n",
    "#  - 実学習では bert_tc.train()、評価や予測では bert_tc.eval() + no_grad() を併用する。\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4122016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
