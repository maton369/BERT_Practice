{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-1\n",
    "!mkdir chap10\n",
    "%cd ./chap10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad745c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-3\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "\n",
    "# BERTの日本語モデル\n",
    "MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2573ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-27 13:52:59--  https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
      "www.rondhuit.com (www.rondhuit.com) をDNSに問いあわせています... 59.106.19.174\n",
      "www.rondhuit.com (www.rondhuit.com)|59.106.19.174|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 8855190 (8.4M) [application/x-gzip]\n",
      "`ldcc-20140209.tar.gz' に保存中\n",
      "\n",
      "ldcc-20140209.tar.g 100%[===================>]   8.44M  8.19MB/s 時間 1.0s       \n",
      "\n",
      "2025-11-27 13:53:01 (8.19 MB/s) - `ldcc-20140209.tar.gz' へ保存完了 [8855190/8855190]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10-4\n",
    "#データのダウンロード\n",
    "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz \n",
    "#ファイルの解凍\n",
    "!tar -zxf ldcc-20140209.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ba9979b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# device = mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [42:40<00:00, 284.49s/it]   \n"
     ]
    }
   ],
   "source": [
    "# 10-5（Mac対応：MPS/CPU/CUDA 自動切替、説明コメント付き）\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "\n",
    "# --- モデル名（未定義なら既定を設定） ---\n",
    "try:\n",
    "    MODEL_NAME\n",
    "except NameError:\n",
    "    MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "\n",
    "# --- デバイス自動選択：MPS → CUDA → CPU の順で利用 ---\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")  # Apple Silicon (Metal Performance Shaders)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA CUDA\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # CPU フォールバック\n",
    "\n",
    "print(f\"# device = {device}\")\n",
    "\n",
    "# ================================================================\n",
    "# 目的\n",
    "# - Livedoorニュース9カテゴリから本文を取り出し、\n",
    "#   BERT の最終層隠れ状態を [PAD] を無視した平均でプーリングして文ベクトル化。\n",
    "#\n",
    "# 理論メモ\n",
    "# - [CLS] ではなく masked mean pooling を使う理由：\n",
    "#   事前学習BERTの [CLS] は次文予測などタスク特化バイアスが強く、\n",
    "#   文章意味の幾何（コサイン距離等）では平均プーリングの方が一貫しやすい知見がある。\n",
    "#   ただし vanilla BERT は文埋め込み専用に訓練されていないため、\n",
    "#   Sentence-BERT系などの専用モデルや微調整でさらに精度が上がる。\n",
    "# ================================================================\n",
    "\n",
    "# カテゴリーのリスト\n",
    "category_list = [\n",
    "    \"dokujo-tsushin\",\n",
    "    \"it-life-hack\",\n",
    "    \"kaden-channel\",\n",
    "    \"livedoor-homme\",\n",
    "    \"movie-enter\",\n",
    "    \"peachy\",\n",
    "    \"smax\",\n",
    "    \"sports-watch\",\n",
    "    \"topic-news\",\n",
    "]\n",
    "\n",
    "# トークナイザとモデルのロード（推論モードへ）\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME).to(device).eval()\n",
    "\n",
    "max_length = 256\n",
    "sentence_vectors = []  # 各記事の文ベクトル（形状: [H]）\n",
    "labels = []  # カテゴリID\n",
    "\n",
    "for label, category in enumerate(tqdm(category_list)):\n",
    "    for file in glob.glob(f\"./text/{category}/{category}*\"):\n",
    "        # --- 本文抽出：先頭3行をメタとみなし、4行目以降を本文として結合 ---\n",
    "        # 文字コードが不明な場合は encoding 指定を調整（例: encoding='utf-8'）\n",
    "        with open(file, \"r\") as f:\n",
    "            lines = f.read().splitlines()\n",
    "        text = \"\\n\".join(lines[3:])\n",
    "\n",
    "        # --- 符号化：固定長化（max_length）＋PAD付与、CLS/SEPは自動付与 ---\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # 張り付け先デバイスへ移動（MPS/CUDA/CPU）\n",
    "        encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "        attention_mask = encoding[\"attention_mask\"]  # [1, T]（実トークン=1, PAD=0）\n",
    "\n",
    "        # --- 文ベクトル化：masked mean pooling（[PAD]無視平均）---\n",
    "        # last_hidden_state: [B=1, T, H]\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoding)\n",
    "            last_hidden_state = output.last_hidden_state\n",
    "            mask = attention_mask.unsqueeze(-1)  # [1, T, 1]\n",
    "            summed = (last_hidden_state * mask).sum(dim=1)  # [1, H]\n",
    "            count = mask.sum(dim=1).clamp(min=1)  # [1, 1]（ゼロ割防止）\n",
    "            pooled = summed / count  # [1, H]\n",
    "\n",
    "        sentence_vectors.append(pooled[0].detach().cpu().numpy())\n",
    "        labels.append(label)\n",
    "\n",
    "# --- numpy 配列へ整形 ---\n",
    "sentence_vectors = np.vstack(sentence_vectors)  # [N_doc, H]\n",
    "labels = np.array(labels)  # [N_doc]\n",
    "\n",
    "# 以降：可視化/分類器学習などに sentence_vectors, labels を使用\n",
    "# 例）sklearn のロジスティック回帰やt-SNE/UMAP可視化など"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30c915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
