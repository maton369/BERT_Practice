{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d2277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-1\n",
    "!mkdir chap7\n",
    "%cd ./chap7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "062b1b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-3\n",
    "import random\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 日本語の事前学習モデル\n",
    "MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a2a5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-4（説明コメント付き：BERTを用いたマルチラベル文分類の最小実装）\n",
    "# --------------------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - 事前学習済み BERT（エンコーダ本体：BertModel）の出力（最終層隠れ状態）から\n",
    "#    「[PAD] を除いたトークン平均（mean pooling）」で文ベクトルを作り、\n",
    "#    線形層でラベル数（num_labels）次元に写像して **マルチラベル**分類を行う。\n",
    "#\n",
    "# 理論メモ（マルチラベル vs マルチクラス）：\n",
    "#  - マルチクラス（排他的1クラス）では softmax + CrossEntropyLoss を用いるが、\n",
    "#  - マルチラベル（複数ラベル同時に1の可能性）では **各ラベルを独立なベルヌーイ**として扱い、\n",
    "#    出力は raw logits（活性前）→ **BCEWithLogitsLoss**（内部で sigmoid + BCE を数値安定に計算）を用いる。\n",
    "#  - 教師ラベルの形：shape [B, C]、各要素は {0,1}（float型推奨）。しきい値 0.5 などで陽性判定を後段で行う。\n",
    "#\n",
    "# プーリング設計（mean pooling の意図）：\n",
    "#  - [CLS] ベクトル単独の利用に比べ、文全体の情報を平均で取り入れやすい。短文～中程度の長さで堅実なベースライン。\n",
    "#  - attention_mask により [PAD] 位置は平均から除外（分母は非PADトークン数）。\n",
    "#  - 分母ゼロ（全PAD）対策：学習データ生成で通常発生しない想定だが、実務では clamp などで保険をかけると安全。\n",
    "#\n",
    "# そのほか設計の注意：\n",
    "#  - token_type_ids（= segment ids）は文対（sentence pair）で区別に使うが、単文では 0 固定でもよい。\n",
    "#  - 出力は慣例的には transformers の `SequenceClassifierOutput` を使うが、\n",
    "#    ここでは attributes にアクセスできる簡易オブジェクトを返す（互換性が必要なら差し替え推奨）。\n",
    "#  - クラス不均衡が強い場合は BCEWithLogitsLoss の `pos_weight` を利用すると勾配が安定する。\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class BertForSequenceClassificationMultiLabel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        # BertModel のロード（分類ヘッドなしの本体のみ）。\n",
    "        # - 入力：input_ids, attention_mask, token_type_ids\n",
    "        # - 出力：last_hidden_state（[B, L, H]）など\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "        # 文ベクトル（プーリング後の [B, H]）から各ラベルのロジット [B, C] を直線写像する層。\n",
    "        # - H: 隠れ次元（bert.config.hidden_size）、C: ラベル数（num_labels）\n",
    "        self.linear = torch.nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None\n",
    "    ):\n",
    "        # 1) BERT でトークン列を符号化し、最終層の隠れ状態を得る。\n",
    "        #    last_hidden_state の形は [B, L, H]（B:バッチ、L:系列長、H:隠れ次元）\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        last_hidden_state = bert_output.last_hidden_state\n",
    "\n",
    "        # 2) [PAD] 以外の位置のみで平均プーリングして文ベクトルを作る。\n",
    "        #    - attention_mask は [B, L] の {0,1}。unsqueeze(-1) で [B, L, 1] に拡張し、\n",
    "        #      last_hidden_state（float）にマスク乗算→ PAD 位置を 0 化。\n",
    "        #    - 分母は非PADトークン数（attention_mask.sum(dim=1)）。\n",
    "        #      典型データでは 0 になることはないが、実運用では clamp_min(1e-9) などで安全策も検討。\n",
    "        averaged_hidden_state = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(\n",
    "            1\n",
    "        ) / attention_mask.sum(1, keepdim=True)\n",
    "        # 参考（安全策の例、実装切替の際に利用）：\n",
    "        # denom = attention_mask.sum(1, keepdim=True).clamp_min(1e-9).float()\n",
    "        # masked = last_hidden_state * attention_mask.unsqueeze(-1).float()\n",
    "        # averaged_hidden_state = masked.sum(1) / denom\n",
    "\n",
    "        # 3) 線形層でラベル数 C 次元のロジットへ（活性はかけない：BCEWithLogitsLoss を使うため）\n",
    "        scores = self.linear(averaged_hidden_state)  # shape: [B, C]\n",
    "\n",
    "        # 4) 出力を dict 形式で用意（logits を必須、labels が来ていれば loss も計算）\n",
    "        output = {\"logits\": scores}\n",
    "\n",
    "        # 5) 教師ラベルが与えられた場合は損失を計算して返す。\n",
    "        #    - BCEWithLogitsLoss は内部で sigmoid を組み合わせた数値安定版。\n",
    "        #    - labels 形状は [B, C]、型は float（{0.0, 1.0}）を想定。\n",
    "        #    - クラス不均衡が強ければ pos_weight（shape [C]）の指定を検討。\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(scores, labels.float())\n",
    "            output[\"loss\"] = loss\n",
    "\n",
    "        # 6) 呼び出し側が `output.logits` / `output.loss` で参照できるよう簡易オブジェクト化。\n",
    "        #    transformers の標準 `SequenceClassifierOutput` を使う場合は差し替え可。\n",
    "        output = type(\"bert_output\", (object,), output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b11ff6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using device = mps\n"
     ]
    }
   ],
   "source": [
    "# 7-5（説明コメント付き：マルチラベル文分類モデルのロードとデバイス配置）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - 日本語BERTと同一語彙のトークナイザをロードし、\n",
    "#  - 7-4で定義した「BertForSequenceClassificationMultiLabel」を初期化、\n",
    "#  - 環境に応じた最適デバイス（MPS/CUDA/CPU）へ安全に配置する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - 本モデルは出力が各ラベルの「ロジット」（活性前）で、マルチラベル想定（独立ベルヌーイ）。\n",
    "#    学習時は BCEWithLogitsLoss（sigmoid + BCE の安定版）を用いる。\n",
    "#  - num_labels=2 の場合、2つのラベルそれぞれを {0,1} で同時に予測する（softmaxではない）。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer\n",
    "\n",
    "# --- MODEL_NAME が未定義でも動くように保険を入れる ---\n",
    "try:\n",
    "    MODEL_NAME\n",
    "except NameError:\n",
    "    MODEL_NAME = (\n",
    "        \"tohoku-nlp/bert-base-japanese-whole-word-masking\"  # 東北大版日本語BERT（WWM）\n",
    "    )\n",
    "\n",
    "\n",
    "# --- デバイス選択：Mac(MPS) → CUDA → CPU の順で自動選択 ---\n",
    "def get_best_device() -> torch.device:\n",
    "    # Apple Silicon + macOS Metal\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    # NVIDIA GPU（Linux/Windowsなど）\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # フォールバック：CPU\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"[info] using device = {device}\")\n",
    "\n",
    "# --- トークナイザのロード（モデルと同じ語彙でないとIDがずれて壊れる） ---\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- マルチラベル分類モデルの構築 ---\n",
    "# num_labels=2：2つのラベルを独立に予測（例：{スポーツ, IT} の属否を同時に判定 など）\n",
    "bert_scml = BertForSequenceClassificationMultiLabel(MODEL_NAME, num_labels=2)\n",
    "\n",
    "# --- 最適デバイスへ配置（.cuda()固定はMacで失敗するため .to(device) を統一使用） ---\n",
    "bert_scml = bert_scml.to(device)\n",
    "\n",
    "# 推論時は Dropout を止めたいので eval()、学習開始時は train() を呼ぶ。\n",
    "# bert_scml.eval()   # 例：推論前\n",
    "# bert_scml.train()  # 例：学習ループ開始時\n",
    "\n",
    "# （参考）出力の扱い：\n",
    "# - forward は {'logits': Tensor[B, C], 'loss': Tensor[]} 相当を返す簡易オブジェクト。\n",
    "# - 学習時は labels=[B, C] を与えると BCEWithLogitsLoss を内部計算し output.loss に入る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "037ea7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# scores (logits): tensor([[0.2937, 0.1129],\n",
      "        [0.2349, 0.0266]], device='mps:0')\n",
      "# probs (sigmoid): tensor([[0.5729, 0.5282],\n",
      "        [0.5585, 0.5066]], device='mps:0')\n",
      "# predicted labels: tensor([[1, 1],\n",
      "        [1, 1]], device='mps:0')\n",
      "# subset accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# 7-6（説明コメント付き：マルチラベル推論・しきい値判定・サブセット精度）\n",
    "import torch\n",
    "\n",
    "# 入力テキスト：各サンプルに複数ラベルが立ち得る（マルチラベル）\n",
    "text_list = [\"今日の仕事はうまくいったが、体調があまり良くない。\", \"昨日は楽しかった。\"]\n",
    "\n",
    "# 教師ラベル（shape=[B, C]）：\n",
    "#  - マルチラベルなので各列（ラベル）を独立に {0,1} で表す\n",
    "#  - 例：C=2 のとき [仕事/体調] のような2軸を同時に予測するイメージ\n",
    "labels_list = [[1, 1], [0, 1]]\n",
    "\n",
    "# --- デバイス整合：モデル(bert_scml)の実デバイスに入力を合わせる ---\n",
    "device = next(bert_scml.parameters()).device\n",
    "\n",
    "# データの符号化\n",
    "#  - padding='longest' はバッチ内の最長系列に合わせて動的にPADを付与\n",
    "#  - 実運用では max_length+truncation=True+padding='max_length' で固定長化すると計算が安定\n",
    "encoding = tokenizer(text_list, padding=\"longest\", return_tensors=\"pt\")\n",
    "encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "# 教師ラベルテンソル\n",
    "#  - 学習で BCEWithLogitsLoss を使う場合は float（{0.0,1.0}）が望ましい\n",
    "#  - ここでは評価用に int 版も用意（比較演算時に便利）\n",
    "labels_float = torch.tensor(labels_list, dtype=torch.float32, device=device)\n",
    "labels_int = labels_float.to(torch.int64)\n",
    "\n",
    "# --- 推論：本モデルは logits（活性前スコア）を返す ---\n",
    "with torch.no_grad():\n",
    "    output = bert_scml(**encoding)\n",
    "scores = output.logits  # shape: [B, C]，各ラベルのロジット\n",
    "\n",
    "# --- 判定（しきい値） ---\n",
    "# 方式A：確率で判定（推奨。しきい値を柔軟に最適化できる）\n",
    "probs = scores.sigmoid()  # p(y_c=1|x)\n",
    "threshold = 0.5  # 必要に応じて ROC/PR で最適化\n",
    "labels_predicted = (probs >= threshold).to(torch.int64)\n",
    "\n",
    "# 方式B：ロジットの符号で判定（sigmoid≥0.5 と等価だが明示性に欠ける）\n",
    "# labels_predicted = (scores > 0).to(torch.int64)\n",
    "\n",
    "# --- 精度計算（subset accuracy：全ラベル一致率） ---\n",
    "#  - サンプルごとに全てのラベルが正解しているかを判定し、その平均をとる厳しめの指標\n",
    "num_correct = (labels_predicted == labels_int).all(dim=-1).sum().item()\n",
    "accuracy = num_correct / labels_int.size(0)\n",
    "\n",
    "# 参考：他の評価指標（マルチラベルで一般的）\n",
    "#  - example-based F1（サンプル単位のF1を平均）\n",
    "#  - micro/macro-F1（ラベル軸で集計）\n",
    "#  - Hamming loss（ラベル単位の誤り率）\n",
    "# これらは torchmetrics（MultilabelF1Score 等）で容易に算出可能\n",
    "\n",
    "print(\"# scores (logits):\", scores)\n",
    "print(\"# probs (sigmoid):\", probs)\n",
    "print(\"# predicted labels:\", labels_predicted)\n",
    "print(\"# subset accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a26f061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# loss: 0.671375036239624\n"
     ]
    }
   ],
   "source": [
    "# 7-7（説明コメント付き：マルチラベル損失（BCEWithLogitsLoss）の計算）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - トークナイズ結果にマルチラベル教師（[B, C]）を同梱し、モデルの forward で\n",
    "#    BCEWithLogitsLoss（sigmoid + BCE の数値安定版）を自動計算させて loss を得る。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - マルチラベルでは各クラスを独立なベルヌーイとみなし、出力はロジット（活性前）。\n",
    "#    教師は float 型の {0.0, 1.0} 行列（shape=[B, C]）。softmax は使わない。\n",
    "#  - BCEWithLogitsLoss は内部で sigmoid を合成するため、出力に sigmoid をかけずに\n",
    "#    生のロジットをそのまま渡すのが正しい。\n",
    "#  - クラス不均衡が強い場合は pos_weight（shape=[C]）を指定すると陽性側の損失寄与を補正できる。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "\n",
    "# --- デバイス整合：モデル(bert_scml)の実デバイスに入力を合わせる ---\n",
    "device = next(bert_scml.parameters()).device\n",
    "\n",
    "# --- データの符号化（バッチ内の最長に合わせて動的PAD） ---\n",
    "# 実運用では max_length+truncation=True+padding=\"max_length\" で固定長化すると計算が安定。\n",
    "encoding = tokenizer(text_list, padding=\"longest\", return_tensors=\"pt\")\n",
    "encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "# --- ラベルを同梱（[B, C]、float 型） ---\n",
    "# BCEWithLogitsLoss は float を前提。int のままでも内部で float 化されるが明示が安全。\n",
    "labels = torch.tensor(labels_list, dtype=torch.float32, device=device)\n",
    "encoding[\"labels\"] = labels\n",
    "\n",
    "# --- 損失の計算 ---\n",
    "# 学習時は train() にして Dropout を有効、評価時は eval() + no_grad()。\n",
    "bert_scml.train()  # 例：学習フェーズを想定（評価なら bert_scml.eval() と no_grad() を併用）\n",
    "output = bert_scml(**encoding)\n",
    "loss = output.loss  # BCEWithLogitsLoss（バッチ平均）\n",
    "\n",
    "print(\"# loss:\", float(loss.detach().cpu()))\n",
    "\n",
    "# （参考）不均衡対策：pos_weight を使う例（実装差し替え時）\n",
    "#   C = labels.size(1)\n",
    "#   pos_weight = torch.tensor([...], dtype=torch.float32, device=device)  # shape=[C]\n",
    "#   criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#   loss = criterion(output.logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5d562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
