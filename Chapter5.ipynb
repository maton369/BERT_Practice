{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f5ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-2\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec9c8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using device = mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=32000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-3\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#  - 日本語BERT（Whole Word Masking 版）を「MLM（Masked Language Modeling）」タスク用の\n",
    "#    モデルとしてロードし、MacBook を含む環境で最適なデバイス（MPS/CUDA/CPU）に載せる。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - MLM（Masked Language Modeling）は、入力トークンの一部を [MASK] に置換し、\n",
    "#    その復元を学習する自己教師ありタスクである。BERTの事前学習の主要素。\n",
    "#  - Whole Word Masking（WWM）は「サブワード単位ではなく“語”単位でまとめてマスクする」\n",
    "#    戦略。語彙断片ではなく語レベルの完形復元圧力を与えるため、語彙的一貫性の向上が期待される。\n",
    "#  - 推論（補完）時は [MASK] の位置の語彙分布（logits）から上位候補を選ぶ。\n",
    "#    学習時は [MASK] 位置以外のロスを無視（ignore_index=-100）するのが一般的（DataCollatorが自動対応可）。\n",
    "#  - 日本語BERT（BertJapaneseTokenizer）は「形態素解析→WordPiece」の二段で分割する。\n",
    "#    分割は辞書・語彙・バージョンに依存するため、再現性は“性質（不変量）”で管理する。\n",
    "#\n",
    "# 実務Tips：\n",
    "#  - MacBook（Apple Silicon）では CUDA は使えないため、Metal(MPS) を優先する。\n",
    "#  - 推論（補完）だけなら model.eval() ＋ torch.no_grad() でメモリ＆速度最適化。\n",
    "#  - 学習する場合は DataCollatorForLanguageModeling を用い、mlm_probability を設定する。\n",
    "#  - トークナイザとモデルは必ず同じモデル名（語彙）を使う（IDずれ防止）。\n",
    "# =========================================================\n",
    "\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "\n",
    "\n",
    "# --- デバイス検出（Mac優先：MPS → CUDA → CPU） ---\n",
    "def get_best_device() -> torch.device:\n",
    "    # Apple Silicon + macOS（MPSバックエンド）\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    # 他環境に移植した場合に備えて CUDA も許容\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # どれも不可なら CPU\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"[info] using device = {device}\")\n",
    "\n",
    "# --- モデル名（WWM版日本語BERT）：トークナイザとモデルはペアで統一 ---\n",
    "model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "\n",
    "# トークナイザ（形態素解析→WordPiece）。特殊トークン：[CLS]/[SEP]/[PAD]/[MASK]/[UNK]\n",
    "# - `mask_token` と `mask_token_id` は MLM の置換に使用する。\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# MLM 用のヘッド付き BERT（語彙サイズ×隠れ次元の出力層を持つ）\n",
    "# - 出力：logits 形状 [B, L, |Vocab|]。各位置の語彙分布を返す。\n",
    "bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# モデルを最適デバイスへ移動（Mac では .cuda() ではなく .to(device) を使う）\n",
    "bert_mlm = bert_mlm.to(device)\n",
    "\n",
    "# 推論モード（補完タスクなど）：dropout を停止\n",
    "bert_mlm.eval()\n",
    "\n",
    "# --- 参考：MLM 推論の最小例（必要ならコメント解除して確認） ---\n",
    "# text = f\"明日は{tokenizer.mask_token}だ。\"\n",
    "# enc = tokenizer(text, return_tensors='pt')\n",
    "# enc = {k: v.to(device) for k, v in enc.items()}\n",
    "# with torch.no_grad():\n",
    "#     logits = bert_mlm(**enc).logits  # [1, L, V]\n",
    "# mask_index = (enc[\"input_ids\"][0] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0].item()\n",
    "# topk = torch.topk(logits[0, mask_index], k=5).indices.tolist()\n",
    "# candidates = [tokenizer.decode([i]) for i in topk]\n",
    "# print(\"Top-5 candidates at [MASK]:\", candidates)\n",
    "\n",
    "# --- 参考：学習時のスケッチ ---\n",
    "# from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer,\n",
    "#     mlm=True,\n",
    "#     mlm_probability=0.15  # 15% を [MASK]（WWM版モデルの事前学習と整合的に設定）\n",
    "# )\n",
    "# # Trainer(...) を用いて学習。PAD位置は自動でロスから除外される（ignore_index=-100）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65bf0913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens: ['今日', 'は', '[MASK]', 'へ', '行く', '。']\n",
      "# ids: [3246, 9, 4, 118, 3488, 8]\n",
      "# mask token: [MASK] mask id: 4 pos(tok): 2 pos(id): 2\n",
      "# input_ids: [[2, 3246, 9, 4, 118, 3488, 8, 3]]\n",
      "# attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "# tokens (with specials): ['[CLS]', '今日', 'は', '[MASK]', 'へ', '行く', '。', '[SEP]']\n",
      "# decode (skip specials): 今日 は へ 行く 。\n"
     ]
    }
   ],
   "source": [
    "# 5-4\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#  - 入力文に含まれる [MASK] を含めて、日本語BERT用トークナイザでサブワード列へ分割し、挙動を確認する。\n",
    "#\n",
    "# 理論メモ（重要ポイント）：\n",
    "#  - MLM（Masked Language Modeling）では、[MASK] 位置の語彙分布を予測する（学習時は [MASK] 以外の損失を無視）。\n",
    "#  - `BertJapaneseTokenizer` は「形態素解析 → WordPiece」の二段で分割する。分割境界は辞書・語彙・バージョンに依存。\n",
    "#  - `tokenize()` は **特殊トークン [CLS]/[SEP] を付けない**“素のサブワード列”を返す。\n",
    "#  - `[MASK]` は **特殊トークンとして保存**される想定（= 文字列 \"[MASK]\" がそのままトークン列に現れる）。\n",
    "#    ※ 安全のため、リテラル \"[MASK]\" 直書きより `tokenizer.mask_token` を使うのが堅牢。\n",
    "#  - `encode()` / `tokenizer(..., return_tensors='pt')` は **ID列**を返し、既定で [CLS]/[SEP] を付与して下流モデルに直結できる。\n",
    "# =========================================================\n",
    "\n",
    "# 保険：tokenizer 未定義なら初期化（WWM版日本語BERTとペアで統一）\n",
    "try:\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    from transformers import BertJapaneseTokenizer\n",
    "\n",
    "    model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# --- 入力文：安全のため tokenizer.mask_token を用いて [MASK] を埋め込む ---\n",
    "text = f\"今日は{tokenizer.mask_token}へ行く。\"\n",
    "\n",
    "# --- サブワード列へ分割（特殊トークン [CLS]/[SEP] は付かない）---\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"# tokens:\", tokens)\n",
    "\n",
    "# --- ID への写像（[MASK] が mask_token_id に対応することを確認）---\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"# ids:\", ids)\n",
    "\n",
    "mask_tok = tokenizer.mask_token  # 例：\"[MASK]\"\n",
    "mask_id = tokenizer.mask_token_id  # 例：mask の語彙ID\n",
    "mask_pos_tok = tokens.index(mask_tok) if mask_tok in tokens else None\n",
    "mask_pos_id = ids.index(mask_id) if mask_id in ids else None\n",
    "print(\n",
    "    \"# mask token:\",\n",
    "    mask_tok,\n",
    "    \"mask id:\",\n",
    "    mask_id,\n",
    "    \"pos(tok):\",\n",
    "    mask_pos_tok,\n",
    "    \"pos(id):\",\n",
    "    mask_pos_id,\n",
    ")\n",
    "\n",
    "# --- 参考：特殊トークン付与あり（モデル入力に近い形；[CLS] と [SEP] を含む）---\n",
    "enc = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "print(\"# input_ids:\", enc[\"input_ids\"].tolist())\n",
    "print(\"# attention_mask:\", enc[\"attention_mask\"].tolist())\n",
    "print(\"# tokens (with specials):\", tokenizer.convert_ids_to_tokens(enc[\"input_ids\"][0]))\n",
    "\n",
    "# --- 復元表示（可逆ではない場合がある点に注意：空白や正規化、未知語の影響）---\n",
    "print(\n",
    "    \"# decode (skip specials):\",\n",
    "    tokenizer.decode(enc[\"input_ids\"][0], skip_special_tokens=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920d8ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using device = mps\n"
     ]
    }
   ],
   "source": [
    "# 5-5\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#  - [MASK] を含む文 `text` を符号化し、MLM（BertForMaskedLM）へ入力して\n",
    "#    各トークン位置の語彙分布（logits）を得る。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - MLM（Masked Language Modeling）では [MASK] 位置での語彙分布 p(token | context) を予測する。\n",
    "#    モデル出力 `logits` は形状 [B, L, |V|]（バッチ×系列長×語彙サイズ）であり、\n",
    "#    「分類スコア」は“各トークン位置における語彙分類のスコア”を意味する。\n",
    "#  - `tokenizer.encode(text, return_tensors='pt')` は **ID列テンソル** を返す（既定で [CLS]/[SEP] 付与）。\n",
    "#    研究・実務では `tokenizer(text, return_tensors='pt', padding=..., truncation=...)` の使用が推奨だが、\n",
    "#    単文・短文であれば `encode` でも問題ない。\n",
    "#  - attention_mask を省略した場合、PAD を含まない単文では実害は基本的にない。\n",
    "#    ただしバッチやパディングを伴う運用では attention_mask を明示するのが安全。\n",
    "#  - デバイス：MacBook(Apple Silicon) では CUDA は使えない。Metal(MPS) を優先し、\n",
    "#    それも不可なら CPU を用いる。モデルとテンソルは**常に同じ device** に揃える。\n",
    "# =========================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# --- デバイス検出（Mac優先：MPS → CUDA → CPU） ---\n",
    "def get_best_device() -> torch.device:\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")  # Apple GPU (Metal)\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")  # 他環境への持ち出し用の保険\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"[info] using device = {device}\")\n",
    "\n",
    "# 文章を符号化し、GPU（またはMPS/CPU）に配置する。\n",
    "# - encode は既定で special tokens（[CLS]/[SEP]）を付与する。\n",
    "# - return_tensors='pt' により torch.Tensor（形状 [B=1, L]）で取得。\n",
    "input_ids = tokenizer.encode(\n",
    "    text, return_tensors=\"pt\"\n",
    ")  # 例：tensor([[CLS, ..., MASK, ..., SEP]])\n",
    "# 元コード： input_ids = input_ids.cuda()\n",
    "# Mac では .cuda() は不可。device に合わせて移動する：\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# 念のためモデル側も同じデバイスへ（5-3でto(device)済みなら実質ノーオペ）\n",
    "bert_mlm = bert_mlm.to(device)\n",
    "bert_mlm.eval()  # 推論のみ。学習なら train() に切替\n",
    "\n",
    "# BERTに入力し、語彙スコア（logits）を得る。\n",
    "# ※ 元コメントの「系列長を揃える必要がない」は“単文かつ短文で PAD を使わない”場合に限って概ね正しい。\n",
    "#    バッチや長文運用では padding / attention_mask を明示すること。\n",
    "with torch.no_grad():  # 推論では計算グラフを作らない：省メモリ・高速化\n",
    "    output = bert_mlm(\n",
    "        input_ids=input_ids\n",
    "    )  # 他に attention_mask 等を省略（単文短文の前提）\n",
    "    scores = output.logits  # 形状 [B, L, |V|]：各位置の語彙ロジット\n",
    "\n",
    "# --- 参考（任意）：[MASK] 位置の Top-k 候補を取得する手順（コメントアウト） ---\n",
    "# mask_id = tokenizer.mask_token_id\n",
    "# mask_pos = (input_ids[0] == mask_id).nonzero(as_tuple=True)[0].item()  # [MASK] の位置（L次元）\n",
    "# topk = torch.topk(scores[0, mask_pos], k=5).indices.tolist()\n",
    "# candidates = [tokenizer.decode([i]) for i in topk]  # 語彙ID → 文字列\n",
    "# print(\"Top-5 candidates at [MASK]:\", candidates)\n",
    "\n",
    "# --- 参考（理論補足）---\n",
    "# - Self-Attention：softmax(QK^T / √d_k) V。系列長 L に対して計算量 O(L^2 H)。\n",
    "#   長文・バッチ運用では L を適切に管理（truncation/stride/動的パディング）すること。\n",
    "# - PAD を含む場合は attention_mask=0 の位置が softmax 前に -∞ 相当で抑制され、注意が向かない（勾配も基本流れない）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f2c228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日は、へ行く。\n"
     ]
    }
   ],
   "source": [
    "# 5-6\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#  - MLM の出力 `scores`（形状 [B, L, |V|]）から [MASK] 位置の最尤トークン（argmax）を選び，\n",
    "#    元の文の [MASK] をそのトークンで置換して，人間可読な文に復元する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - `scores[0, pos]` は語彙分布（ロジット）。argmax は MAP 推定（最尤語彙）に対応する。\n",
    "#  - [MASK] の語彙IDを **固定値で仮定しない**（モデルによって異なる）。必ず `tokenizer.mask_token_id` を用いる。\n",
    "#  - サブワード（WordPiece）の場合，`convert_ids_to_tokens` は '##' 接頭辞が付くことがある。\n",
    "#    文字列復元としては `tokenizer.decode([id])` の方が自然なことが多い（空白処理や正規化を担う）。\n",
    "#  - 複数 [MASK] がある場合：ここでは「独立同時置換」（各位置で argmax）を行う。\n",
    "#    逐次的に再エンコードして条件付きで更新する「反復置換」は別戦略（より厳密だが計算増）。\n",
    "# =========================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "# --- 1) [MASK] の語彙IDを取得（モデル依存；固定値を使わない） ---\n",
    "mask_id = (\n",
    "    tokenizer.mask_token_id\n",
    ")  # 例：東北大日本語BERTでは 4 のことが多いが、必ず動的に取得する\n",
    "mask_tok = tokenizer.mask_token  # 例：\"[MASK]\"\n",
    "\n",
    "# --- 2) 入力ID列から [MASK] の位置を列挙（複数対応） ---\n",
    "# input_ids: 形状 [B=1, L] を想定（5-5の encode に一致）\n",
    "mask_positions = (input_ids[0] == mask_id).nonzero(as_tuple=True)[0].tolist()\n",
    "if len(mask_positions) == 0:\n",
    "    raise ValueError(\n",
    "        \"入力系列に [MASK] が見つかりません。text に tokenizer.mask_token を含めてください。\"\n",
    "    )\n",
    "\n",
    "# --- 3) 各 [MASK] 位置で最尤トークンIDを取得（argmax over vocab） ---\n",
    "# scores: 形状 [B=1, L, |V|] を想定（5-5の出力 logits）\n",
    "pred_token_ids = []\n",
    "for pos in mask_positions:\n",
    "    # scores[0, pos] : [|V|] ベクトル → 最尤語彙ID\n",
    "    best_id = scores[0, pos].argmax(-1).item()\n",
    "    pred_token_ids.append(best_id)\n",
    "\n",
    "# --- 4) ID → 文字列へ変換（decode を優先） ---\n",
    "# decode は空白や '##' の扱いをよしなに調整してくれる（単一IDでも有用）\n",
    "pred_tokens = [\n",
    "    tokenizer.decode([tid], skip_special_tokens=True).strip() for tid in pred_token_ids\n",
    "]\n",
    "\n",
    "# フォールバック（必要時）：WordPieceの『##』を除去して素片文字列だけを得る\n",
    "pred_tokens_fallback = [\n",
    "    tokenizer.convert_ids_to_tokens(tid).lstrip(\"##\") for tid in pred_token_ids\n",
    "]\n",
    "\n",
    "# decode 側で万が一空文字になった箇所はフォールバックを使う\n",
    "pred_tokens = [\n",
    "    pt if pt != \"\" else pf for pt, pf in zip(pred_tokens, pred_tokens_fallback)\n",
    "]\n",
    "\n",
    "# --- 5) 元のテキスト中の [MASK] を順に置換（複数 [MASK] に対応） ---\n",
    "# str.replace は全置換になるため、分割→挿入→連結で「順番に」置換する\n",
    "parts = text.split(mask_tok)  # [seg0, seg1, ..., segN]  （N = マスク数）\n",
    "filled_segments = []\n",
    "for i, seg in enumerate(parts[:-1]):\n",
    "    filled_segments.append(seg)\n",
    "    # i 番目の [MASK] に対する候補を挿入\n",
    "    filled_segments.append(pred_tokens[i] if i < len(pred_tokens) else \"\")\n",
    "filled_segments.append(parts[-1])\n",
    "\n",
    "text_filled = \"\".join(filled_segments)\n",
    "print(text_filled)\n",
    "\n",
    "# --- 備考 ---\n",
    "# - 「独立同時置換」では各 [MASK] を互いに独立に推定するため，相互依存（共起制約）を捉えにくい。\n",
    "#   文脈整合性を高めたい場合は，1つずつ置換→再エンコード→再推論の反復法を検討する。\n",
    "# - サブワードが選ばれた場合でも decode により自然な表記になりやすいが，\n",
    "#   場合によっては不自然な連結になる可能性がある（語境界／表記揺れのため）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c32cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
