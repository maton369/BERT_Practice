{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c83cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-2\n",
    "\n",
    "# =========================================================\n",
    "# BERT（Bidirectional Encoder Representations from Transformers）の理論メモ\n",
    "# ---------------------------------------------------------\n",
    "# ■ 目的\n",
    "#  - 文脈の双方向（左←→右）を同時に考慮した潜在表現 h ∈ R^H を得る。\n",
    "#  - 事前学習（Pretraining）で一般的言語能力を獲得し、下流タスク（分類・系列ラベリング・QA等）に微調整（Fine-tuning）で適用。\n",
    "#\n",
    "# ■ 事前学習タスク\n",
    "#  - MLM（Masked Language Modeling）:\n",
    "#    入力トークン列 x の一部を [MASK] 等で隠し、p(x_masked | context) を最大化する。\n",
    "#    近似的には、各マスク位置 i に対して softmax(W h_i) の対数尤度を最大化。\n",
    "#  - NSP（Next Sentence Prediction）:\n",
    "#    文 A の直後に文 B が来るかを 2 値分類（Tohoku BERT では採用）。RoBERTa 系では廃止例もある。\n",
    "#\n",
    "# ■ モデル（BertModel）の内部\n",
    "#  - 入力埋め込み: token + position + segment（token_type）を加算して X ∈ R^{B×L×H} を作る。\n",
    "#  - Transformer Encoder（L層）: 各層で Multi-Head Self-Attention + FFN。\n",
    "#    • Attention は Scaled Dot-Product:\n",
    "#        Q = XW_Q, K = XW_K, V = XW_V  (各 W_* ∈ R^{H×d_k})\n",
    "#        scores = Q K^T / sqrt(d_k)\n",
    "#        A = softmax(scores + mask)\n",
    "#        head = A V\n",
    "#      複数 head を結合し線形写像で次層へ。\n",
    "#    • 計算量: O(L^2 H)（長文入力ほど二乗で増える）; メモリもほぼ O(L^2)。\n",
    "#  - 出力:\n",
    "#    • last_hidden_state: 形状 [B, L, H]。各トークン位置の文脈化表現。\n",
    "#    • pooler_output: 形状 [B, H]。最終層の [CLS] ベクトルに線形＋tanh を適用（主に文分類の初期ベースライン）。\n",
    "#    • hidden_states, attentions（オプション）: 各層の中間表現やアテンション行列を返す。\n",
    "#\n",
    "# ■ 日本語トークナイザ（BertJapaneseTokenizer）\n",
    "#  - 典型的には形態素解析（MeCab/fugashi 等）で語彙単位に分割 → WordPiece に細分化。\n",
    "#  - 特殊トークン: [CLS]（先頭）, [SEP]（区切り）, [PAD], [MASK], [UNK] など。\n",
    "#  - 下流入力の慣例:\n",
    "#      input_ids        : [B, L]（語彙ID）\n",
    "#      attention_mask   : [B, L]（pad 0/非pad 1）\n",
    "#      token_type_ids   : [B, L]（文 A=0 / 文 B=1）\n",
    "#    これらを model(**batch) で渡す。\n",
    "#\n",
    "# ■ 実運用の注意\n",
    "#  - 最大系列長: 多くの BERT は L ≤ 512。超える場合は分割や Longformer 等の検討。\n",
    "#  - 学習: AdamW + 学習率ウォームアップ + 余弦減衰などが定番。weight decay は LayerNorm/バイアス除外。\n",
    "#  - 乱数・再現性: torch.manual_seed, numpy.random.seed を固定。dropout の影響も考慮。\n",
    "#  - 参照: Devlin et al., 2018; Tohoku NLP \"cl-tohoku/bert-base-japanese\" 系列\n",
    "# =========================================================\n",
    "\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 依存関係の注記（インストール時の目安）\n",
    "#  - transformers >= 4.x\n",
    "#  - fugashi, ipadic（や unidic-lite）: BertJapaneseTokenizer の形態素解析で利用されることが多い\n",
    "#    例) pip install transformers fugashi ipadic\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# （参考）実行例：事前学習済み日本語BERTの読み込み\n",
    "#   ※ 下の例は「使い方の目安」を示すためのコメントであり、実行は任意です。\n",
    "#\n",
    "# # 1) 日本語BERTのトークナイザ／モデルをロード\n",
    "# tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "# model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "# model.eval()  # 推論時は eval モードで dropout 等を停止\n",
    "#\n",
    "# # 2) テキストを ID 列へ変換\n",
    "# #   return_tensors=\"pt\" で PyTorch テンソルを自動作成\n",
    "# batch = tokenizer(\n",
    "#     [\"今日は良い天気ですね。\", \"しかし少し風が強いかもしれません。\"],\n",
    "#     padding=True, truncation=True, max_length=128, return_tensors=\"pt\"\n",
    "# )\n",
    "# # batch: dict で input_ids/attention_mask/token_type_ids を含む\n",
    "# # 形状: input_ids.shape == [B, L]\n",
    "#\n",
    "# # 3) モデルへ前向き計算\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**batch, output_attentions=False, output_hidden_states=False)\n",
    "#\n",
    "# # 4) 出力の形状\n",
    "# # outputs.last_hidden_state: [B, L, H]  各トークン位置の表現\n",
    "# # outputs.pooler_output   : [B, H]      [CLS] 由来（文表現のベースライン）\n",
    "#\n",
    "# # 5) 用途の例\n",
    "# # - 文分類: pooler_output か、[CLS] の last_hidden_state[:, 0, :] を線形分類器へ\n",
    "# # - トークン分類（固有表現抽出など）: last_hidden_state を各位置で線形＋softmax\n",
    "# # - QA: [CLS]/[SEP] を含むペア入力で start/end スパンを回帰\n",
    "#\n",
    "# # 6) 理論と設計の対応\n",
    "# # - attention_mask は pad 位置へのソフトマックスを -∞ マスクで抑制（scores に加算）\n",
    "# # - token_type_ids は NSP/ペア入力の区別（A:0, B:1）。単文なら全 0 でよい。\n",
    "# # - 学習では MLM/NSP（または SOP）に対応するヘッドを別途持つ（BertForPreTraining 等）。\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67cf6330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df59c5a1fb354d99ad3c96f411f0c783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9975b6c464cc46f880ed7d6e4d8053e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e09ee566cb349518530eaf57b946f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/479 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4-3\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 理論メモ：Whole Word Masking（WWM）版日本語BERTを使う理由\n",
    "# ---------------------------------------------------------\n",
    "# ◇ 標準MLM vs. Whole Word Masking\n",
    "#  - 標準のMLM（WordPiece単位の隠し方）では、語が「サブワード」に分割された場合、\n",
    "#    その一部のサブワードだけが[MASK]になることが多い。\n",
    "#  - WWMは「同じ語（= 形態素→WordPieceに分解される前の“語”）」に属する全サブワードを\n",
    "#    まとめて同時にマスクする。これによりモデルは「語レベルの完形復元」を学ぶ圧力が強まり、\n",
    "#    文脈一貫性や語彙レベルの表現獲得に寄与する（サブワード断片の当て推量を減らす）。\n",
    "#  - 直感的には、より強い隠し課題（harder pretext task）→汎化に寄与する可能性。\n",
    "#    ただし学習難易度は上がりうる（学習安定化はデータとハイパラ次第）。\n",
    "#\n",
    "# ◇ 日本語での影響\n",
    "#  - 日本語は語境界が空白で明示されないため、形態素解析（例：MeCab + 辞書）で語単位を推定→\n",
    "#    さらにWordPieceで細分化、という段階を踏む。この「語」単位でマスクを丸ごと掛けるのがWWM。\n",
    "#  - 形態素解析の精度や辞書選択（ipadic / unidic-lite 等）に依存する側面がある。\n",
    "#\n",
    "# ◇ 実務上の注意\n",
    "#  - このモデル名 'tohoku-nlp/bert-base-japanese-whole-word-masking' は、\n",
    "#    東北大NLPが公開している日本語BERT（WWM版）の重み・語彙を指す。\n",
    "#  - Tokenizer/Modelは対応するペアを使う（語彙の不一致はIDずれ→性能劣化）。\n",
    "#  - 依存関係：transformers、fugashi、ipadic（またはunidic-lite）等を事前にインストール。\n",
    "#    例：pip install transformers fugashi ipadic\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"  # WWM版日本語BERT（語レベルでの一括マスキングで事前学習）\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# （参考）挙動確認スニペット：※必要なら実行\n",
    "# text = \"深層学習の事前学習モデルBERTは強力です。\"\n",
    "# enc = tokenizer(\n",
    "#     text,\n",
    "#     return_tensors=\"pt\",      # PyTorchテンソルで返す\n",
    "#     padding=\"max_length\",     # 最大長にパディング（推論/バッチ化の整列用）\n",
    "#     truncation=True,          # 上限長超過時に切り詰め\n",
    "#     max_length=32\n",
    "# )\n",
    "# # enc[\"input_ids\"].shape == [1, 32], enc[\"attention_mask\"].shape == [1, 32]\n",
    "# # WWMは事前学習時のマスキング戦略であり、推論時のトークナイズ結果（input_ids自体）が\n",
    "# # 直接変わるわけではない点に注意（学習済み表現に差が出る）。\n",
    "#\n",
    "# # なお、下流タスクでは対応するモデル（例：BertModel/BertForSequenceClassification など）を\n",
    "# # 同じ `model_name` から読み出し、tokenizerとペアで使うことが重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e26b6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-4 ～ 4-6\n",
    "# =========================================================\n",
    "# 理論補足：BertJapaneseTokenizerのトークナイズ動作（日本語×WordPiece）\n",
    "# ---------------------------------------------------------\n",
    "# ■ 概要\n",
    "#  - BertJapaneseTokenizer は「形態素解析 → WordPiece 分割」という2段階でトークン化する。\n",
    "#    1) 形態素解析：文を語（形態素）へ分割（MeCab + 辞書 ipadic / unidic-lite 等）\n",
    "#    2) WordPiece：各「語」を語彙に基づいて更にサブワードへ分割（語彙外は細分化、最悪 [UNK]）\n",
    "#  - 返り値 tokenizer.tokenize(text) は特殊トークンを付与しない素のサブワード列（list[str]）。\n",
    "#    例：['自', '然', '言', '語', '処', '理'] のような字単位、あるいは ['自然', '言語', '処理'] など\n",
    "#    語彙と辞書の組み合わせにより変動する（＝「必ずこの分割になる」とは限らない）。\n",
    "#\n",
    "# ■ Whole Word Masking（WWM）との関係\n",
    "#  - 本トークナイザ自体は「推論・前処理」でのサブワード列を返すのみ。\n",
    "#  - WWM は「事前学習時のマスク戦略」で、同一“語”（形態素に相当）に属するサブワードを\n",
    "#    まとめて同時にマスクする。推論時の tokenize 結果は WWM の有無で直接は変わらない。\n",
    "#\n",
    "# ■ 文ごとの観察ポイント\n",
    "#  (A) 「明日は自然言語処理の勉強をしよう。」\n",
    "#    - 「自然言語処理」は複合名詞。形態素解析の辞書設定により\n",
    "#      「自然/言語/処理」や「自然言語/処理」等に分かれうる。\n",
    "#    - その後、WordPiece がさらに細分化（語彙にあればそのまま、無ければ細切れ）。\n",
    "#  (B) 「明日はマシンラーニングの勉強をしよう。」\n",
    "#    - カタカナ複合語。「マシン」「ラーニング」に分かれる場合や\n",
    "#      「マシンラーニング」単語→サブワード細分の可能性がある。\n",
    "#  (C) 「機械学習を中国語にすると机器学习だ。」\n",
    "#    - 末尾の「机器学习」は中国語（簡体字）。日本語辞書では未知語扱いになりやすく、\n",
    "#      WordPiece が文字単位や部分列へ分解、最悪 [UNK] へフォールバックしうる。\n",
    "#    - BERT の日本語語彙は CJK を広く含むため [UNK] にならず字単位分割で拾えることも多い。\n",
    "#\n",
    "# ■ 実務上のTips\n",
    "#  - 再現性：辞書種別（ipadic / unidic-lite）・バージョン、transformers のバージョンで分割が変わる。\n",
    "#  - 下流処理：tokenize → convert_tokens_to_ids → [CLS]/[SEP] 付与 → attention_mask 作成、が一般手順。\n",
    "#  - 形状把握：下流モデル入力は通常 encode_plus / __call__（return_tensors=\"pt\"）を使うのが安全。\n",
    "# =========================================================\n",
    "\n",
    "# 既に 4-3 で `tokenizer` を作っている想定。\n",
    "# セッション直実行でも動くように保険として用意（未定義なら初期化）。\n",
    "try:\n",
    "    tokenizer  # 変数が存在するかチェック\n",
    "except NameError:\n",
    "    from transformers import BertJapaneseTokenizer\n",
    "\n",
    "    model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 4-4：語彙・辞書により複合名詞「自然言語処理」の扱いが変動しうる点に注目\n",
    "tokens_44 = tokenizer.tokenize(\"明日は自然言語処理の勉強をしよう。\")\n",
    "# デバッグ表示（必要ならコメント解除）\n",
    "# print(tokens_44)\n",
    "\n",
    "# 4-5：カタカナ複合語の分割（「マシン」「ラーニング」等）やサブワード接頭辞（##）の有無に注目\n",
    "tokens_45 = tokenizer.tokenize(\"明日はマシンラーニングの勉強をしよう。\")\n",
    "# print(tokens_45)\n",
    "\n",
    "# 4-6：簡体字中国語「机器学习」の未知語挙動（字単位分割や [UNK] フォールバック）に注目\n",
    "tokens_46 = tokenizer.tokenize(\"機械学習を中国語にすると机器学习だ。\")\n",
    "# print(tokens_46)\n",
    "\n",
    "# （参考）ID 列へ変換して下流モデル入力へ繋ぐ例：\n",
    "# enc_44 = tokenizer('明日は自然言語処理の勉強をしよう。', return_tensors='pt')\n",
    "# # enc_44 は input_ids / attention_mask / token_type_ids を含む辞書\n",
    "# # enc_44['input_ids'].shape == [1, L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c542f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
