{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a1aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-1\n",
    "!mkdir chap8\n",
    "%cd ./chap8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba06fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-3\n",
    "import itertools\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForTokenClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 日本語学習済みモデル\n",
    "MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8cf15c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ＡＢＣ -> ABC\n",
      "ABC -> ABC\n",
      "１２３ -> 123\n",
      "123 -> 123\n",
      "アイウ -> アイウ\n",
      "ｱｲｳ -> アイウ\n"
     ]
    }
   ],
   "source": [
    "# 8-4（説明コメント付き：Unicode 正規化 NFKC による表記ゆれの吸収）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - Unicode の **互換性分解 + 合成**（NFKC）で、全角/半角・互換文字（例：半角ｶﾅ・ローマ数字・① 等）\n",
    "#    を正規化し、検索/重複排除/機械学習前処理での表記ゆれを減らす。\n",
    "#\n",
    "# 理論メモ（NFC/NFD/NFKC/NFKD の違い）：\n",
    "#  - NFC: 正規分解 → 合成（互換性は無視）。見た目を保ちながら正規の合成形式へ。\n",
    "#  - NFD: 正規分解のみ（結合文字にバラす）。検索・照合の下処理に使うことがある。\n",
    "#  - NFKC: **互換分解**（見た目は同じでも “意味的に同一視される” 文字を分解）→ 合成。\n",
    "#          例：全角英数/記号、半角ｶﾅ、ローマ数字 Ⅳ、丸数字 ① などを通常の文字に畳み込む。\n",
    "#  - NFKD: 互換分解のみ。\n",
    "# 互換分解は “情報の落ち” が起こり得る（例：①→1、㍍→メートル→m など）。監査用途では注意。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "# NFKC で正規化する関数\n",
    "# - 全角英数→半角、半角ｶﾅ→全角カタカナ、結合記号の統合 等を一括で行う\n",
    "normalize = lambda s: unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "# 動作例：全角/半角の統一（学習前の前処理・検索キー作成などで有効）\n",
    "print(f'ＡＢＣ -> {normalize(\"ＡＢＣ\")}')  # 全角アルファベット → 半角 \"ABC\"\n",
    "print(f'ABC -> {normalize(\"ABC\")}')  # 既に半角 → 変化なし\n",
    "print(f'１２３ -> {normalize(\"１２３\")}')  # 全角数字 → 半角 \"123\"\n",
    "print(f'123 -> {normalize(\"123\")}')  # 既に半角 → 変化なし\n",
    "print(f'アイウ -> {normalize(\"アイウ\")}')  # 全角カタカナ → 変化なし\n",
    "print(f'ｱｲｳ -> {normalize(\"ｱｲｳ\")}')  # 半角ｶﾅ → 全角カタカナ \"アイウ\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 追加の知見（必要ならテストして確認）：\n",
    "#  - 濁点付き半角ｶﾅ（例：ｶﾞ）も結合が解決され全角「ガ」に統一される：\n",
    "#      normalize(\"ｶﾞ\") == \"ガ\"\n",
    "#  - 互換文字の折り畳み：\n",
    "#      normalize(\"ⅠⅡⅢ\") -> \"III\"  （ローマ数字 → ラテン大文字）\n",
    "#      normalize(\"①②\")   -> \"12\"   （丸数字 → 通常数字）\n",
    "#  - 正規化は “可逆でない” ことがあるため、**原文は別項目で保存**しておくと安全。\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bea67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-5（説明コメント付き：日本語BERT用 NER 前処理ユーティリティ）\n",
    "# =============================================================================\n",
    "# 前提：\n",
    "#  - BertJapaneseTokenizer（transformers）を継承して、固有表現抽出（NER）向けの\n",
    "#    2種類のエンコード（教師あり/教師なし）と、モデル出力から固有表現スパンを復元する\n",
    "#    補助関数を提供するクラスである。\n",
    "#  - ラベリング方式は BIO ではなく「整数 ID によるスパン連結」方式（O=0, その他=タイプID）。\n",
    "#    → 連続する同一ラベルのトークン列を 1 つのエンティティにまとめる前提。\n",
    "#  - 重複・ネスト・オーバーラップするエンティティは表現できない（単純スパンのみ）。\n",
    "#  - prepare_for_model により [CLS]/[SEP] の特殊トークンが付与される前提で、ラベルを先頭/末尾0に整合。\n",
    "#  - max_length を超える部分は切り捨てられるため、切断されたエンティティは欠落し得る点に注意。\n",
    "#  - encode_plus_untagged では、MeCab 分かち書き（word_tokenizer）→ WordPiece（subword_tokenizer）\n",
    "#    で二段階トークナイズを行い、トークン表層を原文へシーケンシャルマッチしてスパンを得る。\n",
    "#    同一トークンの繰り返しが多い文でも、左から貪欲にマッチしていくため順次対応できる。\n",
    "#  - return_tensors='pt' 指定時、encoding のみ Tensor 化する（spans はリストのまま返る）。\n",
    "#    下流でテンソルを期待する場合は適宜変換すること。\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class NER_tokenizer(BertJapaneseTokenizer):\n",
    "\n",
    "    def encode_plus_tagged(self, text, entities, max_length):\n",
    "        \"\"\"\n",
    "        文章とそれに含まれる固有表現が与えられた時に、\n",
    "        符号化とラベル列の作成を行う。\n",
    "\n",
    "        引数:\n",
    "          text: str\n",
    "            元の文章（原文）。エンティティ抽出の対象。\n",
    "          entities: List[Dict]\n",
    "            [{'span': [start, end], 'type_id': int}, ...] の形を想定。\n",
    "            span は原文 text における [start, end) の半開区間、type_id は 0 以外の整数。\n",
    "          max_length: int\n",
    "            BERT 入力の最大系列長。prepare_for_model で [CLS]/[SEP] を含む長さに正規化される。\n",
    "\n",
    "        戻り値:\n",
    "          encoding: Dict[str, List[int]]\n",
    "            input_ids / attention_mask / token_type_ids / labels を含む辞書。\n",
    "            labels は [0]=CLS, 末尾 [0]=SEP/必要に応じ PAD を 0 で詰める。\n",
    "        \"\"\"\n",
    "        # --- 前処理: エンティティを開始位置で昇順ソート ---\n",
    "        entities = sorted(entities, key=lambda x: x[\"span\"][0])\n",
    "\n",
    "        # --- 原文を「非固有表現片(O=0)」「固有表現片(=type_id)」に分割して並べる ---\n",
    "        splitted = []  # 分割後の文字列片を順に蓄積\n",
    "        position = 0  # 直前に処理した末尾の次インデックス\n",
    "        for entity in entities:\n",
    "            start = entity[\"span\"][0]\n",
    "            end = entity[\"span\"][1]\n",
    "            label = entity[\"type_id\"]\n",
    "            # 非固有表現部分（直近の position からエンティティ開始まで）→ ラベル 0\n",
    "            splitted.append({\"text\": text[position:start], \"label\": 0})\n",
    "            # 固有表現部分（[start:end)）→ ラベル type_id\n",
    "            splitted.append({\"text\": text[start:end], \"label\": label})\n",
    "            position = end\n",
    "        # 最後のエンティティ以降の末尾テキストも非固有表現として追加\n",
    "        splitted.append({\"text\": text[position:], \"label\": 0})\n",
    "        # 空文字は除去（連続するエンティティで start==end 等により空が生じ得るため）\n",
    "        splitted = [s for s in splitted if s[\"text\"]]\n",
    "\n",
    "        # --- 片ごとにトークン化し、各トークンへ片に対応するラベルを付与 ---\n",
    "        tokens = []  # WordPiece トークン列\n",
    "        labels = []  # トークンに対するラベル（0 or type_id）\n",
    "        for text_splitted in splitted:\n",
    "            text = text_splitted[\"text\"]\n",
    "            label = text_splitted[\"label\"]\n",
    "            # BertJapaneseTokenizer.tokenize は日本語前処理（MeCab）+ WordPiece を内部で実施\n",
    "            tokens_splitted = self.tokenize(text)\n",
    "            labels_splitted = [label] * len(tokens_splitted)\n",
    "            tokens.extend(tokens_splitted)\n",
    "            labels.extend(labels_splitted)\n",
    "\n",
    "        # --- トークン列を ID 列に変換し、BERT 入力辞書へ整形 ---\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids, max_length=max_length, padding=\"max_length\", truncation=True\n",
    "        )  # prepare_for_model が [CLS]/[SEP] 等を付与し、長さを揃える\n",
    "\n",
    "        # --- ラベルの特殊トークン・PAD 整合 ---\n",
    "        # 先頭に CLS=0 を付与、本文ラベルは最大長-2（CLS/SEP）までに切詰め、末尾に SEP=0\n",
    "        labels = [0] + labels[: max_length - 2] + [0]\n",
    "        # さらに不足分（PAD 部分）を 0 で埋め、最終的に長さを max_length に一致させる\n",
    "        labels = labels + [0] * (max_length - len(labels))\n",
    "        encoding[\"labels\"] = labels\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def encode_plus_untagged(self, text, max_length=None, return_tensors=None):\n",
    "        \"\"\"\n",
    "        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
    "\n",
    "        引数:\n",
    "          text: str\n",
    "            元の文章。\n",
    "          max_length: Optional[int]\n",
    "            指定時は padding='max_length', truncation=True で固定長化。\n",
    "            未指定時は可変長（特殊トークン付与は維持）。\n",
    "          return_tensors: Optional[str]\n",
    "            'pt' を指定すると encoding の各値を torch.Tensor(batch次元つき) に変換。\n",
    "\n",
    "        戻り値:\n",
    "          encoding: Dict[str, List[int]] or Dict[str, torch.Tensor]\n",
    "            prepare_for_model の出力（[CLS]/[SEP] 付与後）。\n",
    "          spans: List[List[int]]\n",
    "            各トークンに対応する原文上の [start, end) 位置。\n",
    "            特殊トークン([CLS],[SEP],[PAD])の位置は [-1,-1] のダミーにする。\n",
    "        \"\"\"\n",
    "        # --- 形態素（MeCab）→ サブワード（WordPiece）の二段トークナイズ ---\n",
    "        tokens = []  # WordPiece トークン\n",
    "        tokens_original = (\n",
    "            []\n",
    "        )  # スパン計算用の「表層文字列」列（'##' 除去/UNK は単語そのまま）\n",
    "        words = self.word_tokenizer.tokenize(text)  # MeCab による単語列\n",
    "        for word in words:\n",
    "            # 単語を WordPiece に分割\n",
    "            tokens_word = self.subword_tokenizer.tokenize(word)\n",
    "            tokens.extend(tokens_word)\n",
    "            if (\n",
    "                tokens_word[0] == \"[UNK]\"\n",
    "            ):  # 未知語は WordPiece に分割できない → 原単語をそのまま使う\n",
    "                tokens_original.append(word)\n",
    "            else:\n",
    "                # '##' 接頭辞を削って表層形を復元（スパン同定に用いる）\n",
    "                tokens_original.extend(\n",
    "                    [token.replace(\"##\", \"\") for token in tokens_word]\n",
    "                )\n",
    "\n",
    "        # --- 原文上のスパンを左から順次マッチで同定（空白/記号も考慮して進める）---\n",
    "        position = 0  # 原文上の走査位置（左から前進のみ）\n",
    "        spans = []  # 各トークンの [start, end) を蓄積\n",
    "        for token in tokens_original:\n",
    "            l = len(token)\n",
    "            while 1:\n",
    "                # 現在位置から長さ l を切り出して一致判定（空白等を飛ばすため不一致なら1文字進める）\n",
    "                if token != text[position : position + l]:\n",
    "                    position += 1\n",
    "                else:\n",
    "                    spans.append([position, position + l])\n",
    "                    position += l\n",
    "                    break\n",
    "        # ここでの手法は「逐次マッチ」のため、同一部分文字列の繰り返しがあっても左から順に整合が取れる。\n",
    "        # ただし、原文編集（正規化や空白折り畳み）を別途行う場合は、同じ変換を token 側にも適用しておくこと。\n",
    "\n",
    "        # --- BERT 入力辞書へ整形（固定長/可変長を分岐）---\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\" if max_length else False,\n",
    "            truncation=True if max_length else False,\n",
    "        )\n",
    "        sequence_length = len(\n",
    "            encoding[\"input_ids\"]\n",
    "        )  # [CLS]/[SEP] を含む最終トークン列長\n",
    "\n",
    "        # --- 特殊トークン分のダミー span を付与し、長さを encoding に一致させる ---\n",
    "        # 先頭 [CLS] 用のダミー\n",
    "        spans = [[-1, -1]] + spans[: sequence_length - 2]\n",
    "        # 末尾 [SEP] と PAD 分をダミーで埋める（長さを sequence_length に揃える）\n",
    "        spans = spans + [[-1, -1]] * (sequence_length - len(spans))\n",
    "\n",
    "        # --- 必要なら Tensor 化（encoding のみ。spans はリストで返す） ---\n",
    "        if return_tensors == \"pt\":\n",
    "            encoding = {k: torch.tensor([v]) for k, v in encoding.items()}\n",
    "\n",
    "        return encoding, spans\n",
    "\n",
    "    def convert_bert_output_to_entities(self, text, labels, spans):\n",
    "        \"\"\"\n",
    "        文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n",
    "\n",
    "        引数:\n",
    "          text: str\n",
    "            原文。\n",
    "          labels: Sequence[int]\n",
    "            各トークンのラベル（O=0, その他は type_id）。[CLS]/[SEP]/[PAD] 位置の分も含む想定。\n",
    "          spans: Sequence[List[int]]\n",
    "            各トークンの [start, end)。特殊トークンは [-1,-1]。\n",
    "\n",
    "        戻り値:\n",
    "          entities: List[Dict]\n",
    "            {\"name\": 原文片, \"span\": [start,end], \"type_id\": ラベルID} の配列。\n",
    "        \"\"\"\n",
    "        # --- 特殊トークン（span=-1）に対応するラベル/スパンを除去して本文トークンに限定 ---\n",
    "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
    "        spans = [span for span in spans if span[0] != -1]\n",
    "\n",
    "        # --- 連続する同一ラベルをまとめて 1 スパンのエンティティに復元 ---\n",
    "        # itertools.groupby により、labels の連続区間ごとにグルーピングする。\n",
    "        # 例: labels=[0,0,2,2,0,3] → (0区間)(2区間)(0区間)(3区間)\n",
    "        # ラベル0（O）は無視し、非0の区間のみエンティティを生成する。\n",
    "        entities = []\n",
    "        for label, group in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
    "\n",
    "            group = list(group)  # group は [(idx,label), ...] の列\n",
    "            start = spans[group[0][0]][0]  # 区間先頭トークンの start\n",
    "            end = spans[group[-1][0]][1]  # 区間末尾トークンの end（半開区間の終端）\n",
    "\n",
    "            if label != 0:  # O 以外のラベルのみエンティティ化\n",
    "                entity = {\n",
    "                    \"name\": text[start:end],  # 原文断片（復元テキスト）\n",
    "                    \"span\": [start, end],  # 原文上の [start,end)\n",
    "                    \"type_id\": label,  # ラベル ID（BIO ではなく type_id そのもの）\n",
    "                }\n",
    "                entities.append(entity)\n",
    "\n",
    "        # 注意：\n",
    "        #  - BIO 方式ではないため、同一ラベルが連続すれば 1 エンティティに連結される。\n",
    "        #    同じ type_id が離れて複数回出現する場合は、それぞれ別エンティティとして復元される。\n",
    "        #  - ネスト/オーバーラップは扱えない（単純連続区間のみ）。\n",
    "        #  - max_length による切詰めでエンティティの一部が失われると、復元されない場合がある。\n",
    "\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f4f30b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 8-6\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c10f011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 10271, 28486, 5, 546, 3000, 1518, 233, 13, 5, 1878, 2682, 9, 10750, 308, 10, 8, 3, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], 'labels': [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# 8-7（説明コメント付き：NER_tokenizer.encode_plus_tagged の動作確認）\n",
    "# =============================================================================\n",
    "# 目的：\n",
    "#  - 原文 `text` と、原文上のスパン（[start, end) の半開区間）＋ラベルIDからなる `entities`\n",
    "#    を与え、BERT 入力（input_ids/attention_mask/token_type_ids）に整形すると同時に、\n",
    "#    各トークンに対応する **ラベル列**（O=0, エンティティは type_id）を作成する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - encode_plus_tagged は「BIO 方式」ではなく **連結方式**（同一ラベルが連続するトークン列を1つの\n",
    "#    エンティティとみなす）前提でラベルを作る。特殊トークン [CLS]/[SEP]/[PAD] は 0 として埋める。\n",
    "#  - `span=[start,end]` は **半開区間**（start 文字を含み end 文字を含まない）。インデックスは 0 始まり。\n",
    "#  - トークン化は WordPiece なので、1語が複数トークンに分割されても、分割前の片に付けたラベルが\n",
    "#    そのまま各トークンに複製される設計。\n",
    "#  - `max_length` 超過分は切り捨てられるため、末尾側のエンティティが途中で切れると、対応トークンに\n",
    "#    ラベルが乗らず欠落し得る点に注意（長さ設計 or スライディングで対処）。\n",
    "#  - 直前に正規化（例：NFKC）を行った場合、**スパンは正規化後の文字列基準**で与えること（前後で\n",
    "#    文字長が変わるとズレる）。\n",
    "# =============================================================================\n",
    "\n",
    "# （互換/保守）NER_tokenizer が未インポート/未定義でも動くように最小限のフォールバックを用意\n",
    "try:\n",
    "    NER_tokenizer\n",
    "except NameError:\n",
    "    # ここではクラス本体は 8-5 で定義済み想定。未定義なら明示エラーにする。\n",
    "    raise RuntimeError(\n",
    "        \"NER_tokenizer クラスが未定義です。先に 8-5 の定義を実行してください。\"\n",
    "    )\n",
    "\n",
    "# 既存の `tokenizer` が NER_tokenizer でない場合は、同名で置き換えても構わない運用なら差し替える\n",
    "try:\n",
    "    tokenizer\n",
    "    has_encode_tagged = hasattr(tokenizer, \"encode_plus_tagged\")\n",
    "except NameError:\n",
    "    has_encode_tagged = False\n",
    "\n",
    "if not has_encode_tagged:\n",
    "    # 学習時と同じ語彙を使う（未定義なら東北大BERTにフォールバック）\n",
    "    try:\n",
    "        MODEL_NAME\n",
    "    except NameError:\n",
    "        MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --------------------------- 元のコード（＋説明コメント） ---------------------------\n",
    "\n",
    "# 入力テキスト（インデックスは 0 始まり）\n",
    "# 文字位置: 0:昨,1:日,2:の,3:み,4:ら,5:い,6:事,7:務,8:所,9:と,10:の,11:打,12:ち,13:合,14:わ,15:せ,16:は,17:順,18:調,19:だ,20:っ,21:た,22:。\n",
    "text = \"昨日のみらい事務所との打ち合わせは順調だった。\"\n",
    "\n",
    "# エンティティの指定：\n",
    "#  - name は人間可読の補助で、実際のラベル付与は span と type_id によって行われる\n",
    "#  - span=[3,9) → 原文中の「みらい事務所」（3〜8文字目）を指定（end=9 は「と」なので非包含）\n",
    "#  - type_id=1 → O（背景）を 0 とするクラス設計の「1番」カテゴリ\n",
    "entities = [{\"name\": \"みらい事務所\", \"span\": [3, 9], \"type_id\": 1}]\n",
    "\n",
    "# ラベル列作成つきの符号化（最大長は [CLS]/[SEP] を含むトークン長に正規化される）\n",
    "# labels は先頭[CLS]=0、本文（max_length-2 まで）、末尾[SEP]=0、残りPAD=0 で埋められる。\n",
    "encoding = tokenizer.encode_plus_tagged(text, entities, max_length=20)\n",
    "\n",
    "# 出力の中身は Hugging Face の標準キー（input_ids, attention_mask, token_type_ids）に加え、\n",
    "# 1:1 に対応する labels が入っている。確認用にそのまま表示。\n",
    "print(encoding)\n",
    "\n",
    "# 追加の確認（任意）：\n",
    "# - トークン列長とラベル列長が一致していること\n",
    "# - ラベル中の 1 の連続区間が「みらい事務所」の WordPiece 分割数に一致していること\n",
    "# 例：\n",
    "# tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "# print(tokens)\n",
    "# print(encoding[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85a51968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# encoding\n",
      "{'input_ids': tensor([[    2,     1,     5,  1543,   125,     9,  6749, 28550,  2953, 28550,\n",
      "         28566, 21202, 28683, 14050, 12475,    12,    31,     8,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "# spans\n",
      "[[-1, -1], [0, 2], [2, 3], [3, 5], [5, 6], [6, 7], [7, 9], [9, 10], [10, 12], [12, 13], [13, 14], [15, 18], [18, 19], [19, 23], [24, 27], [27, 28], [28, 30], [30, 31], [-1, -1]]\n"
     ]
    }
   ],
   "source": [
    "# 8-8\n",
    "text = \"騰訊の英語名はTencent Holdings Ltdである。\"\n",
    "encoding, spans = tokenizer.encode_plus_untagged(text, return_tensors=\"pt\")\n",
    "print(\"# encoding\")\n",
    "print(encoding)\n",
    "print(\"# spans\")\n",
    "print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55cb9eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': '騰訊の', 'span': [0, 3], 'type_id': 1}, {'name': 'ncent Holdings Ltdで', 'span': [9, 28], 'type_id': 1}]\n"
     ]
    }
   ],
   "source": [
    "# 8-9\n",
    "labels_predicted = [0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "entities = tokenizer.convert_bert_output_to_entities(text, labels_predicted, spans)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb0ff5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] device = mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 8-10（説明コメント付き：日本語BERTをトークン分類（NER）用に初期化）\n",
    "# =============================================================================\n",
    "# 目的：\n",
    "#  - 日本語BERTの語彙・前処理（MeCab + WordPiece）に対応した **NER_tokenizer** を読み込み、\n",
    "#    事前学習モデルの上に **BertForTokenClassification** ヘッド（num_labels=4）を載せる。\n",
    "#  - MacBook 環境（Apple Silicon/MPS）・CUDA・CPU のいずれでも動くように **デバイス自動選択**で配置。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - Token Classification は各トークン t に対してクラスのロジット z_{t,c} を出力し、\n",
    "#    学習時は通常 CrossEntropyLoss（各位置の多クラス）を用いる。\n",
    "#  - NER ではしばしば **BIO/IOBES** を採用するが、本書式は「O=0, エンティティ種別ID>0」\n",
    "#    のシンプル方式（連結方式）も可能。`num_labels=4` は例として\n",
    "#       0: \"O\", 1: \"ORG\", 2: \"PER\", 3: \"LOC\"\n",
    "#    のように解釈できる（実際の定義はデータに合わせて固定・共有すること）。\n",
    "#  - 学習時に `[CLS]/[SEP]/[PAD]` 位置のラベルを損失から除外するには、\n",
    "#    データ側のラベルを **ignore_index（例：-100）** にしておくのが Hugging Face の定石。\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "from transformers import BertForTokenClassification\n",
    "\n",
    "# --- （参考）元のコード（実行はしない。可搬性の観点で .cuda() 固定は避けるため） ---\n",
    "# tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "# bert_tc = BertForTokenClassification.from_pretrained(\n",
    "#     MODEL_NAME, num_labels=4\n",
    "# )\n",
    "# bert_tc = bert_tc.cuda()\n",
    "\n",
    "# --- MODEL_NAME が未定義なら東北大BERTにフォールバック ---\n",
    "try:\n",
    "    MODEL_NAME\n",
    "except NameError:\n",
    "    MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "\n",
    "# --- NER_tokenizer クラスの存在確認（8-5 で定義済み想定） ---\n",
    "try:\n",
    "    NER_tokenizer\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\n",
    "        \"NER_tokenizer クラスが未定義です。先に 8-5 の定義を実行してください。\"\n",
    "    ) from e\n",
    "\n",
    "# --- ラベル定義（例）。実データに合わせて **固定・共有** することが重要 ---\n",
    "num_labels = 4\n",
    "id2label = {0: \"O\", 1: \"ORG\", 2: \"PER\", 3: \"LOC\"}  # 例：組織/人/場所\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "\n",
    "# --- デバイス自動選択（Mac MPS → CUDA → CPU） ---\n",
    "def pick_device() -> torch.device:\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = pick_device()\n",
    "print(f\"[info] device = {device}\")\n",
    "\n",
    "# --- トークナイザ：日本語用の NER_tokenizer（MeCab + WordPiece 連携） ---\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- モデル本体：事前学習BERT + Token Classification ヘッド ---\n",
    "bert_tc = (\n",
    "    BertForTokenClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,  # 推論・可視化で役立つ（config に保存され ckpt へも残る）\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    .to(device)\n",
    "    .eval()\n",
    ")  # 推論時は eval()。学習時は train() に切替。\n",
    "\n",
    "# =============================================================================\n",
    "# 補足（実務向け）：\n",
    "#  - 学習：\n",
    "#      - ラベル列の特殊トークン位置は `-100`（ignore_index）にして DataCollator などで結合。\n",
    "#      - Optimizer は AdamW、Scheduler は linear/warmup を推奨。\n",
    "#  - 推論：\n",
    "#      - `outputs = bert_tc(**batch)` → `logits.argmax(-1)` でラベルID列。\n",
    "#      - `tokenizer` から `offset_mapping`（Fast版）を使える場合、原文スパン復元が堅牢。\n",
    "#  - 再現性：\n",
    "#      - `MODEL_NAME`、`id2label`、`max_length`、正規化の有無（NFKC）を **明示的に固定**・記録。\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29c545c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.3851\n"
     ]
    }
   ],
   "source": [
    "# 8-12（修正版：CUDA 未有効エラーの解消と実務的ガードを追加）\n",
    "# =============================================================================\n",
    "# エラー原因：\n",
    "#   AssertionError: Torch not compiled with CUDA enabled\n",
    "# → 環境が CUDA 非対応なのに `.cuda()` を呼んだため。\n",
    "#\n",
    "# 対処方針：\n",
    "#  1) モデルが載っている実デバイスに合わせて **.to(device)** に統一（MPS/CUDA/CPU どれでも可）。\n",
    "#  2) （任意だが推奨）特殊トークン([CLS]/[SEP]/[PAD])と PAD 部分のラベルを **-100** にして\n",
    "#     CrossEntropyLoss の ignore_index に合わせ、損失に寄与させない。\n",
    "#     ※ 8-5 実装はこれらを 0=O にしているため、何もしないと O クラス過多で学習が歪む。\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- 手作り NER データ（元コードそのまま） ---\n",
    "data = [\n",
    "    {\n",
    "        \"text\": \"AさんはB大学に入学した。\",\n",
    "        \"entities\": [\n",
    "            {\"name\": \"A\", \"span\": [0, 1], \"type_id\": 2},\n",
    "            {\"name\": \"B大学\", \"span\": [4, 7], \"type_id\": 1},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"CDE株式会社は新製品「E」を販売する。\",\n",
    "        \"entities\": [\n",
    "            {\"name\": \"CDE株式会社\", \"span\": [0, 7], \"type_id\": 1},\n",
    "            {\"name\": \"E\", \"span\": [12, 13], \"type_id\": 3},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# --- 符号化：8-5 の encode_plus_tagged を使用（元コードそのまま） ---\n",
    "max_length = 32\n",
    "dataset_for_loader = []\n",
    "for sample in data:\n",
    "    text = sample[\"text\"]\n",
    "    entities = sample[\"entities\"]\n",
    "    encoding = tokenizer.encode_plus_tagged(text, entities, max_length=max_length)\n",
    "    # HF の TokenClassification は labels を long テンソル（クラスID）で受けるのが定石\n",
    "    encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "    dataset_for_loader.append(encoding)\n",
    "\n",
    "# --- DataLoader（元コードの方針：全件を 1 ミニバッチに） ---\n",
    "dataloader = DataLoader(dataset_for_loader, batch_size=len(data))\n",
    "\n",
    "# --- デバイス：モデル側に合わせて自動決定（.cuda() を撤廃） ---\n",
    "device = next(bert_tc.parameters()).device  # 例：mps/cuda/cpu のいずれか\n",
    "\n",
    "\n",
    "# --- 推奨：特殊トークンと PAD を損失から除外（ignore_index=-100）に置換する関数 ---\n",
    "def mask_special_positions_to_ignore_index(batch_dict, tokenizer, ignore_index=-100):\n",
    "    \"\"\"\n",
    "    TokenClassification の損失から除外したい位置（[CLS]/[SEP]/[PAD] と attention_mask=0）を\n",
    "    ラベル上で ignore_index に置き換える。\n",
    "    \"\"\"\n",
    "    input_ids = batch_dict[\"input_ids\"]\n",
    "    attention_mask = batch_dict[\"attention_mask\"]\n",
    "    labels = batch_dict[\"labels\"]\n",
    "\n",
    "    # PAD 位置（attention_mask==0）を -100 に\n",
    "    labels = labels.masked_fill(attention_mask.eq(0), ignore_index)\n",
    "\n",
    "    # CLS/SEP 位置を -100 に（存在チェック込み）\n",
    "    cls_id = tokenizer.cls_token_id\n",
    "    sep_id = tokenizer.sep_token_id\n",
    "    for i in range(labels.size(0)):\n",
    "        cls_pos = (input_ids[i] == cls_id).nonzero(as_tuple=True)[0]\n",
    "        sep_pos = (input_ids[i] == sep_id).nonzero(as_tuple=True)[0]\n",
    "        if cls_pos.numel() > 0:\n",
    "            labels[i, cls_pos] = ignore_index\n",
    "        if sep_pos.numel() > 0:\n",
    "            labels[i, sep_pos] = ignore_index\n",
    "\n",
    "    batch_dict[\"labels\"] = labels\n",
    "    return batch_dict\n",
    "\n",
    "\n",
    "# --- ミニバッチで損失を計算（.to(device) で可搬化） ---\n",
    "for batch in dataloader:\n",
    "    # 元コードの `.cuda()` を **.to(device)** に差し替え（MPS/CPU でも動く）\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    # （任意・推奨）損失に含めない位置を ignore_index=-100 に置換\n",
    "    batch = mask_special_positions_to_ignore_index(batch, tokenizer, ignore_index=-100)\n",
    "\n",
    "    # 推論または学習（ここでは損失取得のみ）\n",
    "    output = bert_tc(**batch)  # labels が含まれているため内部で CrossEntropyLoss を計算\n",
    "    loss = output.loss  # 平均化されたトークン単位 CE 損失\n",
    "    print(f\"loss = {loss.item():.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 理論補足：\n",
    "#  - CrossEntropyLoss はクラス ID（long）を取り、ignore_index（既定 -100）位置は損失計算から除外。\n",
    "#  - ignore_index を適用すると、PAD や特殊トークンの頻度に損失が引っぱられず、学習が安定。\n",
    "#  - 連結方式（O=0, type_id>0）でも動作するが、隣接同タイプを分離したいなら BIO/IOBES へ拡張。\n",
    "#  - 実学習では bert_tc.train()、評価や予測では bert_tc.eval() + no_grad() を併用する。\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4122016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ner-wikipedia-dataset' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch v2.0 https://github.com/stockmarkteam/ner-wikipedia-dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0e3a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-14\n",
    "# データのロード\n",
    "dataset = json.load(open(\"ner-wikipedia-dataset/ner.json\", \"r\"))\n",
    "# ↑ 事前作成済みの NER 用 JSON を読み込む。\n",
    "#    想定フォーマット：\n",
    "#      sample = {\n",
    "#        \"text\": <文字列>,\n",
    "#        \"entities\": [\n",
    "#           {\"name\": <表層>, \"span\": [start, end], \"type\": <カテゴリ名>}, ...\n",
    "#        ]\n",
    "#      }\n",
    "#    ※ 文字インデックス span は 0 始まりの半開区間 [start, end) を想定。\n",
    "#    ※ エンコード（UTF-8 など）に依存する場合は open(..., encoding='utf-8') を明示すると安全。\n",
    "\n",
    "# 固有表現のタイプとIDを対応付る辞書\n",
    "type_id_dict = {\n",
    "    \"人名\": 1,\n",
    "    \"法人名\": 2,\n",
    "    \"政治的組織名\": 3,\n",
    "    \"その他の組織名\": 4,\n",
    "    \"地名\": 5,\n",
    "    \"施設名\": 6,\n",
    "    \"製品名\": 7,\n",
    "    \"イベント名\": 8,\n",
    "}\n",
    "# ↑ 学習・推論で一貫して使う「カテゴリ名 → 整数ID」対応。\n",
    "#    ・TokenClassification/BIO 系では num_labels ≥ 最大ID+1 が必要。\n",
    "#    ・この対応はメタデータとして保存（JSON 等）し、学習・推論で共有すること。\n",
    "\n",
    "# カテゴリーをラベルに変更、文字列の正規化する。\n",
    "for sample in dataset:\n",
    "    sample[\"text\"] = unicodedata.normalize(\"NFKC\", sample[\"text\"])\n",
    "    # ↑ NFKC（互換分解＋合成）で全角/半角・互換文字の表記ゆれを統一。\n",
    "    #    【重要】NFKC は “非可逆” かつ “文字長が変化” することがあるため、\n",
    "    #    もし `entities[*]['span']` が「正規化前の text に基づく」場合は\n",
    "    #    span がズレる＝無効になる点に注意。\n",
    "    #    → 本コードは span を再計算していないため、\n",
    "    #       * データが “すでに正規化済み” である\n",
    "    #       * または 正規化後の text を基準に span が定義されている\n",
    "    #      ことが前提。そうでない場合は offset を再構築する前処理が必要（TODO）。\n",
    "\n",
    "    for e in sample[\"entities\"]:\n",
    "        e[\"type_id\"] = type_id_dict[e[\"type\"]]\n",
    "        del e[\"type\"]\n",
    "        # ↑ 学習で扱うのは数値ラベル（type_id）。人可読名（type）は破棄。\n",
    "        #    推論結果の可視化用に name/type 名称が必要なら、別の辞書を持つか\n",
    "        #    id2type で復号できるようにしておくこと。\n",
    "\n",
    "# データセットの分割\n",
    "random.shuffle(dataset)\n",
    "# ↑ シャッフルしてから split。再現性を担保したい場合は予め random.seed(固定値) を設定する。\n",
    "n = len(dataset)\n",
    "n_train = int(n * 0.6)\n",
    "n_val = int(n * 0.2)\n",
    "dataset_train = dataset[:n_train]\n",
    "dataset_val = dataset[n_train : n_train + n_val]\n",
    "dataset_test = dataset[n_train + n_val :]\n",
    "# ↑ 単純比率 split（60/20/20）。\n",
    "#    ・ラベル分布が偏る可能性があるため、実務では層化分割（stratified split）を推奨。\n",
    "#    ・文長分布やエンティティ有無の偏りも評価指標に影響するため、分割前に統計確認を行うと良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ad7eafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 8-15（説明コメント付き：NER データのエンコード→DataLoader 準備）\n",
    "# =============================================================================\n",
    "# 目的：\n",
    "#  - 文字スパン付き NER データ（sample = {'text': str, 'entities': [{'span':[s,e], 'type_id':k}, ...] }）を\n",
    "#    学習時に直接使える形（BERT 入力辞書＋トークンラベル）へ変換し、DataLoader を構築する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - encode_plus_tagged（8-5で定義）は、原文をエンティティ境界で分割 → WordPiece 化 →\n",
    "#    片ラベル（O=0 / type_id>0）を各トークンへ複写 → [CLS]/[SEP]/PAD を付与して長さ max_length に整形する。\n",
    "#  - Token Classification の学習では **CrossEntropyLoss** を用いるため、labels は **クラスID（LongTensor）** を渡すのが定石。\n",
    "#  - [CLS]/[SEP]/PAD 等の位置は損失から除外するのが一般的（ignore_index=-100）。この関数では 0=O のまま作るので、\n",
    "#    学習ループ側でマスクに置き換える（または DataCollator で処理）と良い。\n",
    "#  - max_length を越える部分は切り捨てられるため、右端でエンティティが途切れる場合がある。実務では\n",
    "#    スライディングウィンドウや長めの max_length を検討する。\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def create_dataset(tokenizer, dataset, max_length):\n",
    "    \"\"\"\n",
    "    データセットをデータローダに入力できる形に整形。\n",
    "\n",
    "    引数:\n",
    "      tokenizer : NER_tokenizer\n",
    "        8-5 で定義した encode_plus_tagged を持つトークナイザ（語彙は学習・推論で固定）。\n",
    "      dataset   : List[Dict]\n",
    "        [{'text': str, 'entities': [{'span':[s,e], 'type_id':k}, ...]}, ...]\n",
    "      max_length: int\n",
    "        BERT 入力系列の上限（[CLS]/[SEP] を含む長さに正規化）。\n",
    "\n",
    "    戻り値:\n",
    "      dataset_for_loader : List[Dict[str, torch.Tensor]]\n",
    "        input_ids/attention_mask/token_type_ids/labels を LongTensor で持つ辞書のリスト。\n",
    "        DataLoader で自動的に key ごとにスタックされミニバッチになる。\n",
    "    \"\"\"\n",
    "    dataset_for_loader = []\n",
    "    for sample in dataset:\n",
    "        text = sample[\"text\"]\n",
    "        entities = sample[\"entities\"]\n",
    "\n",
    "        # 文字スパン→トークンラベル写像（O=0 / type_id>0）。[CLS]/[SEP]/PAD も付与され長さを揃える。\n",
    "        encoding = tokenizer.encode_plus_tagged(text, entities, max_length=max_length)\n",
    "\n",
    "        # HF の TokenClassification は labels を「整数クラスID（Long）」で受けるのが定石。\n",
    "        # attention_mask/input_ids/token_type_ids も Long で問題ない（CE計算は labels の型が重要）。\n",
    "        # ※ 特殊トークン/PAD を損失から除外したい場合は、学習ループ側で -100 に置換すること。\n",
    "        encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "\n",
    "        dataset_for_loader.append(encoding)\n",
    "\n",
    "    return dataset_for_loader\n",
    "\n",
    "\n",
    "# トークナイザのロード\n",
    "# - 事前学習モデルの語彙・前処理（MeCab + WordPiece）と一致させること。\n",
    "# - MODEL_NAME は学習・推論を通じて固定し、再現性のためにメタとして保存しておく。\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# データセットの作成\n",
    "# - 学習安定の観点で 128 などの固定長を採用（動的パディングよりスループットが安定しやすい）。\n",
    "# - 文が長く切れやすいコーパスでは 256/384/512 も検討（VRAM とトレードオフ）。\n",
    "max_length = 128\n",
    "dataset_train_for_loader = create_dataset(tokenizer, dataset_train, max_length)\n",
    "dataset_val_for_loader = create_dataset(tokenizer, dataset_val, max_length)\n",
    "\n",
    "# データローダの作成\n",
    "# - 学習は shuffle=True（i.i.d. 近似と汎化のため）。検証/テストは順序維持で十分。\n",
    "# - 実務では num_workers / pin_memory（CUDA時）や DataCollatorForTokenClassification の導入を検討。\n",
    "# - デバイスへの転送は学習ループ側で `batch = {k: v.to(device) for k,v in batch.items()}` とする。\n",
    "dataloader_train = DataLoader(dataset_train_for_loader, batch_size=32, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-16（説明コメント付き：Lightning v2 対応・可搬デバイス・損失マスク（CLS/SEP/PAD 除外））\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from transformers import BertForTokenClassification, AutoTokenizer\n",
    "\n",
    "# =============================================================================\n",
    "# 目的\n",
    "#  - TokenClassification 用の LightningModule を作成し、学習/検証の損失をロギングする。\n",
    "#  - Lightning v2 で非推奨の `gpus=1` を廃止し、`accelerator`/`devices` を用いた可搬設定に修正。\n",
    "#  - 学習の健全性向上のため、損失計算から [CLS]/[SEP]/PAD を除外（ignore_index 相当のマスク）する。\n",
    "#\n",
    "# 背景理論（要点）\n",
    "#  - トークン分類は位置 t ごとに C クラスのロジット z_{t,c} を出力し、CrossEntropyLoss を計算する。\n",
    "#  - 一般に PAD や特殊トークン（CLS/SEP）は学習に寄与させない（HF の定石は labels を -100 にする）。\n",
    "#  - 本実装では、バッチ受領後に labels を複製し、attention_mask==0 と CLS/SEP 位置を -100 に置換してから\n",
    "#    BertForTokenClassification に渡す。（= ignore_index 処理）\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class BertForTokenClassification_pl(pl.LightningModule):\n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        \"\"\"\n",
    "        model_name: 事前学習BERT（例：tohoku-nlp/bert-base-japanese-whole-word-masking）\n",
    "        num_labels: クラス数（O=0 を含む総数）\n",
    "        lr       : 学習率（微調整の目安 1e-5〜5e-5）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # 本体モデル（トークン分類ヘッド付き）\n",
    "        self.bert_tc = BertForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "        )\n",
    "\n",
    "        # CLS/SEP のトークンIDを取得（損失から除外するために使用）\n",
    "        tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.cls_id = tok.cls_token_id\n",
    "        self.sep_id = tok.sep_token_id\n",
    "\n",
    "    # -------------------- 内部ユーティリティ：損失無視マスクを埋める --------------------\n",
    "    def _apply_ignore_index_mask(self, batch, ignore_index=-100):\n",
    "        \"\"\"\n",
    "        CrossEntropyLoss の対象外にしたい位置（PAD/CLS/SEP）を labels 上で ignore_index に置換する。\n",
    "        - PAD は attention_mask==0 で同定。\n",
    "        - CLS/SEP は input_ids の値から同定。\n",
    "        \"\"\"\n",
    "        if \"labels\" not in batch:\n",
    "            return batch  # 推論時など labels が無い場合はそのまま返す\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"].clone()  # 破壊的変更を避けるため複製\n",
    "\n",
    "        # PAD 位置（attention_mask==0）を ignore_index に\n",
    "        labels = labels.masked_fill(attention_mask.eq(0), ignore_index)\n",
    "\n",
    "        # CLS/SEP の位置も ignore_index に\n",
    "        # 例外的に CLS/SEP が存在しないトークナイザ構成は想定外。存在チェックしつつ置換。\n",
    "        if self.cls_id is not None:\n",
    "            cls_pos = input_ids.eq(self.cls_id)\n",
    "            labels = labels.masked_fill(cls_pos, ignore_index)\n",
    "        if self.sep_id is not None:\n",
    "            sep_pos = input_ids.eq(self.sep_id)\n",
    "            labels = labels.masked_fill(sep_pos, ignore_index)\n",
    "\n",
    "        batch = dict(batch)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    # -------------------- 学習/検証フロー --------------------\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # 損失から除外すべき位置を -100 に置換してから forward\n",
    "        batch = self._apply_ignore_index_mask(batch, ignore_index=-100)\n",
    "        output = self.bert_tc(**batch)\n",
    "        loss = output.loss\n",
    "        # エポック平均で可視化（prog_bar=True で進捗バーにも表示）\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch = self._apply_ignore_index_mask(batch, ignore_index=-100)\n",
    "        output = self.bert_tc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # 最小構成：Adam（実務は AdamW + Scheduler を推奨）\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "\n",
    "# -------------------- チェックポイント（val_loss 最小） --------------------\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,  # 版差あり：必要に応じて state_dict 手動保存を検討\n",
    "    dirpath=\"model/\",\n",
    "    filename=\"epoch={epoch}-val_loss={val_loss:.4f}\",\n",
    ")\n",
    "\n",
    "# -------------------- Trainer（Lightning v2 形式に修正：gpus→accelerator/devices） --------------------\n",
    "# 可搬設定：CUDA があれば GPU、Apple Silicon なら MPS、無ければ CPU を自動選択\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=5,\n",
    "    callbacks=[checkpoint],\n",
    "    # deterministic=True,              # 再現性重視（性能とトレードオフ）\n",
    "    # gradient_clip_val=1.0,           # 勾配クリップ（安定化）\n",
    "    # accumulate_grad_batches=2,       # 勾配蓄積（VRAM 節約）\n",
    "    # precision=16,                    # 混合精度（環境に応じて）\n",
    ")\n",
    "\n",
    "# -------------------- ファインチューニング実行 --------------------\n",
    "model = BertForTokenClassification_pl(MODEL_NAME, num_labels=9, lr=1e-5)\n",
    "\n",
    "# DataLoader は 8-15 で作成済み（dataloader_train / dataloader_val）\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "\n",
    "# 最良モデル（val_loss 最小）のファイルパス\n",
    "best_model_path = checkpoint.best_model_path\n",
    "print(f\"[best ckpt] {best_model_path}\")\n",
    "\n",
    "# 参考：ロード例\n",
    "# loaded = BertForTokenClassification_pl.load_from_checkpoint(best_model_path)\n",
    "# loaded.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "668ca10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1070/1070 [00:30<00:00, 35.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# 8-17（説明コメント付き：推論関数の可搬化・理論補足つき最小実装）\n",
    "\n",
    "import torch\n",
    "\n",
    "# from tqdm import tqdm  # 進捗バーが未インポートなら有効化してください\n",
    "\n",
    "\n",
    "def predict(text, tokenizer, bert_tc):\n",
    "    \"\"\"\n",
    "    BERTで固有表現抽出を行うための関数。\n",
    "    推論専用（dropout 無効化前提：model.eval() を外側で呼ぶ）。\n",
    "\n",
    "    理論メモ：\n",
    "      - トークン分類の出力 logits 形状は [B, T, C]（バッチ×系列長×クラス数）。\n",
    "        B=1 なら scores[0] は [T, C] で、各位置 t の予測ラベルは argmax_c logits[t, c]。\n",
    "      - 本パイプラインは BIO ではなく、O=0／type_id>0 の“連結方式”を想定。\n",
    "        連続する同一ラベル区間を 1 エンティティとして結合して原文スパンへ復元する。\n",
    "    \"\"\"\n",
    "    # モデルの実デバイスを取得（CUDA/MPS/CPU いずれでも可）\n",
    "    device = next(bert_tc.parameters()).device\n",
    "\n",
    "    # 符号化（BERT入力辞書）と、各トークンの原文スパン（[start, end)）を取得\n",
    "    encoding, spans = tokenizer.encode_plus_untagged(text, return_tensors=\"pt\")\n",
    "    # 可搬化：.cuda() 固定ではなく .to(device) に統一\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "    # 推論：dropout を止めるため eval() を事前に呼んでおくこと（本関数外で実施推奨）\n",
    "    with torch.no_grad():\n",
    "        output = bert_tc(**encoding)  # logits: [1, T, C]\n",
    "        scores = output.logits\n",
    "        # 位置 t ごとに argmax を取り最尤クラスID列 [T] を得る\n",
    "        labels_predicted = scores[0].argmax(-1).detach().cpu().tolist()\n",
    "\n",
    "    # ラベル列と spans から固有表現（原文スパン）へ復元\n",
    "    entities = tokenizer.convert_bert_output_to_entities(text, labels_predicted, spans)\n",
    "    return entities\n",
    "\n",
    "\n",
    "# トークナイザのロード（学習時と同一の語彙・前処理に合わせることが重要）\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ファインチューニング済み LightningModule から生の HuggingFace モデルを取り出して配置\n",
    "model = BertForTokenClassification_pl.load_from_checkpoint(best_model_path)\n",
    "\n",
    "\n",
    "# 可搬デバイス選択：MPS → CUDA → CPU の順で利用（Lightning 外の素の推論用）\n",
    "def pick_device():\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = pick_device()\n",
    "bert_tc = model.bert_tc.to(device).eval()  # 推論安定化のため eval() に\n",
    "\n",
    "# 固有表現抽出\n",
    "# 注：以下ではコードのわかりやすさのために 1 データずつ処理しているが、\n",
    "#     実務はバッチ化（複数文をまとめてエンコード→パディング→一括 forward）の方が高速。\n",
    "entities_list = []  # 正解の固有表現（評価用に保持）\n",
    "entities_predicted_list = []  # 予測された固有表現\n",
    "\n",
    "for sample in tqdm(dataset_test):\n",
    "    text = sample[\"text\"]\n",
    "    # BERTで予測（O=0／type_id>0 の連結方式でスパン復元）\n",
    "    entities_predicted = predict(text, tokenizer, bert_tc)\n",
    "    entities_list.append(sample[\"entities\"])\n",
    "    entities_predicted_list.append(entities_predicted)\n",
    "\n",
    "# （任意の確認例）\n",
    "# print(entities_predicted_list[:3])\n",
    "\n",
    "# 補足：\n",
    "# - 学習時に NFKC 正規化をしているなら、推論時も同一の前処理を適用して分布ギャップを避ける。\n",
    "# - BIO/IOBES を採用した評価（seqeval 等）に切り替える場合は、ラベル空間と復元ロジックを合わせて拡張する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ea37be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 正解\n",
      "[{'name': '双竜セメント工業', 'span': [6, 14], 'type_id': 2}, {'name': 'イラン国営石油', 'span': [15, 22], 'type_id': 2}]\n",
      "# 抽出\n",
      "[{'name': '双竜セメント工業', 'span': [6, 14], 'type_id': 2}, {'name': 'イラン国営石油', 'span': [15, 22], 'type_id': 2}]\n"
     ]
    }
   ],
   "source": [
    "# 8-18\n",
    "print(\"# 正解\")\n",
    "print(entities_list[0])\n",
    "print(\"# 抽出\")\n",
    "print(entities_predicted_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f236b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-19（説明コメント付き：固有表現抽出の評価関数）\n",
    "def evaluate_model(entities_list, entities_predicted_list, type_id=None):\n",
    "    \"\"\"\n",
    "    正解（gold）と予測（pred）を比較し、固有表現抽出（NER）の性能を評価する。\n",
    "\n",
    "    評価の前提・理論:\n",
    "      - エンティティの一致判定は **スパン完全一致 + タイプ一致**（start, end, type_id がすべて同一）。\n",
    "        ここではスパンは [start, end) の半開区間を想定（end 文字は含まない）。\n",
    "        ※ 部分一致や重なりのみでは正解とみなさない「厳密一致」基準。\n",
    "      - 本関数は文ごとに集合を作り、**全文書での合計**に対して指標を計算するため、\n",
    "        micro 集計（micro-precision / micro-recall / micro-F1）に相当する。\n",
    "      - 評価指標:\n",
    "          * 適合率（Precision） = 正解かつ予測であった件数 / 予測件数\n",
    "          * 再現率（Recall）    = 正解かつ予測であった件数 / 正解件数\n",
    "          * F値（F1）           = 2 * P * R / (P + R)\n",
    "        ※ ゼロ割の可能性（予測や正解が 0 件）に留意。必要なら分母が 0 のとき 0.0 を返す等の保護を追加すると良い。\n",
    "      - サブセット評価:\n",
    "          * `type_id` を与えると、そのタイプ（ラベル）に限定した P/R/F1 を返す（ラベル別性能）。\n",
    "            型は int を想定。0 は「O（非エンティティ）」であることが多いので通常は 1 以上。\n",
    "    \"\"\"\n",
    "\n",
    "    num_entities = 0  # gold の総エンティティ数（分母: 再現率）\n",
    "    num_predictions = 0  # pred の総エンティティ数（分母: 適合率）\n",
    "    num_correct = 0  # gold ∩ pred（完全一致）の件数\n",
    "\n",
    "    # それぞれの文章で予測と正解を比較。\n",
    "    # 判定基準: 文章中の (start, end, type_id) が完全一致すれば正解とみなす。\n",
    "    for entities, entities_predicted in zip(entities_list, entities_predicted_list):\n",
    "\n",
    "        # --- タイプ別の評価を行う場合のフィルタ ---\n",
    "        # 注意: ここでの if 判定は「0 を偽」とみなすので、type_id=0 を評価したいケースに対応しない。\n",
    "        # 通常 0 は O（非エンティティ）で評価対象外のため問題になりにくいが、より厳密には\n",
    "        #   if type_id is not None:\n",
    "        # にするのが堅牢。\n",
    "        if type_id:\n",
    "            entities = [e for e in entities if e[\"type_id\"] == type_id]\n",
    "            entities_predicted = [\n",
    "                e for e in entities_predicted if e[\"type_id\"] == type_id\n",
    "            ]\n",
    "            # 改善案（コメントアウト）:\n",
    "            # if type_id is not None:\n",
    "            #     entities = [e for e in entities if e['type_id'] == type_id]\n",
    "            #     entities_predicted = [e for e in entities_predicted if e['type_id'] == type_id]\n",
    "\n",
    "        # スパン＋タイプのタプルに写像して集合化（完全一致のみが共通要素になる）\n",
    "        get_span_type = lambda e: (e[\"span\"][0], e[\"span\"][1], e[\"type_id\"])\n",
    "        set_entities = set(get_span_type(e) for e in entities)\n",
    "        set_entities_predicted = set(get_span_type(e) for e in entities_predicted)\n",
    "\n",
    "        # 文ごとの件数を集計（micro 集計のため全文で総和）\n",
    "        num_entities += len(entities)\n",
    "        num_predictions += len(entities_predicted)\n",
    "        num_correct += len(\n",
    "            set_entities & set_entities_predicted\n",
    "        )  # 積集合 = 正解かつ予測\n",
    "\n",
    "    # 指標を計算\n",
    "    # 注意: 分母が 0 の可能性がある（予測 0 件 or 正解 0 件）。\n",
    "    # 実運用ではゼロ割を回避する保護（例: 分母が 0 の場合は 0.0 を返す）を入れると安全。\n",
    "    precision = num_correct / num_predictions  # 適合率\n",
    "    recall = num_correct / num_entities  # 再現率\n",
    "    f_value = 2 * precision * recall / (precision + recall)  # F値（調和平均）\n",
    "\n",
    "    result = {\n",
    "        \"num_entities\": num_entities,  # gold 総数\n",
    "        \"num_predictions\": num_predictions,  # pred 総数\n",
    "        \"num_correct\": num_correct,  # gold ∩ pred\n",
    "        \"precision\": precision,  # 適合率\n",
    "        \"recall\": recall,  # 再現率\n",
    "        \"f_value\": f_value,  # F1\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a856d238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_entities': 2592, 'num_predictions': 2665, 'num_correct': 2209, 'precision': 0.8288930581613508, 'recall': 0.8522376543209876, 'f_value': 0.8404032718280388}\n"
     ]
    }
   ],
   "source": [
    "# 8-20\n",
    "print(evaluate_model(entities_list, entities_predicted_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a03a1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-21\n",
    "class NER_tokenizer_BIO(BertJapaneseTokenizer):\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 概要：\n",
    "    #  - BertJapaneseTokenizer を拡張し、**BIO スキーム**（B: Begin, I: Inside, O: Outside）で\n",
    "    #    固有表現のラベル列を構築・復元するためのユーティリティを提供する。\n",
    "    #  - ラベルのエンコード規則（整数ID）：\n",
    "    #      O = 0\n",
    "    #      B-<k> = k                     （k = 1..num_entity_type）\n",
    "    #      I-<k> = k + num_entity_type   （k = 1..num_entity_type）\n",
    "    #    したがって総ラベル数 m = 2*num_entity_type + 1 になる。\n",
    "    #  - `encode_plus_tagged`：原文＋スパン付きエンティティから BIO ラベル列を構築し、BERT 入力辞書を返す。\n",
    "    #  - `encode_plus_untagged`：原文をトークン化し、トークンと原文の対応スパンを返す（推論時用）。\n",
    "    #  - `Viterbi`：BIO の遷移制約を**ペナルティ行列**で表現し、系列全体の最尤ラベル列を動的計画法で求める。\n",
    "    #  - `convert_bert_output_to_entities`：スコア（各トークン×ラベル）から Viterbi 復号→エンティティに変換。\n",
    "    # 注意：\n",
    "    #  - 本実装は **「同タイプの連続 I」** と **「B→同タイプ I」** を許し、それ以外の I 遷移を\n",
    "    #    大きなペナルティで抑制する設計（厳密な禁止ではない）。ペナルティ値はスコアスケールより十分大きい必要がある。\n",
    "    #  - エンティティ配列 `entities` は **開始位置順に与えられる**前提（未ソートの場合は事前に sort 推奨）。\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # 初期化時に固有表現のカテゴリーの数`num_entity_type`を\n",
    "    # 受け入れるようにする。\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.num_entity_type = kwargs.pop(\"num_entity_type\")\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def encode_plus_tagged(self, text, entities, max_length):\n",
    "        \"\"\"\n",
    "        文章とそれに含まれる固有表現が与えられた時に、\n",
    "        符号化とラベル列の作成を行う。\n",
    "        \"\"\"\n",
    "        # 固有表現の前後で text を分割し、それぞれの片に**基底ラベル**を持たせる。\n",
    "        # 前提：entities は span 昇順で与える（未ソートならソートしてから渡す）。\n",
    "        splitted = []  # 分割後の文字列を追加していく\n",
    "        position = 0\n",
    "        for entity in entities:\n",
    "            start = entity[\"span\"][0]\n",
    "            end = entity[\"span\"][1]\n",
    "            label = entity[\"type_id\"]  # エンティティ種別ID（1..N）\n",
    "            splitted.append(\n",
    "                {\"text\": text[position:start], \"label\": 0}\n",
    "            )  # 非エンティティ区間 → O\n",
    "            splitted.append(\n",
    "                {\"text\": text[start:end], \"label\": label}\n",
    "            )  # エンティティ区間 → 後で B/I 展開\n",
    "            position = end\n",
    "        splitted.append({\"text\": text[position:], \"label\": 0})\n",
    "        splitted = [s for s in splitted if s[\"text\"]]  # 長さ0の片は除去（空文字対策）\n",
    "\n",
    "        # 各片を WordPiece でトークン化し、BIO 規則でラベルを展開する。\n",
    "        tokens = []  # トークンを追加していく\n",
    "        labels = []  # ラベルを追加していく\n",
    "        for s in splitted:\n",
    "            tokens_splitted = self.tokenize(s[\"text\"])\n",
    "            label = s[\"label\"]\n",
    "            if label > 0:  # 固有表現片（type_id = 1..N）\n",
    "                # まず全トークンに I-k を付与し、先頭のみ B-k に書き換える\n",
    "                labels_splitted = [label + self.num_entity_type] * len(\n",
    "                    tokens_splitted\n",
    "                )  # I-k\n",
    "                labels_splitted[0] = label  # B-k\n",
    "            else:  # 非エンティティ片\n",
    "                labels_splitted = [0] * len(tokens_splitted)  # O\n",
    "\n",
    "            tokens.extend(tokens_splitted)\n",
    "            labels.extend(labels_splitted)\n",
    "\n",
    "        # トークン列をID化し、[CLS]/[SEP] 付与＋パディングで固定長に揃える。\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids, max_length=max_length, padding=\"max_length\", truncation=True\n",
    "        )\n",
    "        # ラベル列にも特殊トークン分を追加（[CLS]=O, [SEP]=O, PAD=O）\n",
    "        # ※ 学習時に PAD/CLS/SEP を損失から除外するなら、後段で ignore_index=-100 に置換する。\n",
    "        labels = [0] + labels[: max_length - 2] + [0]\n",
    "        labels = labels + [0] * (max_length - len(labels))\n",
    "        encoding[\"labels\"] = labels\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def encode_plus_untagged(self, text, max_length=None, return_tensors=None):\n",
    "        \"\"\"\n",
    "        文章をトークナイザで分割し、各トークンに対応する**原文側スパン**を記録する（推論時に使用）。\n",
    "        IO法のトークナイザのencode_plus_untaggedと同じ\n",
    "        \"\"\"\n",
    "        # 1) まず形態素（MeCab）で単語分割 → 2) WordPiece でサブワード分割。\n",
    "        #    tokens_original は WordPiece の '##' を外して原文側の文字列断片にそろえる。\n",
    "        tokens = []  # トークンを追加していく。\n",
    "        tokens_original = []  # トークンに対応する文章中の文字列を追加していく。\n",
    "        words = self.word_tokenizer.tokenize(text)  # MeCabで単語に分割\n",
    "        for word in words:\n",
    "            # 単語をサブワードに分割\n",
    "            tokens_word = self.subword_tokenizer.tokenize(word)\n",
    "            tokens.extend(tokens_word)\n",
    "            if (\n",
    "                tokens_word[0] == \"[UNK]\"\n",
    "            ):  # 未知語への対応（WordPiece化できないときは単語全体で対応）\n",
    "                tokens_original.append(word)\n",
    "            else:\n",
    "                tokens_original.extend(\n",
    "                    [token.replace(\"##\", \"\") for token in tokens_word]\n",
    "                )\n",
    "\n",
    "        # 原文テキスト上で tokens_original を**左から貪欲**にマッチさせ、スパンを復元する。\n",
    "        # 注意：同一部分文字列が繰り返し出現する場合は貪欲一致が誤る可能性がある（実運用では空白・正規化を統一）。\n",
    "        position = 0\n",
    "        spans = []  # トークンの位置を追加していく。\n",
    "        for token in tokens_original:\n",
    "            l = len(token)\n",
    "            while 1:\n",
    "                if token != text[position : position + l]:\n",
    "                    position += 1\n",
    "                else:\n",
    "                    spans.append([position, position + l])  # 半開区間 [start, end)\n",
    "                    position += l\n",
    "                    break\n",
    "\n",
    "        # BERT 入力辞書へ整形（[CLS]/[SEP] 付与、必要に応じてパディング/切り詰め）。\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\" if max_length else False,\n",
    "            truncation=True if max_length else False,\n",
    "        )\n",
    "        sequence_length = len(encoding[\"input_ids\"])\n",
    "        # 特殊トークンに対応するダミー span を追加（[-1, -1] は「損失や復元の対象外」を意味する）。\n",
    "        spans = [[-1, -1]] + spans[: sequence_length - 2]\n",
    "        spans = spans + [[-1, -1]] * (sequence_length - len(spans))\n",
    "\n",
    "        # Tensor での返却を求められた場合のラップ（バッチ次元=1を持たせる）\n",
    "        if return_tensors == \"pt\":\n",
    "            encoding = {k: torch.tensor([v]) for k, v in encoding.items()}\n",
    "\n",
    "        return encoding, spans\n",
    "\n",
    "    @staticmethod\n",
    "    def Viterbi(scores_bert, num_entity_type, penalty=10000):\n",
    "        \"\"\"\n",
    "        Viterbiアルゴリズムで最適解を求める。\n",
    "        入力:\n",
    "          scores_bert : 形状 [T, m] の 2次元配列（T=系列長, m=2*num_entity_type+1）\n",
    "                        各トークン t における各ラベルのスコア（通常は logits or log-prob）\n",
    "          num_entity_type : エンティティタイプ数（N）\n",
    "          penalty : BIO 違反遷移に課す大きな負値（スコアから減算）\n",
    "                    ※ スコアスケールより十分大きい値にする（「実質禁止」に近づける）\n",
    "        出力:\n",
    "          labels_optimal : 長さ T の最尤ラベル列（各要素は 0..m-1）\n",
    "        理念：\n",
    "          - BIO の**遷移制約**をペナルティ行列で表現し、動的計画法で時刻 t の到達スコアを更新。\n",
    "          - 計算量は O(T * m^2)。\n",
    "        \"\"\"\n",
    "        m = 2 * num_entity_type + 1  # O + B(1..N) + I(1..N)\n",
    "        penalty_matrix = np.zeros([m, m])\n",
    "        # I-<k>（列 j が I 域）のとき、前状態 i からの遷移は\n",
    "        #   ・同一 I-<k> 継続（i==j）\n",
    "        #   ・B-<k> から I-<k>（i+N==j）\n",
    "        # 以外は大きなペナルティを与えて抑制する（BIO 違反を罰する）。\n",
    "        for i in range(m):\n",
    "            for j in range(1 + num_entity_type, m):  # j: I-領域\n",
    "                if not ((i == j) or (i + num_entity_type == j)):\n",
    "                    penalty_matrix[i, j] = penalty\n",
    "\n",
    "        # 初期化：t=0 の遷移コストとして、仮想前状態を O とみなし O→j のペナルティを適用\n",
    "        path = [[i] for i in range(m)]\n",
    "        scores_path = scores_bert[0] - penalty_matrix[0, :]\n",
    "        scores_bert = scores_bert[1:]\n",
    "\n",
    "        # 前時刻までの最良パスに現在トークンのスコアを加え、遷移ペナルティを引いて DP 更新\n",
    "        for scores in scores_bert:\n",
    "            assert len(scores) == 2 * num_entity_type + 1\n",
    "            score_matrix = (\n",
    "                np.array(scores_path).reshape(-1, 1)\n",
    "                + np.array(scores).reshape(1, -1)\n",
    "                - penalty_matrix\n",
    "            )\n",
    "            scores_path = score_matrix.max(axis=0)  # 各現状態への最良スコア\n",
    "            argmax = score_matrix.argmax(axis=0)  # その最良スコアの直前状態\n",
    "            path_new = []\n",
    "            for i, idx in enumerate(argmax):\n",
    "                path_new.append(path[idx] + [i])  # 経路を連結\n",
    "            path = path_new\n",
    "\n",
    "        labels_optimal = path[np.argmax(scores_path)]\n",
    "        return labels_optimal\n",
    "\n",
    "    def convert_bert_output_to_entities(self, text, scores, spans):\n",
    "        \"\"\"\n",
    "        文章、分類スコア、各トークンの位置から固有表現を得る。\n",
    "        分類スコアはサイズが（系列長、ラベル数）の2次元配列\n",
    "        \"\"\"\n",
    "        assert len(spans) == len(scores)\n",
    "        num_entity_type = self.num_entity_type\n",
    "\n",
    "        # 特殊トークン（[CLS]/[SEP]/[PAD] 等, span==-1）を除外して可視領域のみ扱う。\n",
    "        scores = [score for score, span in zip(scores, spans) if span[0] != -1]\n",
    "        spans = [span for span in spans if span[0] != -1]\n",
    "\n",
    "        # BIO 遷移制約下での最尤ラベル列を Viterbi で推定（スコアは各ラベルの対数尤度相当を想定）。\n",
    "        labels = self.Viterbi(scores, num_entity_type)\n",
    "\n",
    "        # 連続する同ラベルをまとめ、B/I に応じたエンティティ境界を復元する。\n",
    "        entities = []\n",
    "        for label, group in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
    "\n",
    "            group = list(group)\n",
    "            start = spans[group[0][0]][0]\n",
    "            end = spans[group[-1][0]][1]\n",
    "\n",
    "            if label != 0:  # 固有表現であれば（O 以外）\n",
    "                if 1 <= label <= num_entity_type:\n",
    "                    # ラベルが B-k（1..N）ならば、新しい entity を開始\n",
    "                    entity = {\n",
    "                        \"name\": text[start:end],\n",
    "                        \"span\": [start, end],\n",
    "                        \"type_id\": label,\n",
    "                    }\n",
    "                    entities.append(entity)\n",
    "                else:\n",
    "                    # ラベルが I-k（N+1..2N）ならば、直近の entity を延長\n",
    "                    # 注意：理想的には直前が同タイプの B/I であることが前提。\n",
    "                    # BIO 違反（文頭から I が始まる等）が残る場合はここで安全対策が必要。\n",
    "                    entity[\"span\"][1] = end\n",
    "                    entity[\"name\"] = text[entity[\"span\"][0] : entity[\"span\"][1]]\n",
    "\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14f43c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer_BIO'.\n"
     ]
    }
   ],
   "source": [
    "# 8-22\n",
    "# ---------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - BIO 方式の NER トークナイザ（NER_tokenizer_BIO）を事前学習 BERT と同じ語彙でロードし、\n",
    "#    教師データを BERT 入力（input_ids/attention_mask/token_type_ids）＋BIO ラベル列に変換して\n",
    "#    DataLoader を用意する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - BIO ラベル空間は O=0, B-k= k, I-k= k+N（N=num_entity_type）で総数は 2N+1。\n",
    "#    例：N=8 → ラベル総数 m = 17（O=0, B-1..B-8, I-1..I-8）。\n",
    "#  - 学習時の損失はトークン単位の多クラス交差エントロピーで計算するのが定石。\n",
    "#    [CLS]/[SEP]/PAD は損失に含めないため、後段の学習ループで ignore_index=-100 を適用する（8-16参照）。\n",
    "#  - max_length はトークナイズ後の系列長の上限。長文は切り詰められるため、必要に応じて 256/384/512 や\n",
    "#    スライディングウィンドウを検討する。\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# トークナイザのロード\n",
    "# 固有表現のカテゴリーの数`num_entity_type`を入力に入れる必要がある。\n",
    "# 注意：\n",
    "#  - `num_entity_type=8` は 8-14 の type_id_dict（人名/法人名/.../イベント名）に対応。\n",
    "#  - 学習・推論で同じ MODEL_NAME（語彙・形態素前処理）を必ず共有する。\n",
    "tokenizer = NER_tokenizer_BIO.from_pretrained(MODEL_NAME, num_entity_type=8)\n",
    "\n",
    "# データセットの作成\n",
    "# ポイント：\n",
    "#  - create_dataset は encode_plus_tagged を用いて BIO ラベル列を付与し、\n",
    "#    [CLS]/[SEP]/PAD を含む固定長テンソル辞書へ整形する（8-15 参照）。\n",
    "#  - NFKC 正規化など前処理は学習・推論で一貫させる（8-14 参照）。\n",
    "max_length = 128\n",
    "dataset_train_for_loader = create_dataset(tokenizer, dataset_train, max_length)\n",
    "dataset_val_for_loader = create_dataset(tokenizer, dataset_val, max_length)\n",
    "\n",
    "# データローダの作成\n",
    "# 推奨設定メモ：\n",
    "#  - 学習は shuffle=True（i.i.d. 近似と汎化向上）。\n",
    "#  - 検証/テストは順序維持で十分。バッチサイズは VRAM/メモリと相談。\n",
    "#  - 実運用では num_workers, pin_memory（CUDA 時）や独自 collate_fn（可変長扱い）を検討。\n",
    "dataloader_train = DataLoader(dataset_train_for_loader, batch_size=32, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27997659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  | Name    | Type                       | Params | Mode\n",
      "--------------------------------------------------------------\n",
      "0 | bert_tc | BertForTokenClassification | 110 M  | eval\n",
      "--------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.159   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc694e8402d4d2aa979d6067f31c3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cef1bb25b448bfb3c121b132c21d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6293d5f5fc1a41c1b0d3d7be57d8541d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264abb20d0934f3dbf7d35015abd3ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d8935aab02486eb8acd73cce7b242d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f56ac8320544baba009a1bd0e4ad96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93f37f400b14ac2b9c591da889c0bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1070/1070 [00:11<00:00, 93.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_entities': 2592, 'num_predictions': 2623, 'num_correct': 2260, 'precision': 0.8616088448341593, 'recall': 0.8719135802469136, 'f_value': 0.8667305848513903}\n"
     ]
    }
   ],
   "source": [
    "# 8-23（説明コメント付き：Lightning v2 対応・可搬デバイス・BIO NER 推論まで一式）\n",
    "\n",
    "# 目的：\n",
    "#  - BIO 方式で学習するトークン分類（NER）を PyTorch Lightning でファインチューニングし、\n",
    "#    ベスト重みを用いてテスト文書からエンティティを抽出・評価する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - ラベル空間：O=0, B-k=k, I-k=k+N（N=num_entity_type）→ 総ラベル数 = 2N+1\n",
    "#  - 損失：トークン毎の多クラス交差エントロピー。PAD/CLS/SEP は ignore_index=-100 で除外（8-16 参照）。\n",
    "#  - 推論：各トークンのロジット scores[t, c] を得て、BIO 遷移制約付き Viterbi（8-21）で系列的に一貫した\n",
    "#           ラベル列を復号 → 連続区間をエンティティに変換。\n",
    "#  - 評価：スパン完全一致 + タイプ一致で micro-Precision/Recall/F1（8-19 参照）。\n",
    "\n",
    "# 注意点（実装）：\n",
    "#  - Lightning v2 では `gpus=1` は非推奨・廃止。可搬な `accelerator=\"auto\", devices=1` を使用。\n",
    "#  - `.cuda()` 固定は Mac(MPS)/CPU で失敗する。モデルの実デバイスに合わせて `.to(device)` を使う。\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# ファインチューニング\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,  # 版差で無視される可能性あり。必要に応じて state_dict 保存も検討。\n",
    "    dirpath=\"model_BIO/\",\n",
    ")\n",
    "\n",
    "# 旧式：pl.Trainer(gpus=1, ...) は v2 でエラーになるため修正\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",  # CUDA/MPS/CPU を自動選択\n",
    "    devices=1,\n",
    "    max_epochs=5,\n",
    "    callbacks=[checkpoint],\n",
    ")\n",
    "\n",
    "# PyTorch Lightningのモデルのロード\n",
    "num_entity_type = 8\n",
    "num_labels = 2 * num_entity_type + 1  # O=0, B(1..8), I(9..16) → 17\n",
    "model = BertForTokenClassification_pl(MODEL_NAME, num_labels=num_labels, lr=1e-5)\n",
    "\n",
    "# ファインチューニング（train/val）\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "best_model_path = checkpoint.best_model_path\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 性能評価（テスト相当）\n",
    "#  - ベスト ckpt を LightningModule としてロードし、生の HF モデルを取り出す。\n",
    "#  - 推論時は model.eval() + torch.no_grad() を徹底し、ドロップアウトを無効化。\n",
    "#  - 可搬デバイス（MPS → CUDA → CPU）に移す。\n",
    "\n",
    "\n",
    "def pick_device():\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = pick_device()\n",
    "\n",
    "model = BertForTokenClassification_pl.load_from_checkpoint(best_model_path)\n",
    "bert_tc = model.bert_tc.to(device).eval()\n",
    "\n",
    "entities_list = []  # 正解エンティティ（評価用）\n",
    "entities_predicted_list = []  # 予測エンティティ\n",
    "\n",
    "for sample in tqdm(dataset_test):\n",
    "    text = sample[\"text\"]\n",
    "\n",
    "    # 分類スコアを得るために、トークン化 + 原文スパン取得（BIO 復号で境界復元に必要）\n",
    "    encoding, spans = tokenizer.encode_plus_untagged(text, return_tensors=\"pt\")\n",
    "    # 可搬デバイスへ転送（.cuda() 固定は避ける）\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "    # 推論（B=1 → logits 形状は [1, T, C]）\n",
    "    with torch.no_grad():\n",
    "        output = bert_tc(**encoding)\n",
    "        scores = output.logits\n",
    "        # Viterbi は [T, C] を想定。CPU へ移し Python リスト化。\n",
    "        scores = scores[0].detach().cpu().numpy().tolist()\n",
    "\n",
    "    # 分類スコアを固有表現に変換（BIO 遷移制約を尊重した復号）\n",
    "    entities_predicted = tokenizer.convert_bert_output_to_entities(text, scores, spans)\n",
    "\n",
    "    entities_list.append(sample[\"entities\"])\n",
    "    entities_predicted_list.append(entities_predicted)\n",
    "\n",
    "# micro-Precision/Recall/F1 を表示\n",
    "#  - 8-19 の evaluate_model は 0 除算ケアが未実装なら注意（分母=0 のときは 0.0 を返す等の保護推奨）。\n",
    "print(evaluate_model(entities_list, entities_predicted_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
