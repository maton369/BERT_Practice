{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c83cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-2\n",
    "\n",
    "# =========================================================\n",
    "# BERT（Bidirectional Encoder Representations from Transformers）の理論メモ\n",
    "# ---------------------------------------------------------\n",
    "# ■ 目的\n",
    "#  - 文脈の双方向（左←→右）を同時に考慮した潜在表現 h ∈ R^H を得る。\n",
    "#  - 事前学習（Pretraining）で一般的言語能力を獲得し、下流タスク（分類・系列ラベリング・QA等）に微調整（Fine-tuning）で適用。\n",
    "#\n",
    "# ■ 事前学習タスク\n",
    "#  - MLM（Masked Language Modeling）:\n",
    "#    入力トークン列 x の一部を [MASK] 等で隠し、p(x_masked | context) を最大化する。\n",
    "#    近似的には、各マスク位置 i に対して softmax(W h_i) の対数尤度を最大化。\n",
    "#  - NSP（Next Sentence Prediction）:\n",
    "#    文 A の直後に文 B が来るかを 2 値分類（Tohoku BERT では採用）。RoBERTa 系では廃止例もある。\n",
    "#\n",
    "# ■ モデル（BertModel）の内部\n",
    "#  - 入力埋め込み: token + position + segment（token_type）を加算して X ∈ R^{B×L×H} を作る。\n",
    "#  - Transformer Encoder（L層）: 各層で Multi-Head Self-Attention + FFN。\n",
    "#    • Attention は Scaled Dot-Product:\n",
    "#        Q = XW_Q, K = XW_K, V = XW_V  (各 W_* ∈ R^{H×d_k})\n",
    "#        scores = Q K^T / sqrt(d_k)\n",
    "#        A = softmax(scores + mask)\n",
    "#        head = A V\n",
    "#      複数 head を結合し線形写像で次層へ。\n",
    "#    • 計算量: O(L^2 H)（長文入力ほど二乗で増える）; メモリもほぼ O(L^2)。\n",
    "#  - 出力:\n",
    "#    • last_hidden_state: 形状 [B, L, H]。各トークン位置の文脈化表現。\n",
    "#    • pooler_output: 形状 [B, H]。最終層の [CLS] ベクトルに線形＋tanh を適用（主に文分類の初期ベースライン）。\n",
    "#    • hidden_states, attentions（オプション）: 各層の中間表現やアテンション行列を返す。\n",
    "#\n",
    "# ■ 日本語トークナイザ（BertJapaneseTokenizer）\n",
    "#  - 典型的には形態素解析（MeCab/fugashi 等）で語彙単位に分割 → WordPiece に細分化。\n",
    "#  - 特殊トークン: [CLS]（先頭）, [SEP]（区切り）, [PAD], [MASK], [UNK] など。\n",
    "#  - 下流入力の慣例:\n",
    "#      input_ids        : [B, L]（語彙ID）\n",
    "#      attention_mask   : [B, L]（pad 0/非pad 1）\n",
    "#      token_type_ids   : [B, L]（文 A=0 / 文 B=1）\n",
    "#    これらを model(**batch) で渡す。\n",
    "#\n",
    "# ■ 実運用の注意\n",
    "#  - 最大系列長: 多くの BERT は L ≤ 512。超える場合は分割や Longformer 等の検討。\n",
    "#  - 学習: AdamW + 学習率ウォームアップ + 余弦減衰などが定番。weight decay は LayerNorm/バイアス除外。\n",
    "#  - 乱数・再現性: torch.manual_seed, numpy.random.seed を固定。dropout の影響も考慮。\n",
    "#  - 参照: Devlin et al., 2018; Tohoku NLP \"cl-tohoku/bert-base-japanese\" 系列\n",
    "# =========================================================\n",
    "\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 依存関係の注記（インストール時の目安）\n",
    "#  - transformers >= 4.x\n",
    "#  - fugashi, ipadic（や unidic-lite）: BertJapaneseTokenizer の形態素解析で利用されることが多い\n",
    "#    例) pip install transformers fugashi ipadic\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# （参考）実行例：事前学習済み日本語BERTの読み込み\n",
    "#   ※ 下の例は「使い方の目安」を示すためのコメントであり、実行は任意です。\n",
    "#\n",
    "# # 1) 日本語BERTのトークナイザ／モデルをロード\n",
    "# tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "# model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "# model.eval()  # 推論時は eval モードで dropout 等を停止\n",
    "#\n",
    "# # 2) テキストを ID 列へ変換\n",
    "# #   return_tensors=\"pt\" で PyTorch テンソルを自動作成\n",
    "# batch = tokenizer(\n",
    "#     [\"今日は良い天気ですね。\", \"しかし少し風が強いかもしれません。\"],\n",
    "#     padding=True, truncation=True, max_length=128, return_tensors=\"pt\"\n",
    "# )\n",
    "# # batch: dict で input_ids/attention_mask/token_type_ids を含む\n",
    "# # 形状: input_ids.shape == [B, L]\n",
    "#\n",
    "# # 3) モデルへ前向き計算\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**batch, output_attentions=False, output_hidden_states=False)\n",
    "#\n",
    "# # 4) 出力の形状\n",
    "# # outputs.last_hidden_state: [B, L, H]  各トークン位置の表現\n",
    "# # outputs.pooler_output   : [B, H]      [CLS] 由来（文表現のベースライン）\n",
    "#\n",
    "# # 5) 用途の例\n",
    "# # - 文分類: pooler_output か、[CLS] の last_hidden_state[:, 0, :] を線形分類器へ\n",
    "# # - トークン分類（固有表現抽出など）: last_hidden_state を各位置で線形＋softmax\n",
    "# # - QA: [CLS]/[SEP] を含むペア入力で start/end スパンを回帰\n",
    "#\n",
    "# # 6) 理論と設計の対応\n",
    "# # - attention_mask は pad 位置へのソフトマックスを -∞ マスクで抑制（scores に加算）\n",
    "# # - token_type_ids は NSP/ペア入力の区別（A:0, B:1）。単文なら全 0 でよい。\n",
    "# # - 学習では MLM/NSP（または SOP）に対応するヘッドを別途持つ（BertForPreTraining 等）。\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67cf6330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df59c5a1fb354d99ad3c96f411f0c783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9975b6c464cc46f880ed7d6e4d8053e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e09ee566cb349518530eaf57b946f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/479 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4-3\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 理論メモ：Whole Word Masking（WWM）版日本語BERTを使う理由\n",
    "# ---------------------------------------------------------\n",
    "# ◇ 標準MLM vs. Whole Word Masking\n",
    "#  - 標準のMLM（WordPiece単位の隠し方）では、語が「サブワード」に分割された場合、\n",
    "#    その一部のサブワードだけが[MASK]になることが多い。\n",
    "#  - WWMは「同じ語（= 形態素→WordPieceに分解される前の“語”）」に属する全サブワードを\n",
    "#    まとめて同時にマスクする。これによりモデルは「語レベルの完形復元」を学ぶ圧力が強まり、\n",
    "#    文脈一貫性や語彙レベルの表現獲得に寄与する（サブワード断片の当て推量を減らす）。\n",
    "#  - 直感的には、より強い隠し課題（harder pretext task）→汎化に寄与する可能性。\n",
    "#    ただし学習難易度は上がりうる（学習安定化はデータとハイパラ次第）。\n",
    "#\n",
    "# ◇ 日本語での影響\n",
    "#  - 日本語は語境界が空白で明示されないため、形態素解析（例：MeCab + 辞書）で語単位を推定→\n",
    "#    さらにWordPieceで細分化、という段階を踏む。この「語」単位でマスクを丸ごと掛けるのがWWM。\n",
    "#  - 形態素解析の精度や辞書選択（ipadic / unidic-lite 等）に依存する側面がある。\n",
    "#\n",
    "# ◇ 実務上の注意\n",
    "#  - このモデル名 'tohoku-nlp/bert-base-japanese-whole-word-masking' は、\n",
    "#    東北大NLPが公開している日本語BERT（WWM版）の重み・語彙を指す。\n",
    "#  - Tokenizer/Modelは対応するペアを使う（語彙の不一致はIDずれ→性能劣化）。\n",
    "#  - 依存関係：transformers、fugashi、ipadic（またはunidic-lite）等を事前にインストール。\n",
    "#    例：pip install transformers fugashi ipadic\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"  # WWM版日本語BERT（語レベルでの一括マスキングで事前学習）\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# （参考）挙動確認スニペット：※必要なら実行\n",
    "# text = \"深層学習の事前学習モデルBERTは強力です。\"\n",
    "# enc = tokenizer(\n",
    "#     text,\n",
    "#     return_tensors=\"pt\",      # PyTorchテンソルで返す\n",
    "#     padding=\"max_length\",     # 最大長にパディング（推論/バッチ化の整列用）\n",
    "#     truncation=True,          # 上限長超過時に切り詰め\n",
    "#     max_length=32\n",
    "# )\n",
    "# # enc[\"input_ids\"].shape == [1, 32], enc[\"attention_mask\"].shape == [1, 32]\n",
    "# # WWMは事前学習時のマスキング戦略であり、推論時のトークナイズ結果（input_ids自体）が\n",
    "# # 直接変わるわけではない点に注意（学習済み表現に差が出る）。\n",
    "#\n",
    "# # なお、下流タスクでは対応するモデル（例：BertModel/BertForSequenceClassification など）を\n",
    "# # 同じ `model_name` から読み出し、tokenizerとペアで使うことが重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e26b6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-4 ～ 4-6\n",
    "# =========================================================\n",
    "# 理論補足：BertJapaneseTokenizerのトークナイズ動作（日本語×WordPiece）\n",
    "# ---------------------------------------------------------\n",
    "# ■ 概要\n",
    "#  - BertJapaneseTokenizer は「形態素解析 → WordPiece 分割」という2段階でトークン化する。\n",
    "#    1) 形態素解析：文を語（形態素）へ分割（MeCab + 辞書 ipadic / unidic-lite 等）\n",
    "#    2) WordPiece：各「語」を語彙に基づいて更にサブワードへ分割（語彙外は細分化、最悪 [UNK]）\n",
    "#  - 返り値 tokenizer.tokenize(text) は特殊トークンを付与しない素のサブワード列（list[str]）。\n",
    "#    例：['自', '然', '言', '語', '処', '理'] のような字単位、あるいは ['自然', '言語', '処理'] など\n",
    "#    語彙と辞書の組み合わせにより変動する（＝「必ずこの分割になる」とは限らない）。\n",
    "#\n",
    "# ■ Whole Word Masking（WWM）との関係\n",
    "#  - 本トークナイザ自体は「推論・前処理」でのサブワード列を返すのみ。\n",
    "#  - WWM は「事前学習時のマスク戦略」で、同一“語”（形態素に相当）に属するサブワードを\n",
    "#    まとめて同時にマスクする。推論時の tokenize 結果は WWM の有無で直接は変わらない。\n",
    "#\n",
    "# ■ 文ごとの観察ポイント\n",
    "#  (A) 「明日は自然言語処理の勉強をしよう。」\n",
    "#    - 「自然言語処理」は複合名詞。形態素解析の辞書設定により\n",
    "#      「自然/言語/処理」や「自然言語/処理」等に分かれうる。\n",
    "#    - その後、WordPiece がさらに細分化（語彙にあればそのまま、無ければ細切れ）。\n",
    "#  (B) 「明日はマシンラーニングの勉強をしよう。」\n",
    "#    - カタカナ複合語。「マシン」「ラーニング」に分かれる場合や\n",
    "#      「マシンラーニング」単語→サブワード細分の可能性がある。\n",
    "#  (C) 「機械学習を中国語にすると机器学习だ。」\n",
    "#    - 末尾の「机器学习」は中国語（簡体字）。日本語辞書では未知語扱いになりやすく、\n",
    "#      WordPiece が文字単位や部分列へ分解、最悪 [UNK] へフォールバックしうる。\n",
    "#    - BERT の日本語語彙は CJK を広く含むため [UNK] にならず字単位分割で拾えることも多い。\n",
    "#\n",
    "# ■ 実務上のTips\n",
    "#  - 再現性：辞書種別（ipadic / unidic-lite）・バージョン、transformers のバージョンで分割が変わる。\n",
    "#  - 下流処理：tokenize → convert_tokens_to_ids → [CLS]/[SEP] 付与 → attention_mask 作成、が一般手順。\n",
    "#  - 形状把握：下流モデル入力は通常 encode_plus / __call__（return_tensors=\"pt\"）を使うのが安全。\n",
    "# =========================================================\n",
    "\n",
    "# 既に 4-3 で `tokenizer` を作っている想定。\n",
    "# セッション直実行でも動くように保険として用意（未定義なら初期化）。\n",
    "try:\n",
    "    tokenizer  # 変数が存在するかチェック\n",
    "except NameError:\n",
    "    from transformers import BertJapaneseTokenizer\n",
    "\n",
    "    model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 4-4：語彙・辞書により複合名詞「自然言語処理」の扱いが変動しうる点に注目\n",
    "tokens_44 = tokenizer.tokenize(\"明日は自然言語処理の勉強をしよう。\")\n",
    "# デバッグ表示（必要ならコメント解除）\n",
    "# print(tokens_44)\n",
    "\n",
    "# 4-5：カタカナ複合語の分割（「マシン」「ラーニング」等）やサブワード接頭辞（##）の有無に注目\n",
    "tokens_45 = tokenizer.tokenize(\"明日はマシンラーニングの勉強をしよう。\")\n",
    "# print(tokens_45)\n",
    "\n",
    "# 4-6：簡体字中国語「机器学习」の未知語挙動（字単位分割や [UNK] フォールバック）に注目\n",
    "tokens_46 = tokenizer.tokenize(\"機械学習を中国語にすると机器学习だ。\")\n",
    "# print(tokens_46)\n",
    "\n",
    "# （参考）ID 列へ変換して下流モデル入力へ繋ぐ例：\n",
    "# enc_44 = tokenizer('明日は自然言語処理の勉強をしよう。', return_tensors='pt')\n",
    "# # enc_44 は input_ids / attention_mask / token_type_ids を含む辞書\n",
    "# # enc_44['input_ids'].shape == [1, L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03c542f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> input_ids: [2, 11475, 9, 1757, 1882, 2762, 5, 8192, 11, 2132, 28489, 8, 3]\n",
      ">> tokens: ['[CLS]', '明日', 'は', '自然', '言語', '処理', 'の', '勉強', 'を', 'しよ', '##う', '。', '[SEP]']\n",
      ">> decode (with special): [CLS] 明日 は 自然 言語 処理 の 勉強 を しよう 。 [SEP]\n",
      ">> decode (no  special): 明日 は 自然 言語 処理 の 勉強 を しよう 。\n",
      ">> batch keys: ['input_ids', 'token_type_ids', 'attention_mask']\n",
      ">> shapes: input_ids (1, 13) attention_mask (1, 13) token_type_ids (1, 13)\n",
      ">> packed shapes: input_ids (1, 64) attention_mask (1, 64)\n",
      ">> non-pad length: 64\n"
     ]
    }
   ],
   "source": [
    "# 4-7\n",
    "# =========================================================\n",
    "# 理論メモ：`tokenize` と `encode` の違い（BERT日本語，WWM版）\n",
    "# ---------------------------------------------------------\n",
    "# ■ `tokenize(text)`：サブワード列（strのリスト）を返すだけ。特殊トークンは付与しない。\n",
    "# ■ `encode(text)`  ：語彙ID列（intのリスト）を返す。既定で [CLS] と [SEP] が付与される\n",
    "#                      （transformers では `add_special_tokens=True` が既定）。\n",
    "# つまり、学習/推論でモデルに渡す最小単位は「ID列」であり、`encode` または\n",
    "# `tokenizer(..., return_tensors='pt')` を使ってテンソル化するのが実務的。\n",
    "#\n",
    "# ■ 特殊トークン\n",
    "#  - [CLS]：文頭（文分類などで表現を使用）\n",
    "#  - [SEP]：文区切り（単文でも文末に付く）\n",
    "#  - [PAD]：バッチ整列用（`padding` で追加）\n",
    "#  - [MASK]：事前学習MLMで使用（推論の前処理では出てこない）\n",
    "#\n",
    "# ■ 再現性の注意\n",
    "#  - 形態素辞書（ipadic / unidic-liteなど）と語彙（model_name）に依存してIDが決まる。\n",
    "#  - 同じテキストでも tokenizer/model の種類やバージョンが異なると ID は変わりうる。\n",
    "#\n",
    "# ■ 下流タスクへの橋渡し\n",
    "#  - `encode` は ID のみで attention_mask を返さない（= PAD無視マスクが無い）。\n",
    "#    実運用は `tokenizer(text, return_tensors='pt', padding=..., truncation=...)`\n",
    "#    で `input_ids / attention_mask / token_type_ids` を一括生成するのが安全。\n",
    "# =========================================================\n",
    "\n",
    "# 依存：4-3 で tokenizer が定義済みの想定だが、未定義なら初期化しておく\n",
    "try:\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    from transformers import BertJapaneseTokenizer\n",
    "\n",
    "    model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# --- 観察対象テキスト ---\n",
    "text = \"明日は自然言語処理の勉強をしよう。\"\n",
    "\n",
    "# 1) encode：ID列（既定で [CLS], [SEP] 付き）を得る\n",
    "input_ids = tokenizer.encode(text)  # add_special_tokens=True（既定）\n",
    "print(\">> input_ids:\", input_ids)\n",
    "\n",
    "# 2) 可読化：ID → サブワード列（特殊トークンを含む）\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(\">> tokens:\", tokens)\n",
    "#   実務メモ：先頭が [CLS]、末尾が [SEP] になっていることを確認すると下流処理が安定\n",
    "\n",
    "# 3) 逆変換：ID列 → 文字列（decode）\n",
    "decoded_with_special = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "decoded_without_special = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "print(\">> decode (with special):\", decoded_with_special)\n",
    "print(\">> decode (no  special):\", decoded_without_special)\n",
    "#   理論的含意：decodeは可逆でない場合がある（空白や正規化の扱い，未登録語の分解など）\n",
    "\n",
    "# 4) 実運用：モデル入力で必要なテンソルを一括生成（推奨パス）\n",
    "batch = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",  # PyTorchテンソル化\n",
    "    padding=False,  # 単文なので不要（バッチなら True/ \"longest\"/ \"max_length\" など）\n",
    "    truncation=True,  # 最大長越えの安全弁（デフォルトでは切り捨てない）\n",
    "    max_length=128,\n",
    ")\n",
    "print(\">> batch keys:\", list(batch.keys()))\n",
    "print(\n",
    "    \">> shapes:\",\n",
    "    \"input_ids\",\n",
    "    tuple(batch[\"input_ids\"].shape),\n",
    "    \"attention_mask\",\n",
    "    tuple(batch[\"attention_mask\"].shape),\n",
    "    \"token_type_ids\",\n",
    "    tuple(batch[\"token_type_ids\"].shape),\n",
    ")\n",
    "#   理論メモ：attention_mask は softmax(QK^T/√d_k) の前に「PAD位置へ -∞ 加算」相当の抑制に使われる\n",
    "#             （= PADに注意が向かない）。token_type_ids は文A=0/文B=1 の区別（単文ならゼロ列）。\n",
    "\n",
    "# 5) 参考：長文での切り詰め・パディング（計算量 O(L^2) への配慮）\n",
    "long_text = text * 100\n",
    "enc_packed = tokenizer(\n",
    "    long_text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",  # 事前に固定長へ整列（デプロイでのスループット向上）\n",
    "    truncation=True,  # 最大長を超えた分をカット\n",
    "    max_length=64,  # 例：64に固定（BERT標準は最大512）\n",
    ")\n",
    "print(\n",
    "    \">> packed shapes:\",\n",
    "    \"input_ids\",\n",
    "    tuple(enc_packed[\"input_ids\"].shape),\n",
    "    \"attention_mask\",\n",
    "    tuple(enc_packed[\"attention_mask\"].shape),\n",
    ")\n",
    "print(\">> non-pad length:\", int(enc_packed[\"attention_mask\"].sum().item()))\n",
    "#   理論メモ：Self-Attention の計算量は O(L^2 H)。L（系列長）を管理する設計は実運用で重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b82c58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> tokens: ['[CLS]', '明日', 'は', '自然', '言語', '処理', 'の', '勉強', 'を', 'しよ', '##う', '。', '[SEP]']\n",
      ">> roundtrip equal (ids): True\n",
      ">> roundtrip equal (toks): True\n",
      ">> convert_tokens_to_string: [CLS] 明日 は 自然 言語 処理 の 勉強 を しよう 。 [SEP]\n",
      ">> decode (keep specials):   [CLS] 明日 は 自然 言語 処理 の 勉強 を しよう 。 [SEP]\n",
      ">> decode (skip  specials):  明日 は 自然 言語 処理 の 勉強 を しよう 。\n",
      ">> specials in tokens: ['[CLS]', '[SEP]']\n",
      ">> contains [UNK]    : False\n",
      ">> tokens (no specials): ['明日', 'は', '自然', '言語', '処理', 'の', '勉強', 'を', 'しよ', '##う', '。']\n"
     ]
    }
   ],
   "source": [
    "# 4-8\n",
    "# =========================================================\n",
    "# 理論メモ：`convert_ids_to_tokens` の役割と `decode` との違い\n",
    "# ---------------------------------------------------------\n",
    "# ■ 役割\n",
    "#   - `convert_ids_to_tokens(ids)` は、各 ID を **WordPiece サブワード** 文字列へ 1:1 で写像する。\n",
    "#   - 特殊トークン（[CLS], [SEP], [PAD], [MASK], [UNK]）も **そのまま文字列**として返す。\n",
    "#\n",
    "# ■ `decode` との違い\n",
    "#   - `decode(ids)` は **可読な文章**へ連結・正規化を行う（特殊トークンは既定で除去可能）。\n",
    "#   - 一方 `convert_ids_to_tokens` は **分割境界を保持**（学習・解析・デバッグ向け）。\n",
    "#\n",
    "# ■ 実務的含意\n",
    "#   - NER 等のトークン分類では **サブワード境界**が重要なので `convert_ids_to_tokens` で形を確認。\n",
    "#   - 文章表示は `decode` / `convert_tokens_to_string` を使う（可逆でない場合あり：空白・正規化・未知語）。\n",
    "#   - WWM（Whole Word Masking）は「事前学習時のマスク戦略」であり、ここでの表示自体は変えない。\n",
    "# =========================================================\n",
    "\n",
    "# 依存：4-3/4-7 で tokenizer と input_ids が定義済みの想定。なければ初期化して作る。\n",
    "try:\n",
    "    tokenizer  # 存在確認\n",
    "except NameError:\n",
    "    from transformers import BertJapaneseTokenizer\n",
    "\n",
    "    model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "try:\n",
    "    input_ids  # 存在確認\n",
    "except NameError:\n",
    "    # 4-7 相当の最小再現\n",
    "    text = \"明日は自然言語処理の勉強をしよう。\"\n",
    "    input_ids = tokenizer.encode(text)  # 既定で [CLS], [SEP] が付与される\n",
    "\n",
    "# 1) ID → サブワード列（特殊トークン含む）\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(\">> tokens:\", tokens)\n",
    "\n",
    "# 2) 可逆性の軽い確認：tokens → ids → tokens\n",
    "roundtrip_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "roundtrip_tokens = tokenizer.convert_ids_to_tokens(roundtrip_ids)\n",
    "print(\">> roundtrip equal (ids):\", roundtrip_ids == input_ids)\n",
    "print(\">> roundtrip equal (toks):\", roundtrip_tokens == tokens)\n",
    "\n",
    "# 3) 文章可読化パス（分割境界を潰して連結）\n",
    "text_from_tokens = tokenizer.convert_tokens_to_string(tokens)\n",
    "text_from_decode_keep = tokenizer.decode(\n",
    "    input_ids, skip_special_tokens=False\n",
    ")  # 特殊トークン表示\n",
    "text_from_decode_skip = tokenizer.decode(\n",
    "    input_ids, skip_special_tokens=True\n",
    ")  # 特殊トークン除去\n",
    "print(\">> convert_tokens_to_string:\", text_from_tokens)\n",
    "print(\">> decode (keep specials):  \", text_from_decode_keep)\n",
    "print(\">> decode (skip  specials): \", text_from_decode_skip)\n",
    "\n",
    "# 4) 解析のコツ：特殊トークンや未知語の存在確認\n",
    "specials = {\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"[UNK]\"}\n",
    "has_unk = any(t == \"[UNK]\" for t in tokens)\n",
    "present_specials = sorted(set(t for t in tokens if t in specials))\n",
    "print(\">> specials in tokens:\", present_specials)\n",
    "print(\">> contains [UNK]    :\", has_unk)\n",
    "\n",
    "# 5) （参考）特殊トークンを除いた実トークン列\n",
    "tokens_wo_specials = [t for t in tokens if t not in specials]\n",
    "print(\">> tokens (no specials):\", tokens_wo_specials)\n",
    "\n",
    "# 6) 補足理論メモ：\n",
    "#   - WordPiece は未知語を細分化し、語彙に存在しない断片は最終的に [UNK] へ落ちる場合がある。\n",
    "#   - 日本語では「形態素解析→WordPiece」の二段で分割されるため、辞書・語彙・バージョンで結果が揺れる。\n",
    "#   - サブワード接頭辞（例：'##学' のような継続マーク）は語彙に依存（日本語BERTでも用いられる）。\n",
    "#   - トークン分類では「語→サブワード」のアライメント管理が必要（fast系トークナイザなら offset_mapping）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7559e579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# encoding (BatchEncoding repr):\n",
      "{'input_ids': [2, 11475, 5, 11385, 9, 16577, 75, 8, 3, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}\n",
      "\n",
      "# input_ids: [2, 11475, 5, 11385, 9, 16577, 75, 8, 3, 0, 0, 0]\n",
      "# attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "# token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "# tokens:\n",
      "['[CLS]', '明日', 'の', '天気', 'は', '晴れ', 'だ', '。', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "# specials & length:\n",
      "CLS at: 0  / SEP at: 8  / PAD positions: [9, 10, 11]\n",
      "effective (non-PAD) length = 9  / total length = 12\n",
      "\n",
      "# decode (with specials): [CLS] 明日 の 天気 は 晴れ だ 。 [SEP] [PAD] [PAD] [PAD]\n",
      "# decode (skip specials):  明日 の 天気 は 晴れ だ 。\n",
      "\n",
      "# truncation check:\n",
      "input_ids (truncated)  : [2, 11475, 5, 11385, 9, 16577, 75, 8, 11475, 5, 11385, 3]\n",
      "attention_mask         : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "num_truncated_tokens   : 130\n",
      "overflowing_tokens (len): 130\n",
      "tokens (truncated)     : ['[CLS]', '明日', 'の', '天気', 'は', '晴れ', 'だ', '。', '明日', 'の', '天気', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 4-9\n",
    "# =========================================================\n",
    "# 理論メモ：パディング（padding='max_length'）と切り詰め（truncation=True）の意味\n",
    "# ---------------------------------------------------------\n",
    "# ■ 何が起きるか\n",
    "#  - `max_length=12`：出力系列長 L を 12 に固定する（特殊トークン [CLS]/[SEP] を含む）。\n",
    "#  - `padding='max_length'`：不足ぶんを [PAD] で右詰めパディングし、attention_mask は PAD 位置を 0 に。\n",
    "#  - `truncation=True`：L を超える場合、右側を切り詰める（通常 [CLS] 先頭固定、[SEP] は末尾側に残す）。\n",
    "#\n",
    "# ■ Attention とマスク\n",
    "#  - Self-Attention は softmax(QK^T / √d_k) V。PAD 位置は mask によりスコアへ -∞（相当）を加算し無視。\n",
    "#  - よって attention_mask=0 のトークン（PAD）には注意が向かず、勾配も基本的に流れない。\n",
    "#\n",
    "# ■ token_type_ids\n",
    "#  - 単文入力では通常すべて 0。文対（A,B）では A=0, B=1 で区別（NSP系やQAで利用）。\n",
    "#\n",
    "# ■ 実務含意\n",
    "#  - バッチ推論/学習で長さをそろえるための固定長化（throughput・安定化）。\n",
    "#  - 計算量 O(L^2 H) のため、L を管理する設計は重要（過大な max_length は計算/メモリ負荷）。\n",
    "# =========================================================\n",
    "\n",
    "# 依存：前セルで tokenizer が定義済みの前提（なければ初期化）\n",
    "try:\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    from transformers import BertJapaneseTokenizer\n",
    "\n",
    "    model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# --- 入力 ---\n",
    "text = \"明日の天気は晴れだ。\"\n",
    "\n",
    "# --- エンコード：固定長（L=12）でのパディング・切り詰め ---\n",
    "encoding = tokenizer(text, max_length=12, padding=\"max_length\", truncation=True)\n",
    "print(\"# encoding (BatchEncoding repr):\")\n",
    "print(encoding)  # dictライクな表示（input_ids / attention_mask / token_type_ids）\n",
    "\n",
    "# 個別に中身を明示（学習・デバッグ時に便利）\n",
    "print(\"\\n# input_ids:\", encoding[\"input_ids\"])\n",
    "print(\"# attention_mask:\", encoding[\"attention_mask\"])\n",
    "print(\"# token_type_ids:\", encoding.get(\"token_type_ids\"))\n",
    "\n",
    "# --- 可視化：ID -> サブワード列（特殊トークン込み） ---\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "print(\"\\n# tokens:\")\n",
    "print(tokens)\n",
    "\n",
    "# --- 解析：特殊トークンの位置と有効長（非PAD長） ---\n",
    "CLS_ID = tokenizer.cls_token_id\n",
    "SEP_ID = tokenizer.sep_token_id\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "ids = encoding[\"input_ids\"]\n",
    "mask = encoding[\"attention_mask\"]\n",
    "\n",
    "cls_pos = ids.index(CLS_ID) if CLS_ID in ids else None\n",
    "sep_pos = ids.index(SEP_ID) if SEP_ID in ids else None\n",
    "pad_positions = [i for i, t in enumerate(ids) if t == PAD_ID]\n",
    "effective_len = sum(mask)  # 非PAD長 = 有効トークン数\n",
    "\n",
    "print(\"\\n# specials & length:\")\n",
    "print(\"CLS at:\", cls_pos, \" / SEP at:\", sep_pos, \" / PAD positions:\", pad_positions)\n",
    "print(\"effective (non-PAD) length =\", effective_len, \" / total length =\", len(ids))\n",
    "\n",
    "# --- デコード（人が読む文字列へ） ---\n",
    "decoded_keep = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "decoded_skip = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "print(\"\\n# decode (with specials):\", decoded_keep)\n",
    "print(\"# decode (skip specials): \", decoded_skip)\n",
    "\n",
    "# --- 切り詰めの挙動確認（長文にして強制的にトランケーション） ---\n",
    "long_text = text * 20  # わざと長く\n",
    "enc_trunc = tokenizer(\n",
    "    long_text,\n",
    "    max_length=12,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_overflowing_tokens=True,  # どれだけ削られたかのメタ情報を得る\n",
    ")\n",
    "print(\"\\n# truncation check:\")\n",
    "print(\"input_ids (truncated)  :\", enc_trunc[\"input_ids\"])\n",
    "print(\"attention_mask         :\", enc_trunc[\"attention_mask\"])\n",
    "print(\"num_truncated_tokens   :\", enc_trunc.get(\"num_truncated_tokens\"))\n",
    "print(\"overflowing_tokens (len):\", len(enc_trunc.get(\"overflowing_tokens\", [])))\n",
    "print(\n",
    "    \"tokens (truncated)     :\", tokenizer.convert_ids_to_tokens(enc_trunc[\"input_ids\"])\n",
    ")\n",
    "\n",
    "# 理論補足：\n",
    "# - 右側切り詰めでは [CLS] は index 0 に残り、[SEP] は系列の末端側（通常 index = 有効長-1）を占める。\n",
    "# - 残りは [PAD] で埋められ、attention_mask=0 となるため、Self-Attention の計算から実質除外される。\n",
    "# - 文字列→サブワード→ID の過程で完全可逆性は保証されない（空白・正規化・未知語などの影響）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b97d4e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '明日', 'の', '天気', 'は', '[SEP]']\n",
      "# len(ids)= 6  effective(non-PAD)= 6\n",
      "# CLS at 0 / SEP at 5 / PAD positions []\n",
      "# raw subwords = 7  capacity = 4  -> truncated(est)= 3\n",
      "# decode(with specials): [CLS] 明日 の 天気 は [SEP]\n",
      "# decode(skip  specials): 明日 の 天気 は\n"
     ]
    }
   ],
   "source": [
    "# 4-10\n",
    "# =========================================================\n",
    "# 理論メモ：極小 max_length（=6）でのトランケーションとパディング\n",
    "# ---------------------------------------------------------\n",
    "# ■ 目標\n",
    "#   - 出力系列長 L を 6 に固定しつつ、[CLS]/[SEP] を必ず含める。\n",
    "#   - 単文入力のため、最大で「L-2=4 個」のサブワードしか保持できない。\n",
    "#\n",
    "# ■ 何が起こるか（単文・右詰め想定）\n",
    "#   - 変換順：text → 形態素解析 → WordPiece → 特殊トークン付与 → 切り詰め → パディング\n",
    "#   - 切り詰め（truncation=True）：右側（末尾側）から削る（単文は first sequence のみを削減）。\n",
    "#   - パディング（padding='max_length'）：不足分を [PAD] で右詰め。attention_mask=0 で無視。\n",
    "#   - 結果：index=0 に [CLS]、index = 有効長-1 に [SEP]、末尾側は [PAD] が埋まる（またはトークンが削られる）。\n",
    "#\n",
    "# ■ Self-Attention への影響\n",
    "#   - softmax(QK^T/√d_k) において、PAD 位置は mask によりスコアへ -∞ 相当が加わり、注意が向かない。\n",
    "#   - 有効長（non-PAD 長）を最小限にすると、計算量 O(L^2 H) は抑えられるが、文脈の損失（情報欠落）と交換になる。\n",
    "#\n",
    "# ■ 再現性の注意\n",
    "#   - 「保持できるサブワード数」は固定（ここでは 4）だが、実際にどのサブワードがそこに入るかは\n",
    "#     形態素辞書・語彙・バージョンに依存（= tokenize 結果が揺れる）。ID列の完全一致を要件にしない。\n",
    "# =========================================================\n",
    "\n",
    "# 依存：前セルで tokenizer と text が定義済みの前提。未定義なら保険として初期化する。\n",
    "try:\n",
    "    tokenizer  # 存在確認\n",
    "except NameError:\n",
    "    from transformers import BertJapaneseTokenizer\n",
    "\n",
    "    model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "try:\n",
    "    text  # 存在確認（前セル #4-9 と同一を想定）\n",
    "except NameError:\n",
    "    text = \"明日の天気は晴れだ。\"\n",
    "\n",
    "# --- エンコード：L=6 で固定長、右側切り詰め＋右詰めPad ---\n",
    "encoding = tokenizer(text, max_length=6, padding=\"max_length\", truncation=True)\n",
    "\n",
    "# --- 表示：サブワード列（特殊トークン込み） ---\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "print(tokens)\n",
    "\n",
    "# --- 解析（参考）：有効長と特殊トークンの配置を確認 ---\n",
    "ids = encoding[\"input_ids\"]\n",
    "mask = encoding[\"attention_mask\"]\n",
    "\n",
    "CLS_ID = tokenizer.cls_token_id\n",
    "SEP_ID = tokenizer.sep_token_id\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "cls_pos = ids.index(CLS_ID) if CLS_ID in ids else None\n",
    "sep_pos = ids.index(SEP_ID) if SEP_ID in ids else None\n",
    "pad_positions = [i for i, t in enumerate(ids) if t == PAD_ID]\n",
    "effective_len = sum(mask)  # 非PAD長\n",
    "\n",
    "print(\"# len(ids)=\", len(ids), \" effective(non-PAD)=\", effective_len)\n",
    "print(\"# CLS at\", cls_pos, \"/ SEP at\", sep_pos, \"/ PAD positions\", pad_positions)\n",
    "\n",
    "# --- どれだけ削られた可能性があるか（理論キャパ：L-2=4 サブワード） ---\n",
    "# tokenize（特殊トークンなし）で生のサブワード長を観察し、理論上の削減量を見積もる。\n",
    "raw_tokens = tokenizer.tokenize(text)\n",
    "capacity = 6 - 2  # [CLS] と [SEP] のぶん\n",
    "truncated_count_est = max(0, len(raw_tokens) - capacity)\n",
    "print(\n",
    "    \"# raw subwords =\",\n",
    "    len(raw_tokens),\n",
    "    \" capacity =\",\n",
    "    capacity,\n",
    "    \" -> truncated(est)=\",\n",
    "    truncated_count_est,\n",
    ")\n",
    "\n",
    "# --- 人が読む表示（可逆ではない点に注意） ---\n",
    "decoded_keep = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "decoded_skip = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "print(\"# decode(with specials):\", decoded_keep)\n",
    "print(\"# decode(skip  specials):\", decoded_skip)\n",
    "\n",
    "# 補足：\n",
    "# - 「切り詰め優先度」は単文では first sequence の右側。ペア入力では longest_first が既定（必要に応じ設定可）。\n",
    "# - L を極端に小さくすると [CLS]/[SEP] を除く保持域が狭く、文脈情報が失われやすい（下流性能に影響）。\n",
    "# - 固定長化はスループット安定化に有用だが、現実の文長分布を踏まえた L 設計（あるいは動的パディング）が望ましい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45c20a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# keys: ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "# input_ids[0]: [2, 11475, 5, 11385, 9, 16577, 75, 8, 3, 0]\n",
      "# input_ids[1]: [2, 6311, 14, 1132, 7, 16084, 332, 58, 10, 3]\n",
      "# attention_mask[0]: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "# attention_mask[1]: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "# token_type_ids[0]: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "# token_type_ids[1]: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "# sample[0] tokens:\n",
      "['[CLS]', '明日', 'の', '天気', 'は', '晴れ', 'だ', '。', '[SEP]', '[PAD]']\n",
      "# specials & length:\n",
      "  CLS at: 0 / SEP at: 8 / PAD positions: [9]\n",
      "  effective (non-PAD) length = 9  / total length = 10\n",
      "# decode (skip specials): 明日 の 天気 は 晴れ だ 。\n",
      "\n",
      "# sample[1] tokens:\n",
      "['[CLS]', 'パソコン', 'が', '急', 'に', '動か', 'なく', 'なっ', 'た', '[SEP]']\n",
      "# specials & length:\n",
      "  CLS at: 0 / SEP at: 9 / PAD positions: []\n",
      "  effective (non-PAD) length = 10  / total length = 10\n",
      "# decode (skip specials): パソコン が 急 に 動か なく なっ た\n",
      "\n",
      "# sample[0] raw_subwords=7 capacity=8 -> truncated(est)=0\n",
      "\n",
      "# sample[1] raw_subwords=9 capacity=8 -> truncated(est)=1\n",
      "\n",
      "# tensor shapes: input_ids (2, 10) attention_mask (2, 10) token_type_ids (2, 10)\n"
     ]
    }
   ],
   "source": [
    "# 4-11\n",
    "# =========================================================\n",
    "# 理論メモ：ミニバッチ入力（list[str]）＋固定長エンコードの挙動\n",
    "# ---------------------------------------------------------\n",
    "# ■ 目的\n",
    "#  - 2文を同時にトークナイズし、系列長 L=10 に固定（padding='max_length'）する。\n",
    "#  - 各文に [CLS]・[SEP] を必ず含めるので、実トークンの保持域は L-2=8 サブワードまで。\n",
    "#\n",
    "# ■ 生成される3要素\n",
    "#  - input_ids      : 各サブワード（含む特殊トークン）を語彙IDへ写像した配列（B×L）\n",
    "#  - attention_mask : 非PAD=1, PAD=0（Self-Attention の softmax 前スコアに -∞ 相当を与え注意抑制）\n",
    "#  - token_type_ids : 単文は全0（文対のとき A=0, B=1）\n",
    "#\n",
    "# ■ トランケーション（truncation=True）\n",
    "#  - 単文バッチなので「右側を優先的に切り詰め（only_first 相当）」→ 末尾側のサブワードが削られる。\n",
    "#  - [CLS] は index=0 に、[SEP] は「有効長-1」に配置されるのが不変量。\n",
    "#\n",
    "# ■ 実務上の含意\n",
    "#  - 長さ L の管理は計算量 O(L^2 H) に効く（BERTのSelf-Attentionは系列長二乗）。\n",
    "#  - 完全一致のトークン列は「辞書・語彙・バージョン」に依存して揺れるため、再現性要件は“性質”で置く。\n",
    "#    （例：有効長が上限内、特殊トークンの位置、PADのマスク整合 など）\n",
    "# =========================================================\n",
    "\n",
    "# 保険：前セルで tokenizer が未定義なら初期化\n",
    "try:\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    from transformers import BertJapaneseTokenizer\n",
    "\n",
    "    model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# --- 入力バッチ（B=2） ---\n",
    "text_list = [\n",
    "    \"明日の天気は晴れだ。\",\n",
    "    \"パソコンが急に動かなくなった。\",\n",
    "]  # 例：単文2件（文対ではない）\n",
    "\n",
    "# --- エンコード：固定長 L=10、右側切り詰め＋右詰めPad ---\n",
    "encoding = tokenizer(text_list, max_length=10, padding=\"max_length\", truncation=True)\n",
    "\n",
    "# 表示：BatchEncoding は dict ライク。学習・デバッグで中身を明示する。\n",
    "print(\"# keys:\", list(encoding.keys()))\n",
    "print(\"# input_ids[0]:\", encoding[\"input_ids\"][0])\n",
    "print(\"# input_ids[1]:\", encoding[\"input_ids\"][1])\n",
    "print(\"# attention_mask[0]:\", encoding[\"attention_mask\"][0])\n",
    "print(\"# attention_mask[1]:\", encoding[\"attention_mask\"][1])\n",
    "print(\"# token_type_ids[0]:\", encoding[\"token_type_ids\"][0])\n",
    "print(\"# token_type_ids[1]:\", encoding[\"token_type_ids\"][1])\n",
    "\n",
    "# --- 可視化：ID → サブワード列（特殊トークン込み） ---\n",
    "for i, ids in enumerate(encoding[\"input_ids\"]):\n",
    "    toks = tokenizer.convert_ids_to_tokens(ids)\n",
    "    print(f\"\\n# sample[{i}] tokens:\")\n",
    "    print(toks)\n",
    "\n",
    "    # 不変量チェック：特殊トークン位置と有効長\n",
    "    CLS_ID = tokenizer.cls_token_id\n",
    "    SEP_ID = tokenizer.sep_token_id\n",
    "    PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "    mask = encoding[\"attention_mask\"][i]\n",
    "    cls_pos = ids.index(CLS_ID) if CLS_ID in ids else None\n",
    "    sep_pos = ids.index(SEP_ID) if SEP_ID in ids else None\n",
    "    pad_positions = [j for j, t in enumerate(ids) if t == PAD_ID]\n",
    "    effective_len = sum(mask)  # 非PAD長\n",
    "\n",
    "    print(\"# specials & length:\")\n",
    "    print(\"  CLS at:\", cls_pos, \"/ SEP at:\", sep_pos, \"/ PAD positions:\", pad_positions)\n",
    "    print(\n",
    "        \"  effective (non-PAD) length =\", effective_len, \" / total length =\", len(ids)\n",
    "    )\n",
    "\n",
    "    # 参考：人が読む表示（可逆ではない点に注意）\n",
    "    print(\"# decode (skip specials):\", tokenizer.decode(ids, skip_special_tokens=True))\n",
    "\n",
    "# --- どれだけ削られた可能性があるか：理論キャパ（L-2）と比較 ---\n",
    "capacity = 10 - 2  # [CLS] と [SEP] のぶん\n",
    "for i, text in enumerate(text_list):\n",
    "    raw = tokenizer.tokenize(text)  # 特殊トークン無しのサブワード列\n",
    "    truncated_est = max(0, len(raw) - capacity)\n",
    "    print(\n",
    "        f\"\\n# sample[{i}] raw_subwords={len(raw)} capacity={capacity} -> truncated(est)={truncated_est}\"\n",
    "    )\n",
    "    # 備考：実際の削減は特殊トークン付与後・右側優先。語彙・辞書差で raw の値は変動しうる。\n",
    "\n",
    "# --- 参考：実運用（学習/推論）ではテンソル出力を一括生成するのが安全 ---\n",
    "batch_pt = tokenizer(\n",
    "    text_list,\n",
    "    max_length=10,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",  # ← PyTorch テンソルで返す（B×L）\n",
    ")\n",
    "print(\n",
    "    \"\\n# tensor shapes:\",\n",
    "    \"input_ids\",\n",
    "    tuple(batch_pt[\"input_ids\"].shape),\n",
    "    \"attention_mask\",\n",
    "    tuple(batch_pt[\"attention_mask\"].shape),\n",
    "    \"token_type_ids\",\n",
    "    tuple(batch_pt[\"token_type_ids\"].shape),\n",
    ")\n",
    "# 理論補足：attention_mask=0 の位置は Self-Attention の softmax 前に -∞ 相当が加算され、注意が向かない。\n",
    "#           これにより PAD トークンは表現学習に寄与せず、勾配も基本的に流れない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1afe958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# keys: ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "# input_ids[0]: [2, 11475, 5, 11385, 9, 16577, 75, 8, 3, 0, 0]\n",
      "# input_ids[1]: [2, 6311, 14, 1132, 7, 16084, 332, 58, 10, 8, 3]\n",
      "# attention_mask[0]: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "# attention_mask[1]: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "# token_type_ids[0]: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "# token_type_ids[1]: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "# sample[0] tokens:\n",
      "['[CLS]', '明日', 'の', '天気', 'は', '晴れ', 'だ', '。', '[SEP]', '[PAD]', '[PAD]']\n",
      "\n",
      "# sample[1] tokens:\n",
      "['[CLS]', 'パソコン', 'が', '急', 'に', '動か', 'なく', 'なっ', 'た', '。', '[SEP]']\n",
      "# sample[0] effective_len=9 / total=11 / PAD positions=[9, 10]\n",
      "# sample[1] effective_len=11 / total=11 / PAD positions=[]\n",
      "# longest effective_len in batch = 11\n",
      "\n",
      "# tensor shapes: input_ids (2, 11) attention_mask (2, 11) token_type_ids (2, 11)\n",
      "\n",
      "# fixed-length tensor shapes: input_ids (2, 24) attention_mask (2, 24)\n",
      "# fixed non-PAD per sample: [9, 11]\n"
     ]
    }
   ],
   "source": [
    "# 4-12\n",
    "# =========================================================\n",
    "# 理論メモ：padding='longest'（動的パディング）の意味と利点\n",
    "# ---------------------------------------------------------\n",
    "# ■ 目的\n",
    "#  - 同一ミニバッチ内で「最長系列（＝各サンプルのトークン長＋[CLS]/[SEP]）」に合わせて右詰めPad。\n",
    "#  - 各サンプルの「足りないぶん」だけ [PAD] を付与し、attention_mask=0 で無視させる。\n",
    "#\n",
    "# ■ 利点（O(L^2 H)観点）\n",
    "#  - BERTのSelf-Attention計算量は O(L^2 H)。固定長（max_length）で過剰Padを入れると無駄が増える。\n",
    "#  - 動的パディングは **バッチごとに最小限の長さ**に抑えるため、計算・メモリ効率が向上。\n",
    "#\n",
    "# ■ 注意\n",
    "#  - 右側切り詰め（truncation）はここでは指定しない（=元の長さを保つ）。長文が来るとそのまま長くなる。\n",
    "#  - 単文入力の token_type_ids は通常ゼロ列（文対時に A=0, B=1）。\n",
    "#  - WWM（Whole Word Masking）は事前学習時のマスク戦略であり、ここでのパディング挙動自体は変わらない。\n",
    "# =========================================================\n",
    "\n",
    "# 保険：tokenizer 未定義なら初期化（4-3 相当）\n",
    "try:\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    from transformers import BertJapaneseTokenizer\n",
    "\n",
    "    model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 保険：text_list 未定義なら用意（4-11 と同一想定）\n",
    "try:\n",
    "    text_list\n",
    "except NameError:\n",
    "    text_list = [\"明日の天気は晴れだ。\", \"パソコンが急に動かなくなった。\"]\n",
    "\n",
    "# --- エンコード：動的パディング（最長列に合わせる）---\n",
    "encoding = tokenizer(\n",
    "    text_list,\n",
    "    padding=\"longest\",  # ← バッチ内の最長列に合わせてPad\n",
    "    # truncation=False  # ← 既定（バッチ内では切り詰めない）\n",
    ")\n",
    "print(\"# keys:\", list(encoding.keys()))\n",
    "print(\"# input_ids[0]:\", encoding[\"input_ids\"][0])\n",
    "print(\"# input_ids[1]:\", encoding[\"input_ids\"][1])\n",
    "print(\"# attention_mask[0]:\", encoding[\"attention_mask\"][0])\n",
    "print(\"# attention_mask[1]:\", encoding[\"attention_mask\"][1])\n",
    "print(\"# token_type_ids[0]:\", encoding[\"token_type_ids\"][0])\n",
    "print(\"# token_type_ids[1]:\", encoding[\"token_type_ids\"][1])\n",
    "\n",
    "# --- 可視化：各サンプルのトークン列（特殊トークン込み）---\n",
    "for i, ids in enumerate(encoding[\"input_ids\"]):\n",
    "    toks = tokenizer.convert_ids_to_tokens(ids)\n",
    "    print(f\"\\n# sample[{i}] tokens:\")\n",
    "    print(toks)\n",
    "\n",
    "# --- 理論チェック：有効長（= attention_mask の合計）が「最長列」に合っているか ---\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "eff_lens = []\n",
    "for i, (ids, mask) in enumerate(zip(encoding[\"input_ids\"], encoding[\"attention_mask\"])):\n",
    "    eff_len = sum(mask)  # 非PAD長\n",
    "    pad_pos = [j for j, t in enumerate(ids) if t == PAD_ID]\n",
    "    eff_lens.append(eff_len)\n",
    "    print(\n",
    "        f\"# sample[{i}] effective_len={eff_len} / total={len(ids)} / PAD positions={pad_pos}\"\n",
    "    )\n",
    "print(\"# longest effective_len in batch =\", max(eff_lens))\n",
    "\n",
    "# --- 実運用（学習/推論向け）：PyTorchテンソルでの出力 ---\n",
    "batch_pt = tokenizer(\n",
    "    text_list,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",  # ← B×L のテンソル（動的にLが決まる）\n",
    ")\n",
    "print(\n",
    "    \"\\n# tensor shapes:\",\n",
    "    \"input_ids\",\n",
    "    tuple(batch_pt[\"input_ids\"].shape),\n",
    "    \"attention_mask\",\n",
    "    tuple(batch_pt[\"attention_mask\"].shape),\n",
    "    \"token_type_ids\",\n",
    "    tuple(batch_pt[\"token_type_ids\"].shape),\n",
    ")\n",
    "\n",
    "# 参考：固定長（max_length）との比較（過剰Padを可視化）\n",
    "batch_fixed = tokenizer(\n",
    "    text_list,\n",
    "    max_length=24,  # 仮に固定長を大きめに設定\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(\n",
    "    \"\\n# fixed-length tensor shapes:\",\n",
    "    \"input_ids\",\n",
    "    tuple(batch_fixed[\"input_ids\"].shape),\n",
    "    \"attention_mask\",\n",
    "    tuple(batch_fixed[\"attention_mask\"].shape),\n",
    ")\n",
    "print(\"# fixed non-PAD per sample:\", batch_fixed[\"attention_mask\"].sum(dim=1).tolist())\n",
    "# 理論補足：\n",
    "# - longest は「そのバッチの最長」に合わせるため、バッチが変われば L も変わる（=動的）。\n",
    "# - max_length は常に同じ L に張り付ける（=静的）。スループット安定の代わりに無駄Padが乗りやすい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "568bdf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# keys: ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "# input_ids: shape=(2, 10), dtype=torch.int64, device=cpu\n",
      "# token_type_ids: shape=(2, 10), dtype=torch.int64, device=cpu\n",
      "# attention_mask: shape=(2, 10), dtype=torch.int64, device=cpu\n",
      "\n",
      "# sample[0]\n",
      "tokens: ['[CLS]', '明日', 'の', '天気', 'は', '晴れ', 'だ', '。', '[SEP]', '[PAD]']\n",
      "effective_len(non-PAD)= 9 / total L= 10\n",
      "CLS at 0 / SEP at 8\n",
      "PAD positions: [9]\n",
      "\n",
      "# sample[1]\n",
      "tokens: ['[CLS]', 'パソコン', 'が', '急', 'に', '動か', 'なく', 'なっ', 'た', '[SEP]']\n",
      "effective_len(non-PAD)= 10 / total L= 10\n",
      "CLS at 0 / SEP at 9\n",
      "PAD positions: []\n",
      "\n",
      "# decode(sample[0]): 明日 の 天気 は 晴れ だ 。\n",
      "\n",
      "# decode(sample[1]): パソコン が 急 に 動か なく なっ た\n"
     ]
    }
   ],
   "source": [
    "# 4-13\n",
    "# =========================================================\n",
    "# 理論メモ：PyTorchテンソル（B×L）でのバッチ前処理と不変量\n",
    "# ---------------------------------------------------------\n",
    "# ■ 目的\n",
    "#   - 下流タスクにそのまま渡せるテンソル（`input_ids`, `attention_mask`, `token_type_ids`）を\n",
    "#     B×L 形状で得る。ここでは L=10（固定長）・右側切り詰め＋右詰めPad。\n",
    "#\n",
    "# ■ 生成物と役割\n",
    "#   - input_ids      : 語彙ID列（[CLS]/[SEP]/[PAD]を含む）  → 形状 [B, L]\n",
    "#   - attention_mask : 非PAD=1, PAD=0。Self-Attention の softmax(QK^T/√d_k) 前に\n",
    "#                     PAD位置へ -∞ 相当を加算し注意を抑制（勾配も基本流れない） → 形状 [B, L]\n",
    "#   - token_type_ids : 文A=0, 文B=1。単文は通常ゼロ列（NSP/QAなどの文対で利用） → 形状 [B, L]\n",
    "#\n",
    "# ■ 切り詰めと特殊トークン配置（単文）\n",
    "#   - truncation=True → 右側（末尾側）から削る（first sequence を短縮）。\n",
    "#   - 不変量：index=0 に [CLS]，index=(有効長-1) に [SEP] が来る（有効長 = attention_mask の合計）。\n",
    "#\n",
    "# ■ 実務設計（計算量 O(L^2 H) への配慮）\n",
    "#   - 固定長（max_length）でスループットは安定。ただし過剰Padで無駄計算が増えやすい。\n",
    "#   - 動的パディング（padding='longest'）は各バッチの最長に合わせるため計算効率が良い。\n",
    "# =========================================================\n",
    "\n",
    "# 保険：tokenizer 未定義なら初期化（WWM版日本語BERTの語彙と対応）\n",
    "try:\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    from transformers import BertJapaneseTokenizer\n",
    "\n",
    "    model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 保険：text_list 未定義なら用意（単文のミニバッチ）\n",
    "try:\n",
    "    text_list\n",
    "except NameError:\n",
    "    text_list = [\"明日の天気は晴れだ。\", \"パソコンが急に動かなくなった。\"]\n",
    "\n",
    "# --- エンコード：B×L テンソルを直接得る（実務の標準パス） ---\n",
    "import torch\n",
    "\n",
    "batch_pt = tokenizer(\n",
    "    text_list,\n",
    "    max_length=10,  # L=10 に固定\n",
    "    padding=\"max_length\",  # 右詰めPadで長さを揃える\n",
    "    truncation=True,  # 右側切り詰め（単文）\n",
    "    return_tensors=\"pt\",  # ← PyTorch テンソル（B×L）で返す\n",
    ")\n",
    "\n",
    "# --- 出力の基本確認 ---\n",
    "print(\"# keys:\", list(batch_pt.keys()))\n",
    "for k, v in batch_pt.items():\n",
    "    print(f\"# {k}: shape={tuple(v.shape)}, dtype={v.dtype}, device={v.device}\")\n",
    "\n",
    "# --- 不変量チェック（理論に基づく簡易検証） ---\n",
    "CLS_ID = tokenizer.cls_token_id\n",
    "SEP_ID = tokenizer.sep_token_id\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "def check_invariants(i: int):\n",
    "    ids = batch_pt[\"input_ids\"][i].tolist()\n",
    "    mask = batch_pt[\"attention_mask\"][i].tolist()\n",
    "    toks = tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "    eff_len = sum(mask)  # 非PAD長（=有効長）\n",
    "    cls_pos = ids.index(CLS_ID) if CLS_ID in ids else None\n",
    "    sep_pos = ids.index(SEP_ID) if SEP_ID in ids else None\n",
    "\n",
    "    print(f\"\\n# sample[{i}]\")\n",
    "    print(\"tokens:\", toks)\n",
    "    print(\"effective_len(non-PAD)=\", eff_len, \"/ total L=\", len(ids))\n",
    "    print(\"CLS at\", cls_pos, \"/ SEP at\", sep_pos)\n",
    "    print(\"PAD positions:\", [j for j, t in enumerate(ids) if t == PAD_ID])\n",
    "\n",
    "    # 理論的不変量の検証（単文前提）\n",
    "    assert cls_pos == 0, \"CLS は index=0 のはず\"\n",
    "    assert sep_pos == eff_len - 1, \"SEP は有効長-1 のはず（単文）\"\n",
    "    # 単文なら token_type_ids は全0\n",
    "    assert torch.all(\n",
    "        batch_pt[\"token_type_ids\"][i].eq(0)\n",
    "    ), \"単文の token_type_ids は 0 のはず\"\n",
    "\n",
    "\n",
    "for i in range(len(text_list)):\n",
    "    check_invariants(i)\n",
    "\n",
    "# --- 可読表示（人間向け）。可逆ではない点に注意（空白/正規化/未知語の影響）。---\n",
    "for i in range(len(text_list)):\n",
    "    s = tokenizer.decode(batch_pt[\"input_ids\"][i], skip_special_tokens=True)\n",
    "    print(f\"\\n# decode(sample[{i}]):\", s)\n",
    "\n",
    "# 参考：\n",
    "# - 下流（分類/トークン分類/QA）では、この batch_pt をそのまま model(**batch_pt) に渡す。\n",
    "# - 勾配は attention_mask=1 の位置から流れる。PAD=0 の位置は Self-Attention で抑制されるため学習に寄与しない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03359740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using device = mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636bc7ec3bba4eaaa9c93fcc66761ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state: (1, 13, 768)\n",
      "pooler_output   : (1, 768)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bbc9b9486c4522bc35cd212cb64eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4-14（Mac対応版）\n",
    "# =========================================================\n",
    "# 理論メモ：MacBookにおけるPyTorchデバイス選択\n",
    "# ---------------------------------------------------------\n",
    "# ■ 背景\n",
    "#  - macOS（Apple Silicon搭載のMacBook）では CUDA は利用できない。\n",
    "#  - 代替として PyTorch は Metal（Apple製GPUバックエンド）＝ MPS を提供。\n",
    "#    → `torch.backends.mps.is_available()` が True なら GPU 相当の計算を MPS で実行可能。\n",
    "#  - Intel Mac などで MPS が使えない場合は CPU フォールバックが必要。\n",
    "#\n",
    "# ■ デバイス選択の方針\n",
    "#   1) MPS が使えれば 'mps' を使用（Apple SiliconのGPUを活用）\n",
    "#   2) それ以外で CUDA が使える環境なら 'cuda'（eGPU や別環境への移植時の保険）\n",
    "#   3) どちらも不可なら 'cpu'\n",
    "#\n",
    "# ■ 実務的含意\n",
    "#  - Self-Attention のアルゴリズム（softmax(QK^T/√d_k) V）はデバイスに依存しない。\n",
    "#    デバイスは「同じ計算をどこで行うか」を決めるだけで、理論上のモデルは不変。\n",
    "#  - MPS の半精度（fp16/bfloat16）は演算未対応のオペがある場合があり、基本は fp32 を推奨。\n",
    "#  - テンソルもモデルも **同じ device** に移動するのが原則（混在はエラー）。\n",
    "# =========================================================\n",
    "\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "\n",
    "\n",
    "# --- 1) デバイス検出（Mac優先でMPSを試みる） ---\n",
    "def get_best_device() -> torch.device:\n",
    "    # Apple Silicon + macOS 12.3+ で有効\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    # （保険）他環境へ移植されたときのために CUDA も許容\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"[info] using device = {device}\")\n",
    "\n",
    "# --- 2) モデルとトークナイザのロード ---\n",
    "model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# BertModel: 事前学習済みエンコーダ本体（タスク固有ヘッドは無し）\n",
    "bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# --- 3) モデルをデバイスへ配置 ---\n",
    "# 以前の `.cuda()` は Mac ではエラー（CUDA非対応）→ `.to(device)` に統一\n",
    "bert = bert.to(device)\n",
    "bert.eval()  # 推論時は dropout 等を停止\n",
    "\n",
    "# --- 4) サンプル前処理：テンソルも同じ device へ ---\n",
    "text = \"明日は自然言語処理の勉強をしよう。\"\n",
    "batch = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=False,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    ")\n",
    "\n",
    "# テンソルをモデルと同じデバイスへ\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "# --- 5) 前向き計算（推論） ---\n",
    "with torch.no_grad():\n",
    "    outputs = bert(**batch, output_hidden_states=False, output_attentions=False)\n",
    "\n",
    "# 形状確認：last_hidden_state = [B, L, H], pooler_output = [B, H]\n",
    "print(\"last_hidden_state:\", tuple(outputs.last_hidden_state.shape))\n",
    "if hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n",
    "    print(\"pooler_output   :\", tuple(outputs.pooler_output.shape))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 補足（理論と実務のTips）\n",
    "# - MPSはApple GPU向けバックエンド。計算量は依然 O(L^2 H) のため、mac上でも max_length 設計で\n",
    "#   速度・メモリを管理する（長文タスクではスライディングや動的パディングが有効）。\n",
    "# - 学習時：mixed precisionはMPSで未対応opがある場合があるため、まずは fp32 で安定化→段階的に検討。\n",
    "# - 乱数再現：`torch.manual_seed(...)` を設定し、バージョン・辞書・語彙を固定（日本語は形態素辞書差に注意）。\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74d232a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] device = mps\n",
      "[info] last_hidden_state shape = torch.Size([2, 32, 768])\n",
      "[sample 0] effective_len=13, CLS@0, SEP@12\n",
      "[sample 1] effective_len=14, CLS@0, SEP@13\n"
     ]
    }
   ],
   "source": [
    "# 4-16（Mac対応・理論コメント付き）\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#  - 2文を日本語BERT（WWM版）へ入力し、最終層の隠れ状態（last_hidden_state: [B, L, H]）を取得する。\n",
    "#  - MacBook（Apple Silicon想定）では CUDA ではなく Metal(MPS) を優先的に用い、テンソル/モデルを同一デバイスへ移動。\n",
    "#\n",
    "# 理論メモ（重要ポイント）：\n",
    "#  1) 前処理（tokenizer）\n",
    "#     - 日本語は「形態素解析 → WordPiece」の二段でサブワード化される。\n",
    "#       同じ文でも辞書・語彙・バージョンで分割境界が揺れる（＝ID列は固定ではない）。\n",
    "#     - 特殊トークン：[CLS]（先頭）, [SEP]（文末／区切り）。バッチ整列では [PAD] が追加される。\n",
    "#     - attention_mask=0 の位置（PAD）は Self-Attention の softmax 前に -∞ 相当を加算し注意を抑制 → 学習・推論に寄与しない。\n",
    "#\n",
    "#  2) BERTエンコーダ（BertModel）\n",
    "#     - 出力：last_hidden_state[B,L,H]（各トークンの文脈化表現）、pooler_output[B,H]（[CLS]に線形+Tanh）\n",
    "#     - Self-Attention：softmax(QK^T / √d_k) V。系列長 L に対して計算量 O(L^2 H)、メモリも概ね O(L^2)。\n",
    "#       → max_length の設計・パディング方針（'longest' 動的パディング等）がスループットとメモリに直結。\n",
    "#\n",
    "#  3) Macでのデバイス選択\n",
    "#     - Apple Silicon + macOS では CUDA は使えない → torch.backends.mps（Metal）が True なら 'mps' を使用。\n",
    "#     - モデルとテンソルは必ず同じ device に乗せる（不一致は RuntimeError の主要因）。\n",
    "#\n",
    "#  4) テキスト差分とトークナイズ\n",
    "#     - 「マシーンラーニング」 vs 「マシンラーニング」の表記差は形態素・サブワード境界に影響し得る。\n",
    "#       WWM（Whole Word Masking）は“事前学習時の語単位マスク戦略”であり、推論時の分割表示自体は変わらない。\n",
    "# =========================================================\n",
    "\n",
    "from typing import Dict\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "\n",
    "\n",
    "# --- デバイス検出（Mac優先：MPS → CUDA → CPU） ---\n",
    "def get_best_device() -> torch.device:\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")  # Apple GPU (Metal)\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")  # 他環境へ移植時の保険\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"[info] device = {device}\")\n",
    "\n",
    "# --- モデル & トークナイザ（WWM版とペアで統一） ---\n",
    "model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "try:\n",
    "    tokenizer  # 既存セッションにあれば再利用\n",
    "except NameError:\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "try:\n",
    "    bert  # 既存セッションにあれば再利用\n",
    "    # 既存モデルが別deviceなら移し替える\n",
    "    bert = bert.to(device)\n",
    "except NameError:\n",
    "    bert = BertModel.from_pretrained(model_name).to(device)\n",
    "bert.eval()  # 推論モード（dropout停止）; 学習なら train() に切替\n",
    "\n",
    "# --- 入力文（B=2）：表記差によりサブワード分割が変わる可能性に留意 ---\n",
    "text_list = [\n",
    "    \"明日は自然言語処理の勉強をしよう。\",\n",
    "    \"明日はマシーンラーニングの勉強をしよう。\",\n",
    "]\n",
    "\n",
    "# --- 文章の符号化：固定長（L=32）; PADはattention_mask=0で無視される ---\n",
    "# 返り値（return_tensors='pt'）：\n",
    "#   input_ids[B,L], attention_mask[B,L], token_type_ids[B,L] （単文は原則0列）\n",
    "encoding: Dict[str, torch.Tensor] = tokenizer(\n",
    "    text_list, max_length=32, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# --- データをモデルと同じデバイスへ（Macでは 'mps' が想定；.cuda() は使用しない） ---\n",
    "encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "# --- 前向き計算（推論）。学習で勾配が要らなければ no_grad で省メモリ・高速化 ---\n",
    "with torch.no_grad():\n",
    "    # 出力：BaseModelOutputWithPoolingAndCrossAttentions\n",
    "    output = bert(**encoding, output_hidden_states=False, output_attentions=False)\n",
    "\n",
    "# --- 最終層の出力（各トークンの文脈化表現）：形状 [B, L, H] ---\n",
    "last_hidden_state: torch.Tensor = output.last_hidden_state\n",
    "\n",
    "# --- 参考：shapeと不変量（単文）を軽く確認 ---\n",
    "B, L, H = last_hidden_state.shape\n",
    "cls_id = tokenizer.cls_token_id\n",
    "sep_id = tokenizer.sep_token_id\n",
    "pad_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\n",
    "    f\"[info] last_hidden_state shape = {last_hidden_state.shape}\"\n",
    ")  # 例：torch.Size([2, 32, 768])\n",
    "for i in range(B):\n",
    "    ids = encoding[\"input_ids\"][i].tolist()\n",
    "    mask = encoding[\"attention_mask\"][i].tolist()\n",
    "    eff_len = sum(mask)  # 非PAD長（=有効長）\n",
    "    cls_pos = ids.index(cls_id) if cls_id in ids else None\n",
    "    sep_pos = ids.index(sep_id) if sep_id in ids else None\n",
    "    print(f\"[sample {i}] effective_len={eff_len}, CLS@{cls_pos}, SEP@{sep_pos}\")\n",
    "\n",
    "    # 不変量チェック（単文前提）：CLSは先頭、SEPは有効長-1\n",
    "    assert cls_pos == 0, \"CLS は index=0 のはず（単文）\"\n",
    "    assert sep_pos == eff_len - 1, \"SEP は有効長-1 のはず（単文）\"\n",
    "\n",
    "# --- 補足：下流タスク接続例 ---\n",
    "# 文分類：vec = output.pooler_output もしくは last_hidden_state[:, 0, :] を線形層へ\n",
    "# トークン分類：last_hidden_state を各位置で線形→softmax（PAD位置は mask で損失除外 or ignore_index）\n",
    "# QA：質問・文脈ペア（[CLS] Q [SEP] C [SEP]）で start/end ロジットを回帰（長文はstride+overlapで分割）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa39bfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] device = mps\n",
      "last_hidden_state shape: (2, 32, 768)\n",
      "pooler_output    shape: (2, 768)\n",
      "[sample 0] effective_len=13, CLS@0, SEP@12\n",
      "[sample 1] effective_len=14, CLS@0, SEP@13\n"
     ]
    }
   ],
   "source": [
    "# 4-17（Mac対応・理論コメント込み）\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#  - BERTエンコーダ（BertModel）へ「input_ids / attention_mask / token_type_ids」を明示的に渡し、\n",
    "#    出力（last_hidden_state, pooler_output など）を取得する。\n",
    "#\n",
    "# 理論メモ（引数の意味と学習への影響）：\n",
    "#  - input_ids        : サブワードの語彙ID列（[CLS]/[SEP]/[PAD] 含む）。形状 [B, L]\n",
    "#  - attention_mask   : 非PAD=1, PAD=0。Self-Attention の softmax(QK^T/√d_k) 前に\n",
    "#                       PAD位置へ -∞ 相当を加算して注意を抑制 → PADは表現学習と勾配から実質除外。\n",
    "#  - token_type_ids   : 文A=0, 文B=1。単文は原則0列（NSP/QAなどの文対で区別）。\n",
    "#  - 出力:\n",
    "#     * last_hidden_state [B, L, H]：各トークン位置の文脈化表現\n",
    "#     * pooler_output    [B, H]    ：[CLS]に線形＋tanh（文分類の初期ベースライン）\n",
    "#  - 計算量：Self-Attention は O(L^2 H)。max_length とパディング設計（'longest'動的パディング等）が\n",
    "#    速度・メモリに直結する。デバイス（CPU/MPS/CUDA）は計算場所の違いであり理論は不変。\n",
    "# =========================================================\n",
    "\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "\n",
    "\n",
    "# --- デバイス（Mac優先：MPS → CUDA → CPU） ---\n",
    "def get_best_device() -> torch.device:\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")  # Apple GPU (Metal)\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"[info] device = {device}\")\n",
    "\n",
    "# --- モデル/トークナイザの用意（既存が無ければ初期化） ---\n",
    "model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "try:\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "try:\n",
    "    bert\n",
    "    bert = bert.to(device)\n",
    "except NameError:\n",
    "    bert = BertModel.from_pretrained(model_name).to(device)\n",
    "bert.eval()  # 推論モード（学習時は train()）\n",
    "\n",
    "# --- エンコーディング準備（前セルの encoding が無ければ作る） ---\n",
    "try:\n",
    "    encoding  # 4-16由来の BatchEncoding / dict[str, Tensor] を想定\n",
    "except NameError:\n",
    "    text_list: List[str] = [\n",
    "        \"明日は自然言語処理の勉強をしよう。\",\n",
    "        \"明日はマシーンラーニングの勉強をしよう。\",\n",
    "    ]\n",
    "    encoding = tokenizer(\n",
    "        text_list,\n",
    "        max_length=32,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "# テンソルをモデルと同じデバイスへ（Macでは 'mps' が想定；.cuda() は使わない）\n",
    "encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "# --- 前向き計算（勾配不要なら no_grad で省メモリ・高速化） ---\n",
    "with torch.no_grad():\n",
    "    output = bert(\n",
    "        input_ids=encoding[\"input_ids\"],\n",
    "        attention_mask=encoding[\"attention_mask\"],\n",
    "        token_type_ids=encoding[\"token_type_ids\"],\n",
    "    )  # BaseModelOutputWithPoolingAndCrossAttentions\n",
    "\n",
    "# --- 出力の取り出し ---\n",
    "last_hidden_state: torch.Tensor = output.last_hidden_state  # [B, L, H]\n",
    "pooler_output: torch.Tensor = (\n",
    "    output.pooler_output\n",
    ")  # [B, H]（存在しない設定もある点に注意）\n",
    "\n",
    "print(\"last_hidden_state shape:\", tuple(last_hidden_state.shape))\n",
    "print(\n",
    "    \"pooler_output    shape:\",\n",
    "    tuple(pooler_output.shape) if pooler_output is not None else None,\n",
    ")\n",
    "\n",
    "# --- 不変量の軽い検証（単文想定）：CLSは index=0、SEP は有効長-1 ---\n",
    "CLS_ID = tokenizer.cls_token_id\n",
    "SEP_ID = tokenizer.sep_token_id\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "for i in range(encoding[\"input_ids\"].size(0)):\n",
    "    ids = encoding[\"input_ids\"][i].tolist()\n",
    "    mask = encoding[\"attention_mask\"][i].tolist()\n",
    "    eff_len = sum(mask)\n",
    "    cls_pos = ids.index(CLS_ID) if CLS_ID in ids else None\n",
    "    sep_pos = ids.index(SEP_ID) if SEP_ID in ids else None\n",
    "    print(f\"[sample {i}] effective_len={eff_len}, CLS@{cls_pos}, SEP@{sep_pos}\")\n",
    "    assert cls_pos == 0, \"CLS は index=0 のはず（単文）\"\n",
    "    assert sep_pos == eff_len - 1, \"SEP は有効長-1 のはず（単文）\"\n",
    "\n",
    "# --- 参考：下流タスク接続（理論観点） ---\n",
    "# 文分類：vec = pooler_output（または last_hidden_state[:,0,:]）→ Linear(num_labels)\n",
    "# トークン分類：last_hidden_state を位置ごとに Linear(num_tags) → CrossEntropy（PADは ignore_index または mask で除外）\n",
    "# QA：ペア入力（[CLS] Q [SEP] C [SEP]）で start/end ロジットを回帰。長文は stride+overlap で分割・統合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9fbadbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 768])\n"
     ]
    }
   ],
   "source": [
    "# 4-18\n",
    "print(last_hidden_state.size())  # テンソルのサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ac1436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ids] CLS=2, SEP=3, PAD=0, CLS_tok=[CLS], SEP_tok=[SEP]\n",
      "[sample 0] eff_len=13, CLS@0, SEP@12\n",
      "tokens: ['[CLS]', '明日', 'は', '自然', '言語', '処理', 'の', '勉強', 'を', 'しよ', '##う', '。', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[sample 1] eff_len=14, CLS@0, SEP@13\n",
      "tokens: ['[CLS]', '明日', 'は', 'マ', '##シーン', 'ラー', '##ニング', 'の', '勉強', 'を', 'しよ', '##う', '。', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# エラー原因と修正案（AssertionError: CLS は index=0 のはず）\n",
    "# =========================================================\n",
    "# ■ なぜ失敗したか\n",
    "# - チェック用の ID を `bert.config.cls_token_id / sep_token_id` から読んでいましたが、\n",
    "#   これは None のことがあり（モデル設定に未保存）、あるいは英語BERTの既定値（101/102）と\n",
    "#   食い違う可能性があります。\n",
    "# - 東北大日本語BERTでは一般に [CLS]=2, [SEP]=3, [PAD]=0 です。よって tokenizer から ID を取得すべきです。\n",
    "#\n",
    "# ■ 対処方針\n",
    "# - `tokenizer.cls_token_id / sep_token_id / pad_token_id` を使う。\n",
    "# - ついでにトークン列で直接確認する安全版（文字列 \"[CLS]\"/\"[SEP]\" を探す）も示します。\n",
    "# - もし `encoding` を作る際に `add_special_tokens=False` を明示していた場合、[CLS]/[SEP] は付きません。\n",
    "#   （通常は既定 True なので付いています）\n",
    "\n",
    "# --- 修正済みの不変量チェック（単文前提） ---\n",
    "CLS_ID = tokenizer.cls_token_id\n",
    "SEP_ID = tokenizer.sep_token_id\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "print(\n",
    "    f\"[ids] CLS={CLS_ID}, SEP={SEP_ID}, PAD={PAD_ID}, CLS_tok={tokenizer.cls_token}, SEP_tok={tokenizer.sep_token}\"\n",
    ")\n",
    "\n",
    "for i in range(encoding[\"input_ids\"].size(0)):\n",
    "    ids = encoding[\"input_ids\"][i].tolist()\n",
    "    mask = encoding[\"attention_mask\"][i].tolist()\n",
    "    toks = tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "    eff_len = sum(mask)  # 非PAD長（=有効長）\n",
    "    cls_pos = ids.index(CLS_ID) if CLS_ID in ids else None\n",
    "    sep_pos = ids.index(SEP_ID) if SEP_ID in ids else None\n",
    "\n",
    "    print(f\"[sample {i}] eff_len={eff_len}, CLS@{cls_pos}, SEP@{sep_pos}\")\n",
    "    print(\"tokens:\", toks)\n",
    "\n",
    "    # ① IDベースの不変量（最速）\n",
    "    assert cls_pos == 0, \"CLS は index=0（先頭）のはず（単文）\"\n",
    "    assert sep_pos == eff_len - 1, \"SEP は有効長-1（末尾）のはず（単文）\"\n",
    "\n",
    "    # ② トークン文字列ベースの保険（より頑健）\n",
    "    #    ※ tokenizer の特別トークン文字列を使って位置を検出\n",
    "    try:\n",
    "        cls_pos_tok = toks.index(tokenizer.cls_token)\n",
    "        sep_pos_tok = toks.index(tokenizer.sep_token)\n",
    "        assert cls_pos_tok == 0, \"トークン '[CLS]' は先頭のはず（単文）\"\n",
    "        assert sep_pos_tok == eff_len - 1, \"トークン '[SEP]' は有効長-1 のはず（単文）\"\n",
    "    except ValueError:\n",
    "        # [CLS]/[SEP] が見つからない場合は、add_special_tokens=False で符号化していないか確認する\n",
    "        raise AssertionError(\n",
    "            \"特殊トークンが見つかりません。encoding を作る際に add_special_tokens=True（既定）を使っているか確認してください。\"\n",
    "        )\n",
    "\n",
    "# --- 参考：encoding を作る際の推奨（既定で特殊トークン付与されます） ---\n",
    "# encoding = tokenizer(\n",
    "#     text_list,\n",
    "#     max_length=32,\n",
    "#     padding='max_length',\n",
    "#     truncation=True,\n",
    "#     return_tensors='pt'     # add_special_tokens は既定 True（[CLS]/[SEP] 付与）\n",
    "# )\n",
    "# encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "# --- 注意：ペア入力（質問+文脈など）の場合 ---\n",
    "# [CLS] A [SEP] B [SEP] の形になるため、SEP は2つ現れる。単文用の不変量アサートはそのまま使えません。\n",
    "# その場合は token_type_ids（A=0, B=1）で区間を分けて確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0833ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state (torch) shape: (2, 32, 768) dtype: torch.float32 device: mps:0\n",
      "last_hidden_state (numpy) shape: (2, 32, 768) dtype: float32\n",
      "numpy nbytes: 196,608 bytes ≈ 0.19 MiB\n",
      "converted to Python list (length, depth): 2 32\n"
     ]
    }
   ],
   "source": [
    "# 4-20（テンソル→CPU→NumPy→Pythonリストへの変換：理論コメント付き）\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#  - BERTの出力 last_hidden_state（形状 [B, L, H] の torch.Tensor）を\n",
    "#    推論後の解析やシリアライズ（JSON, CSVなど）で扱いやすい形式に落とす。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - PyTorch Tensor → NumPy は **同一メモリを共有**（コピー無し）が基本だが、\n",
    "#    共有できるのは **CPU上の連続（contiguous）浮動小数テンソル**のみ。\n",
    "#    GPU(CUDA/MPS) 上の Tensor は **いったん .to('cpu')** でホスト側へ移す必要がある。\n",
    "#  - 推論時でもグラフが付いた Tensor を直接 .numpy() するのは非推奨。\n",
    "#    **.detach()** で計算グラフから切り離してから変換するのが安全。\n",
    "#  - リスト化（.tolist()）は **可搬性は高いが非常にメモリと時間を消費**する。\n",
    "#    大規模行列はできる限り NumPy のまま保存（.npy / .npz）や圧縮型（float16等）を検討。\n",
    "#\n",
    "# 実装（安全版の手順）：\n",
    "#  1) detach：勾配追跡を外す（推論なら原則不要だが安全策）\n",
    "#  2) to('cpu')：GPU/MPS から CPU へ移動（NumPyはCPU配列のみ）\n",
    "#  3) contiguous：必要に応じて連続化（ストライドが特殊な場合の保険）\n",
    "#  4) numpy()：NumPy ndarray へ変換\n",
    "#  5) tolist()：最終的にPythonリストへ（本当に必要な場合のみ）\n",
    "# =========================================================\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# （前提）last_hidden_state: torch.Tensor [B, L, H]\n",
    "# 例：torch.Size([2, 32, 768]) など\n",
    "print(\n",
    "    \"last_hidden_state (torch) shape:\",\n",
    "    tuple(last_hidden_state.shape),\n",
    "    \"dtype:\",\n",
    "    last_hidden_state.dtype,\n",
    "    \"device:\",\n",
    "    last_hidden_state.device,\n",
    ")\n",
    "\n",
    "# --- 推奨：安全な変換パス ---\n",
    "lhs_cpu = last_hidden_state.detach().to(\"cpu\")  # 1) detach + 2) CPU へ移動\n",
    "lhs_cpu = lhs_cpu.contiguous()  # 3) 必要に応じて連続化（保険）\n",
    "lhs_np = lhs_cpu.numpy()  # 4) NumPy ndarray へ（ゼロコピー想定）\n",
    "print(\"last_hidden_state (numpy) shape:\", lhs_np.shape, \"dtype:\", lhs_np.dtype)\n",
    "\n",
    "# --- メモリ見積り（実務上の注意喚起） ---\n",
    "# bytes = B * L * H * sizeof(dtype)。float32なら 4 バイト\n",
    "bytes_total = lhs_np.nbytes\n",
    "print(f\"numpy nbytes: {bytes_total:,} bytes ≈ {bytes_total/1024/1024:.2f} MiB\")\n",
    "\n",
    "# --- 省メモリオプション（必要なときのみ） ---\n",
    "# ※ 数値誤差を許容できるなら、float16 / bfloat16 などへ圧縮して保存する。\n",
    "# lhs_np16 = lhs_np.astype(np.float16)  # 約半分のサイズ\n",
    "# np.save('last_hidden_state_fp16.npy', lhs_np16)\n",
    "\n",
    "# --- JSON等に出すなど「純Python」形式が必要な場合のみリスト化 ---\n",
    "#    （巨大行列での tolist() は非常に重いので、本当に必要なときに限定）\n",
    "lhs_list = lhs_np.tolist()\n",
    "print(\n",
    "    \"converted to Python list (length, depth):\",\n",
    "    len(lhs_list),\n",
    "    len(lhs_list[0]) if len(lhs_list) > 0 else 0,\n",
    ")\n",
    "\n",
    "# --- 参考：よくある落とし穴と対処 ---\n",
    "# 1) GPU/MPS テンソルで .numpy() → エラー：必ず .to('cpu') してから\n",
    "# 2) 勾配付きテンソルで .numpy() → 警告/エラー：.detach() を挟む\n",
    "# 3) 速度/メモリ問題 → できるだけ NumPy のまま保存（.npy/.npz）や圧縮（float16）を検討\n",
    "# 4) JSONシリアライズ → リスト化は可読だが巨大。集約（平均/プーリング）して次元を落としてからの出力が実務的"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
