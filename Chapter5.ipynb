{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f5ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-2\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec9c8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using device = mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=32000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-3\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#  - 日本語BERT（Whole Word Masking 版）を「MLM（Masked Language Modeling）」タスク用の\n",
    "#    モデルとしてロードし、MacBook を含む環境で最適なデバイス（MPS/CUDA/CPU）に載せる。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - MLM（Masked Language Modeling）は、入力トークンの一部を [MASK] に置換し、\n",
    "#    その復元を学習する自己教師ありタスクである。BERTの事前学習の主要素。\n",
    "#  - Whole Word Masking（WWM）は「サブワード単位ではなく“語”単位でまとめてマスクする」\n",
    "#    戦略。語彙断片ではなく語レベルの完形復元圧力を与えるため、語彙的一貫性の向上が期待される。\n",
    "#  - 推論（補完）時は [MASK] の位置の語彙分布（logits）から上位候補を選ぶ。\n",
    "#    学習時は [MASK] 位置以外のロスを無視（ignore_index=-100）するのが一般的（DataCollatorが自動対応可）。\n",
    "#  - 日本語BERT（BertJapaneseTokenizer）は「形態素解析→WordPiece」の二段で分割する。\n",
    "#    分割は辞書・語彙・バージョンに依存するため、再現性は“性質（不変量）”で管理する。\n",
    "#\n",
    "# 実務Tips：\n",
    "#  - MacBook（Apple Silicon）では CUDA は使えないため、Metal(MPS) を優先する。\n",
    "#  - 推論（補完）だけなら model.eval() ＋ torch.no_grad() でメモリ＆速度最適化。\n",
    "#  - 学習する場合は DataCollatorForLanguageModeling を用い、mlm_probability を設定する。\n",
    "#  - トークナイザとモデルは必ず同じモデル名（語彙）を使う（IDずれ防止）。\n",
    "# =========================================================\n",
    "\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "\n",
    "\n",
    "# --- デバイス検出（Mac優先：MPS → CUDA → CPU） ---\n",
    "def get_best_device() -> torch.device:\n",
    "    # Apple Silicon + macOS（MPSバックエンド）\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    # 他環境に移植した場合に備えて CUDA も許容\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # どれも不可なら CPU\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"[info] using device = {device}\")\n",
    "\n",
    "# --- モデル名（WWM版日本語BERT）：トークナイザとモデルはペアで統一 ---\n",
    "model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "\n",
    "# トークナイザ（形態素解析→WordPiece）。特殊トークン：[CLS]/[SEP]/[PAD]/[MASK]/[UNK]\n",
    "# - `mask_token` と `mask_token_id` は MLM の置換に使用する。\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# MLM 用のヘッド付き BERT（語彙サイズ×隠れ次元の出力層を持つ）\n",
    "# - 出力：logits 形状 [B, L, |Vocab|]。各位置の語彙分布を返す。\n",
    "bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# モデルを最適デバイスへ移動（Mac では .cuda() ではなく .to(device) を使う）\n",
    "bert_mlm = bert_mlm.to(device)\n",
    "\n",
    "# 推論モード（補完タスクなど）：dropout を停止\n",
    "bert_mlm.eval()\n",
    "\n",
    "# --- 参考：MLM 推論の最小例（必要ならコメント解除して確認） ---\n",
    "# text = f\"明日は{tokenizer.mask_token}だ。\"\n",
    "# enc = tokenizer(text, return_tensors='pt')\n",
    "# enc = {k: v.to(device) for k, v in enc.items()}\n",
    "# with torch.no_grad():\n",
    "#     logits = bert_mlm(**enc).logits  # [1, L, V]\n",
    "# mask_index = (enc[\"input_ids\"][0] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0].item()\n",
    "# topk = torch.topk(logits[0, mask_index], k=5).indices.tolist()\n",
    "# candidates = [tokenizer.decode([i]) for i in topk]\n",
    "# print(\"Top-5 candidates at [MASK]:\", candidates)\n",
    "\n",
    "# --- 参考：学習時のスケッチ ---\n",
    "# from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer,\n",
    "#     mlm=True,\n",
    "#     mlm_probability=0.15  # 15% を [MASK]（WWM版モデルの事前学習と整合的に設定）\n",
    "# )\n",
    "# # Trainer(...) を用いて学習。PAD位置は自動でロスから除外される（ignore_index=-100）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65bf0913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens: ['今日', 'は', '[MASK]', 'へ', '行く', '。']\n",
      "# ids: [3246, 9, 4, 118, 3488, 8]\n",
      "# mask token: [MASK] mask id: 4 pos(tok): 2 pos(id): 2\n",
      "# input_ids: [[2, 3246, 9, 4, 118, 3488, 8, 3]]\n",
      "# attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "# tokens (with specials): ['[CLS]', '今日', 'は', '[MASK]', 'へ', '行く', '。', '[SEP]']\n",
      "# decode (skip specials): 今日 は へ 行く 。\n"
     ]
    }
   ],
   "source": [
    "# 5-4\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#  - 入力文に含まれる [MASK] を含めて、日本語BERT用トークナイザでサブワード列へ分割し、挙動を確認する。\n",
    "#\n",
    "# 理論メモ（重要ポイント）：\n",
    "#  - MLM（Masked Language Modeling）では、[MASK] 位置の語彙分布を予測する（学習時は [MASK] 以外の損失を無視）。\n",
    "#  - `BertJapaneseTokenizer` は「形態素解析 → WordPiece」の二段で分割する。分割境界は辞書・語彙・バージョンに依存。\n",
    "#  - `tokenize()` は **特殊トークン [CLS]/[SEP] を付けない**“素のサブワード列”を返す。\n",
    "#  - `[MASK]` は **特殊トークンとして保存**される想定（= 文字列 \"[MASK]\" がそのままトークン列に現れる）。\n",
    "#    ※ 安全のため、リテラル \"[MASK]\" 直書きより `tokenizer.mask_token` を使うのが堅牢。\n",
    "#  - `encode()` / `tokenizer(..., return_tensors='pt')` は **ID列**を返し、既定で [CLS]/[SEP] を付与して下流モデルに直結できる。\n",
    "# =========================================================\n",
    "\n",
    "# 保険：tokenizer 未定義なら初期化（WWM版日本語BERTとペアで統一）\n",
    "try:\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    from transformers import BertJapaneseTokenizer\n",
    "\n",
    "    model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# --- 入力文：安全のため tokenizer.mask_token を用いて [MASK] を埋め込む ---\n",
    "text = f\"今日は{tokenizer.mask_token}へ行く。\"\n",
    "\n",
    "# --- サブワード列へ分割（特殊トークン [CLS]/[SEP] は付かない）---\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"# tokens:\", tokens)\n",
    "\n",
    "# --- ID への写像（[MASK] が mask_token_id に対応することを確認）---\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"# ids:\", ids)\n",
    "\n",
    "mask_tok = tokenizer.mask_token  # 例：\"[MASK]\"\n",
    "mask_id = tokenizer.mask_token_id  # 例：mask の語彙ID\n",
    "mask_pos_tok = tokens.index(mask_tok) if mask_tok in tokens else None\n",
    "mask_pos_id = ids.index(mask_id) if mask_id in ids else None\n",
    "print(\n",
    "    \"# mask token:\",\n",
    "    mask_tok,\n",
    "    \"mask id:\",\n",
    "    mask_id,\n",
    "    \"pos(tok):\",\n",
    "    mask_pos_tok,\n",
    "    \"pos(id):\",\n",
    "    mask_pos_id,\n",
    ")\n",
    "\n",
    "# --- 参考：特殊トークン付与あり（モデル入力に近い形；[CLS] と [SEP] を含む）---\n",
    "enc = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "print(\"# input_ids:\", enc[\"input_ids\"].tolist())\n",
    "print(\"# attention_mask:\", enc[\"attention_mask\"].tolist())\n",
    "print(\"# tokens (with specials):\", tokenizer.convert_ids_to_tokens(enc[\"input_ids\"][0]))\n",
    "\n",
    "# --- 復元表示（可逆ではない場合がある点に注意：空白や正規化、未知語の影響）---\n",
    "print(\n",
    "    \"# decode (skip specials):\",\n",
    "    tokenizer.decode(enc[\"input_ids\"][0], skip_special_tokens=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920d8ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using device = mps\n"
     ]
    }
   ],
   "source": [
    "# 5-5\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#  - [MASK] を含む文 `text` を符号化し、MLM（BertForMaskedLM）へ入力して\n",
    "#    各トークン位置の語彙分布（logits）を得る。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - MLM（Masked Language Modeling）では [MASK] 位置での語彙分布 p(token | context) を予測する。\n",
    "#    モデル出力 `logits` は形状 [B, L, |V|]（バッチ×系列長×語彙サイズ）であり、\n",
    "#    「分類スコア」は“各トークン位置における語彙分類のスコア”を意味する。\n",
    "#  - `tokenizer.encode(text, return_tensors='pt')` は **ID列テンソル** を返す（既定で [CLS]/[SEP] 付与）。\n",
    "#    研究・実務では `tokenizer(text, return_tensors='pt', padding=..., truncation=...)` の使用が推奨だが、\n",
    "#    単文・短文であれば `encode` でも問題ない。\n",
    "#  - attention_mask を省略した場合、PAD を含まない単文では実害は基本的にない。\n",
    "#    ただしバッチやパディングを伴う運用では attention_mask を明示するのが安全。\n",
    "#  - デバイス：MacBook(Apple Silicon) では CUDA は使えない。Metal(MPS) を優先し、\n",
    "#    それも不可なら CPU を用いる。モデルとテンソルは**常に同じ device** に揃える。\n",
    "# =========================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# --- デバイス検出（Mac優先：MPS → CUDA → CPU） ---\n",
    "def get_best_device() -> torch.device:\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")  # Apple GPU (Metal)\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")  # 他環境への持ち出し用の保険\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"[info] using device = {device}\")\n",
    "\n",
    "# 文章を符号化し、GPU（またはMPS/CPU）に配置する。\n",
    "# - encode は既定で special tokens（[CLS]/[SEP]）を付与する。\n",
    "# - return_tensors='pt' により torch.Tensor（形状 [B=1, L]）で取得。\n",
    "input_ids = tokenizer.encode(\n",
    "    text, return_tensors=\"pt\"\n",
    ")  # 例：tensor([[CLS, ..., MASK, ..., SEP]])\n",
    "# 元コード： input_ids = input_ids.cuda()\n",
    "# Mac では .cuda() は不可。device に合わせて移動する：\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# 念のためモデル側も同じデバイスへ（5-3でto(device)済みなら実質ノーオペ）\n",
    "bert_mlm = bert_mlm.to(device)\n",
    "bert_mlm.eval()  # 推論のみ。学習なら train() に切替\n",
    "\n",
    "# BERTに入力し、語彙スコア（logits）を得る。\n",
    "# ※ 元コメントの「系列長を揃える必要がない」は“単文かつ短文で PAD を使わない”場合に限って概ね正しい。\n",
    "#    バッチや長文運用では padding / attention_mask を明示すること。\n",
    "with torch.no_grad():  # 推論では計算グラフを作らない：省メモリ・高速化\n",
    "    output = bert_mlm(\n",
    "        input_ids=input_ids\n",
    "    )  # 他に attention_mask 等を省略（単文短文の前提）\n",
    "    scores = output.logits  # 形状 [B, L, |V|]：各位置の語彙ロジット\n",
    "\n",
    "# --- 参考（任意）：[MASK] 位置の Top-k 候補を取得する手順（コメントアウト） ---\n",
    "# mask_id = tokenizer.mask_token_id\n",
    "# mask_pos = (input_ids[0] == mask_id).nonzero(as_tuple=True)[0].item()  # [MASK] の位置（L次元）\n",
    "# topk = torch.topk(scores[0, mask_pos], k=5).indices.tolist()\n",
    "# candidates = [tokenizer.decode([i]) for i in topk]  # 語彙ID → 文字列\n",
    "# print(\"Top-5 candidates at [MASK]:\", candidates)\n",
    "\n",
    "# --- 参考（理論補足）---\n",
    "# - Self-Attention：softmax(QK^T / √d_k) V。系列長 L に対して計算量 O(L^2 H)。\n",
    "#   長文・バッチ運用では L を適切に管理（truncation/stride/動的パディング）すること。\n",
    "# - PAD を含む場合は attention_mask=0 の位置が softmax 前に -∞ 相当で抑制され、注意が向かない（勾配も基本流れない）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f2c228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日は、へ行く。\n"
     ]
    }
   ],
   "source": [
    "# 5-6\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#  - MLM の出力 `scores`（形状 [B, L, |V|]）から [MASK] 位置の最尤トークン（argmax）を選び，\n",
    "#    元の文の [MASK] をそのトークンで置換して，人間可読な文に復元する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - `scores[0, pos]` は語彙分布（ロジット）。argmax は MAP 推定（最尤語彙）に対応する。\n",
    "#  - [MASK] の語彙IDを **固定値で仮定しない**（モデルによって異なる）。必ず `tokenizer.mask_token_id` を用いる。\n",
    "#  - サブワード（WordPiece）の場合，`convert_ids_to_tokens` は '##' 接頭辞が付くことがある。\n",
    "#    文字列復元としては `tokenizer.decode([id])` の方が自然なことが多い（空白処理や正規化を担う）。\n",
    "#  - 複数 [MASK] がある場合：ここでは「独立同時置換」（各位置で argmax）を行う。\n",
    "#    逐次的に再エンコードして条件付きで更新する「反復置換」は別戦略（より厳密だが計算増）。\n",
    "# =========================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "# --- 1) [MASK] の語彙IDを取得（モデル依存；固定値を使わない） ---\n",
    "mask_id = (\n",
    "    tokenizer.mask_token_id\n",
    ")  # 例：東北大日本語BERTでは 4 のことが多いが、必ず動的に取得する\n",
    "mask_tok = tokenizer.mask_token  # 例：\"[MASK]\"\n",
    "\n",
    "# --- 2) 入力ID列から [MASK] の位置を列挙（複数対応） ---\n",
    "# input_ids: 形状 [B=1, L] を想定（5-5の encode に一致）\n",
    "mask_positions = (input_ids[0] == mask_id).nonzero(as_tuple=True)[0].tolist()\n",
    "if len(mask_positions) == 0:\n",
    "    raise ValueError(\n",
    "        \"入力系列に [MASK] が見つかりません。text に tokenizer.mask_token を含めてください。\"\n",
    "    )\n",
    "\n",
    "# --- 3) 各 [MASK] 位置で最尤トークンIDを取得（argmax over vocab） ---\n",
    "# scores: 形状 [B=1, L, |V|] を想定（5-5の出力 logits）\n",
    "pred_token_ids = []\n",
    "for pos in mask_positions:\n",
    "    # scores[0, pos] : [|V|] ベクトル → 最尤語彙ID\n",
    "    best_id = scores[0, pos].argmax(-1).item()\n",
    "    pred_token_ids.append(best_id)\n",
    "\n",
    "# --- 4) ID → 文字列へ変換（decode を優先） ---\n",
    "# decode は空白や '##' の扱いをよしなに調整してくれる（単一IDでも有用）\n",
    "pred_tokens = [\n",
    "    tokenizer.decode([tid], skip_special_tokens=True).strip() for tid in pred_token_ids\n",
    "]\n",
    "\n",
    "# フォールバック（必要時）：WordPieceの『##』を除去して素片文字列だけを得る\n",
    "pred_tokens_fallback = [\n",
    "    tokenizer.convert_ids_to_tokens(tid).lstrip(\"##\") for tid in pred_token_ids\n",
    "]\n",
    "\n",
    "# decode 側で万が一空文字になった箇所はフォールバックを使う\n",
    "pred_tokens = [\n",
    "    pt if pt != \"\" else pf for pt, pf in zip(pred_tokens, pred_tokens_fallback)\n",
    "]\n",
    "\n",
    "# --- 5) 元のテキスト中の [MASK] を順に置換（複数 [MASK] に対応） ---\n",
    "# str.replace は全置換になるため、分割→挿入→連結で「順番に」置換する\n",
    "parts = text.split(mask_tok)  # [seg0, seg1, ..., segN]  （N = マスク数）\n",
    "filled_segments = []\n",
    "for i, seg in enumerate(parts[:-1]):\n",
    "    filled_segments.append(seg)\n",
    "    # i 番目の [MASK] に対する候補を挿入\n",
    "    filled_segments.append(pred_tokens[i] if i < len(pred_tokens) else \"\")\n",
    "filled_segments.append(parts[-1])\n",
    "\n",
    "text_filled = \"\".join(filled_segments)\n",
    "print(text_filled)\n",
    "\n",
    "# --- 備考 ---\n",
    "# - 「独立同時置換」では各 [MASK] を互いに独立に推定するため，相互依存（共起制約）を捉えにくい。\n",
    "#   文脈整合性を高めたい場合は，1つずつ置換→再エンコード→再推論の反復法を検討する。\n",
    "# - サブワードが選ばれた場合でも decode により自然な表記になりやすいが，\n",
    "#   場合によっては不自然な連結になる可能性がある（語境界／表記揺れのため）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2c32cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日は、へ行く。\n",
      "今日ははへ行く。\n",
      "今日はのへ行く。\n",
      "今日はにへ行く。\n",
      "今日はでへ行く。\n",
      "今日は「へ行く。\n",
      "今日はへへ行く。\n",
      "今日はをへ行く。\n",
      "今日はとへ行く。\n",
      "今日はからへ行く。\n"
     ]
    }
   ],
   "source": [
    "# 5-7\n",
    "# =========================================================\n",
    "# 説明のコメント付き：最初の [MASK] を上位K候補で穴埋めする関数\n",
    "# ---------------------------------------------------------\n",
    "# ■ 目的\n",
    "#   - 文章中の「最初の」 [MASK] を、MLM（BertForMaskedLM）の出力ロジットから\n",
    "#     上位K（num_topk）件の最尤候補で置換した文を生成する。\n",
    "#   - 戻り値は「穴埋め後テキストのリスト」と「各候補のスコア（ロジット）」。\n",
    "#\n",
    "# ■ 理論メモ\n",
    "#   - MLM（Masked Language Modeling）は、[MASK] 位置で p(token | context) を推定するタスク。\n",
    "#     モデル出力 logits の形状は [B, L, |Vocab|]。位置 pos の分布は logits[0, pos]。\n",
    "#   - 最尤候補は argmax、上位K候補は topk（確率化するなら softmax / 温度 / top-p 等も可）。\n",
    "#   - 特殊トークンの ID（[MASK], [CLS], [SEP] など）は **モデル依存**。ハードコード禁止。\n",
    "#     → `tokenizer.mask_token_id` / `tokenizer.mask_token` を常に用いる。\n",
    "#   - サブワード（WordPiece）の '##' 接頭辞を手作業で剥がすより、`tokenizer.decode([id])`\n",
    "#     を使う方が自然な表記（空白・正規化）になりやすい。\n",
    "#   - Self-Attention の計算量は O(L^2 H)。本関数は単文・短文想定のため padding/attention_mask 省略でも実害は小さいが、\n",
    "#     実運用（バッチ/長文）では tokenizer(..., padding/truncation, return_tensors='pt') を推奨。\n",
    "#\n",
    "# ■ 実装メモ\n",
    "#   - デバイスは「モデルの実デバイス」に合わせる（MPS/CUDA/CPU）。Macでは MPS（Metal）を想定。\n",
    "#   - [MASK] が複数ある場合は **最初の一つだけ** を対象にする（要件どおり）。拡張は容易。\n",
    "#   - スコア（ロジット）は CPU の NumPy に落として返す（ログや外部I/Oに使いやすい）。\n",
    "# =========================================================\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def predict_mask_topk(\n",
    "    text: str, tokenizer, bert_mlm, num_topk: int\n",
    ") -> Tuple[List[str], \"np.ndarray\"]:\n",
    "    \"\"\"\n",
    "    文章中の「最初の」[MASK]を、スコア上位のトークンで置換する。\n",
    "    上位何位まで使うかは num_topk で指定。\n",
    "    出力は（穴埋め後テキストのリスト, スコア配列[NumPy]）。\n",
    "    \"\"\"\n",
    "    if num_topk <= 0:\n",
    "        raise ValueError(\"num_topk は 1 以上を指定してください。\")\n",
    "\n",
    "    # --- 1) エンコード：既定で [CLS]/[SEP] が付く。return_tensors='pt' で ID列テンソル化（B=1, L）\n",
    "    enc = tokenizer(\n",
    "        text, return_tensors=\"pt\"\n",
    "    )  # 単文・短文のため padding/truncation は省略\n",
    "\n",
    "    # --- 2) デバイス整合：モデルの実デバイスへ移動（Macなら多くは 'mps'）\n",
    "    device = next(bert_mlm.parameters()).device\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "    # --- 3) 推論：勾配は不要（省メモリ・高速化）\n",
    "    bert_mlm.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = bert_mlm(**enc).logits  # [1, L, |Vocab|]\n",
    "\n",
    "    input_ids = enc[\"input_ids\"]  # [1, L]\n",
    "    mask_id = tokenizer.mask_token_id  # モデル依存；固定値を使わない\n",
    "    mask_tok = tokenizer.mask_token  # 文字列 \"[MASK]\"\n",
    "\n",
    "    # --- 4) 「最初の [MASK]」の位置を取得\n",
    "    mask_positions = (input_ids[0] == mask_id).nonzero(as_tuple=True)[0].tolist()\n",
    "    if not mask_positions:\n",
    "        raise ValueError(\n",
    "            \"入力文に [MASK] が見つかりません。text に tokenizer.mask_token を含めてください。\"\n",
    "        )\n",
    "    pos = mask_positions[0]  # 最初の [MASK]\n",
    "\n",
    "    # --- 5) 上位K候補を取得（ロジットの topk）\n",
    "    # logits[0, pos] は語彙サイズ |V| のスコアベクトル\n",
    "    topk = torch.topk(logits[0, pos], k=num_topk)\n",
    "    ids_topk = topk.indices.tolist()  # 候補の語彙ID列\n",
    "    scores_topk = topk.values.detach().to(\"cpu\").numpy()  # NumPy に落として返す\n",
    "\n",
    "    # --- 6) ID → 文字列へ復元（decode を優先；空になった場合はトークン文字列をフォールバック）\n",
    "    tokens_topk = []\n",
    "    for tid in ids_topk:\n",
    "        s = tokenizer.decode([tid], skip_special_tokens=True).strip()\n",
    "        if not s:\n",
    "            s = tokenizer.convert_ids_to_tokens(tid).lstrip(\"##\")\n",
    "        tokens_topk.append(s)\n",
    "\n",
    "    # --- 7) 「最初の1箇所のみ」置換：全置換を避けるため split → 1回だけ挿入 → 連結\n",
    "    pre, sep, post = text.partition(mask_tok)  # 最初の [MASK] で三分割\n",
    "    if sep == \"\":\n",
    "        # 理論的にはここに来ない（上で検出済み）が、防御的に処理\n",
    "        return [text], scores_topk\n",
    "\n",
    "    text_topk: List[str] = [pre + tok + post for tok in tokens_topk]\n",
    "\n",
    "    return text_topk, scores_topk\n",
    "\n",
    "\n",
    "# --- 使用例 ---\n",
    "# 例文：最初の [MASK] のみを置換\n",
    "text = \"今日は[MASK]へ行く。\"\n",
    "text_topk, scores_topk = predict_mask_topk(text, tokenizer, bert_mlm, 10)\n",
    "\n",
    "# 可読表示：上位候補を上から順に（logits は単調変換で確率に相当。softmax するなら別途）\n",
    "print(*text_topk, sep=\"\\n\")\n",
    "# （必要なら確率化）例：\n",
    "# import numpy as np\n",
    "# probs = np.exp(scores_topk - scores_topk.max())  # softmax の安定版の一部\n",
    "# probs = probs / probs.sum()\n",
    "# print(\"probs:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd467f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日は、東京へ行く。\n"
     ]
    }
   ],
   "source": [
    "# 5-8\n",
    "# =========================================================\n",
    "# 説明のコメント付き：貪欲法（Greedy）による逐次穴埋め\n",
    "# ---------------------------------------------------------\n",
    "# ■ 目的\n",
    "#   - [MASK] を含む文章に対し、左から順に 1 箇所ずつ最尤候補（top-1, argmax）で置換していき、\n",
    "#     すべての [MASK] を埋めた文を返す。\n",
    "#\n",
    "# ■ 理論メモ\n",
    "#   - MLM（Masked Language Modeling）は「各位置のトークン分布 p(token | context)」を返すが、\n",
    "#     同時に複数の [MASK] を最適化する（joint）わけではない。\n",
    "#   - 本関数は **貪欲（greedy）**：最左の [MASK] を top-1 で即確定 → 文を更新 → 次の [MASK] … を繰り返す。\n",
    "#     - 長所：実装・計算が簡単（前向き推論を #MASK 回まわすだけ）\n",
    "#     - 短所：**順序依存**かつ**局所最適**に陥りやすい（後の候補が前決めに縛られる）\n",
    "#   - 代替案（必要に応じて検討）：\n",
    "#     - **ビーム探索**：上位 b 個の候補で分岐し、スコア合算の高い文を保持\n",
    "#     - **確率的生成**：top-k / top-p（nucleus）サンプリングで多様性を確保\n",
    "#     - **スパン復元型**モデル（T5 など）：複数トークンの欠落（span corruption）に強い\n",
    "#\n",
    "# ■ 実装上の注意\n",
    "#   - `predict_mask_topk` は 1 箇所の [MASK] を対象にする前提。ここでは「最初の 1 箇所」を top-1 で埋める挙動を利用。\n",
    "#   - `tokenizer.mask_token` を使って [MASK] 文字列を特定（モデル依存で文字列が異なる可能性に備える）。\n",
    "#   - ループ回数は **現在のテキスト中の [MASK] 個数**。各反復で文が更新され、次の [MASK] の分布も変わる。\n",
    "#   - まれに decode の結果が空文字/特殊トークンになる場合があるため、`predict_mask_topk` 側でフォールバック処理済み。\n",
    "# =========================================================\n",
    "\n",
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "def greedy_prediction(text: str, tokenizer, bert_mlm) -> str:\n",
    "    \"\"\"\n",
    "    [MASK] を含む文章を入力として、左から順に 1 箇所ずつ最尤候補（top-1）で置換し、\n",
    "    すべての [MASK] を埋めた文章を返す。\n",
    "    \"\"\"\n",
    "    # モデル依存の [MASK] リテラル（例：\"[MASK]\"）\n",
    "    mask_tok = getattr(tokenizer, \"mask_token\", \"[MASK]\")\n",
    "\n",
    "    # 互換性のため：もし text にハードコード \"[MASK]\" が含まれ、mask_tok が異なるなら置換\n",
    "    if mask_tok != \"[MASK]\" and \"[MASK]\" in text:\n",
    "        text = text.replace(\"[MASK]\", mask_tok)\n",
    "\n",
    "    # 現在の文に残っている [MASK] の数だけ繰り返し\n",
    "    num_masks = text.count(mask_tok)\n",
    "    for _ in range(num_masks):\n",
    "        # 1 箇所だけ（最初の [MASK]）を top-1 で置換\n",
    "        filled_list, _scores = predict_mask_topk(text, tokenizer, bert_mlm, num_topk=1)\n",
    "        # predict_mask_topk は常に少なくとも 1 件返す想定（例外時は raise）\n",
    "        new_text = filled_list[0]\n",
    "\n",
    "        # 防御的：もし何らかの理由でテキストが変わらなければ早期終了（無限ループ回避）\n",
    "        if new_text == text:\n",
    "            break\n",
    "\n",
    "        text = new_text\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# --- 使用例（2 マスクを貪欲に埋める） ---\n",
    "text = \"今日は[MASK][MASK]へ行く。\"\n",
    "result = greedy_prediction(text, tokenizer, bert_mlm)\n",
    "print(result)\n",
    "\n",
    "# 参考：\n",
    "# - 生成の質を上げたい場合は、greedy の代わりに top-k / top-p サンプリングやビーム探索を検討。\n",
    "# - 「今日は [MASK] 駅 へ行く」のように右文脈を具体化すると、名詞候補が上がりやすい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c17bd591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日は社会社会的な地位\n"
     ]
    }
   ],
   "source": [
    "# 5-9\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#   - 5 連続の [MASK] を含む文を、貪欲法（greedy）で左から順に 1 トークンずつ埋めて最終文を得る。\n",
    "#\n",
    "# 理論メモ：\n",
    "#   - BERT の MLM は「各位置の 1 トークン」分布 p(token | 文脈) を返す（自己回帰ではない）。\n",
    "#   - 貪欲法は「最左の [MASK] を argmax で即確定 → 文を更新 → 次の [MASK] …」の繰り返し。\n",
    "#     長所：計算が軽い。短所：局所最適に陥りやすく、記号や助詞の“安全牌”が連続しがち。\n",
    "#   - 5 連続 [MASK] は内容語（名詞句）よりも、高頻度の助詞・記号が上位に来やすい（1 トークン最尤の積み重ね）。\n",
    "#   - 品質を上げたい場合：ビーム探索／top-k・top-p サンプリング／名詞フィルタ／右文脈強化（例：「…[MASK][MASK]駅へ行く」）が有効。\n",
    "#\n",
    "# 前提：\n",
    "#   - `predict_mask_topk(text, tokenizer, bert_mlm, k)` と `greedy_prediction(text, tokenizer, bert_mlm)` は前セルで定義済み。\n",
    "#   - `tokenizer` と `bert_mlm` は同一モデル名でロード済み、かつ同一 device（MPS/CUDA/CPU）に配置済み。\n",
    "# =========================================================\n",
    "\n",
    "# 入力：5 連続 [MASK]\n",
    "text = \"今日は[MASK][MASK][MASK][MASK][MASK]\"\n",
    "\n",
    "# 実行：貪欲法で順次埋める\n",
    "result = greedy_prediction(text, tokenizer, bert_mlm)\n",
    "print(result)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# （任意）逐次の埋め替え過程を可視化したい場合は、以下のデバッグ関数を使う：\n",
    "# ---------------------------------------------------------\n",
    "def greedy_prediction_debug(text, tokenizer, bert_mlm):\n",
    "    \"\"\"\n",
    "    貪欲法の各ステップで文を表示するデバッグ版。\n",
    "    \"\"\"\n",
    "    mask_tok = getattr(tokenizer, \"mask_token\", \"[MASK]\")\n",
    "    # 互換：text が \"[MASK]\" リテラルの場合、モデル依存の mask_tok に揃える\n",
    "    if mask_tok != \"[MASK]\" and \"[MASK]\" in text:\n",
    "        text = text.replace(\"[MASK]\", mask_tok)\n",
    "\n",
    "    step = 0\n",
    "    while mask_tok in text:\n",
    "        step += 1\n",
    "        print(f\"# step {step} (before): {text}\")\n",
    "        filled_list, _ = predict_mask_topk(text, tokenizer, bert_mlm, num_topk=1)\n",
    "        text = filled_list[0]\n",
    "        print(f\"# step {step}  (after): {text}\")\n",
    "    return text\n",
    "\n",
    "\n",
    "# デバッグ出力を見たいときに有効化：\n",
    "# _ = greedy_prediction_debug('今日は[MASK][MASK][MASK][MASK][MASK]', tokenizer, bert_mlm)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# （応用）助詞・記号を簡易フィルタして内容語を優先したい場合の例（ヒューリスティック）\n",
    "# ---------------------------------------------------------\n",
    "import re\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "# “名詞っぽさ”の非常に粗いヒューリスティック（厳密には形態素解析を推奨）\n",
    "_PUNCT_OR_PARTICLES = set(\n",
    "    [\n",
    "        \"、\",\n",
    "        \"。\",\n",
    "        \"・\",\n",
    "        \"「\",\n",
    "        \"」\",\n",
    "        \"（\",\n",
    "        \"）\",\n",
    "        \"[\",\n",
    "        \"]\",\n",
    "        \"(\",\n",
    "        \")\",\n",
    "        \"は\",\n",
    "        \"の\",\n",
    "        \"に\",\n",
    "        \"で\",\n",
    "        \"を\",\n",
    "        \"と\",\n",
    "        \"へ\",\n",
    "        \"から\",\n",
    "        \"より\",\n",
    "        \"や\",\n",
    "        \"も\",\n",
    "        \"が\",\n",
    "    ]\n",
    ")\n",
    "_JA_WORD_PATTERN = re.compile(\n",
    "    r\"^[\\u3040-\\u30ff\\u3400-\\u9fffA-Za-z0-9ー・]+$\"\n",
    ")  # 仮名・漢字・英数・長音・中黒\n",
    "\n",
    "\n",
    "def predict_mask_topk_filtered(\n",
    "    text: str, tokenizer, bert_mlm, num_topk: int, k_expand: int = 50\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    predict_mask_topk の派生：top-k を広めに取ってから（k_expand）、助詞・記号を粗く除外し、\n",
    "    上位 num_topk を返す。完全ではないが内容語が出やすくなることがある。\n",
    "    \"\"\"\n",
    "    enc = tokenizer(text, return_tensors=\"pt\")\n",
    "    device = next(bert_mlm.parameters()).device\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    bert_mlm.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = bert_mlm(**enc).logits\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    mask_id = tokenizer.mask_token_id\n",
    "    mask_tok = tokenizer.mask_token\n",
    "\n",
    "    pos_list = (input_ids[0] == mask_id).nonzero(as_tuple=True)[0].tolist()\n",
    "    if not pos_list:\n",
    "        return [text]\n",
    "    pos = pos_list[0]\n",
    "\n",
    "    # 広めの top-k を取り、その中からフィルタで間引く\n",
    "    k = max(num_topk, k_expand)\n",
    "    cand_ids = torch.topk(logits[0, pos], k=k).indices.tolist()\n",
    "\n",
    "    def _token_str(tid: int) -> str:\n",
    "        s = tokenizer.decode([tid], skip_special_tokens=True).strip()\n",
    "        return s if s else tokenizer.convert_ids_to_tokens(tid).lstrip(\"##\")\n",
    "\n",
    "    cands = []\n",
    "    for tid in cand_ids:\n",
    "        s = _token_str(tid)\n",
    "        # 粗フィルタ：助詞・記号っぽいものを除外\n",
    "        if (s in _PUNCT_OR_PARTICLES) or (not _JA_WORD_PATTERN.match(s)):\n",
    "            continue\n",
    "        cands.append(s)\n",
    "        if len(cands) >= num_topk:\n",
    "            break\n",
    "\n",
    "    if not cands:\n",
    "        # すべて弾かれたら元の top-1 を返す（安全弁）\n",
    "        cands = [_token_str(cand_ids[0])]\n",
    "\n",
    "    pre, sep, post = text.partition(mask_tok)\n",
    "    return [pre + c + post for c in cands]\n",
    "\n",
    "\n",
    "# 使い方（任意）：\n",
    "# text = '今日は[MASK][MASK][MASK][MASK][MASK]'\n",
    "# t, _ = predict_mask_topk(text, tokenizer, bert_mlm, 1)  # まず 1 個目\n",
    "# text = t[0]\n",
    "# text = predict_mask_topk_filtered(text, tokenizer, bert_mlm, num_topk=1)[0]  # フィルタ版で次を埋める …といった併用も可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80e406a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日はお台場へ行く。\n",
      "今日はお祭りへ行く。\n",
      "今日はゲーム##センターへ行く。\n",
      "今日はお風呂へ行く。\n",
      "今日はゲームショップへ行く。\n",
      "今日は東京ディズニーランドへ行く。\n",
      "今日はお店へ行く。\n",
      "今日は同じ場所へ行く。\n",
      "今日はあの場所へ行く。\n",
      "今日は同じ学校へ行く。\n"
     ]
    }
   ],
   "source": [
    "# 5-10\n",
    "# =========================================================\n",
    "# 理論的な説明コメント（ビームサーチによる連続[MASK]の穴埋め）\n",
    "# ---------------------------------------------------------\n",
    "# ■ 目的\n",
    "#   - 文章中の複数の [MASK] を、ビームサーチ（Beam Search）により高スコアな候補列で逐次埋める。\n",
    "#   - 各ステップで「最初の [MASK]」を top-k 展開し、合計スコアの高い上位k本の“部分列”を保持して次ステップへ進む。\n",
    "#\n",
    "# ■ ビームサーチの理論背景\n",
    "#   - 貪欲法（greedy）は各ステップで argmax を即確定するため、系列全体の最適性を考慮できず局所最適に陥りやすい。\n",
    "#   - ビームサーチは、各ステップで上位k候補（ビーム幅）を“並走”させ、累積スコアで比較することで\n",
    "#     系列全体の尤もらしさを近似的に最大化する探索法である。\n",
    "#\n",
    "# ■ スコア設計に関する注意（重要）\n",
    "#   - 本コードでは predict_mask_topk が返す「scores_topk_inner」を**そのまま加算**している。\n",
    "#     これが「ロジット（未正規化スコア）」である場合、理論上は各ステップごとに log-softmax で\n",
    "#     **対数確率**へ変換し、その和（累積 log p）を最大化するのが尤度に整合的である。\n",
    "#   - ただし、上位候補の相対順位が大きく変わらない前提では「ロジット和」でも経験的に動作することがある。\n",
    "#     再現性・理論整合性を重視するなら、predict_mask_topk 側で log-softmax へ変換してから返す設計が望ましい。\n",
    "#\n",
    "# ■ 探索幅（num_topk）の解釈\n",
    "#   - 本コードでは num_topk を「各ステップの展開幅（top-k）」と「ビーム幅（保持本数）」の**兼用**として使っている。\n",
    "#   - 実務では通常、`top_k_each`（展開幅）と `beam_size`（保持幅）を**別パラメータ**に分けると性能/計算の調整がしやすい。\n",
    "#\n",
    "# ■ 複数[MASK]の扱い\n",
    "#   - 各ステップでは「現在の部分解テキスト」の**最初の [MASK] だけ**を predict_mask_topk で埋める。\n",
    "#   - ステップ数は初期の num_mask だけ繰り返す。毎ステップでビーム内の各文が1つずつ [MASK] を減らすイメージ。\n",
    "#\n",
    "# ■ 脱落・同値・冗長性\n",
    "#   - 同一文字列が複数の経路から生成される場合がある（冗長化）。一意化すると効率が上がる場合もある。\n",
    "#   - 同スコア同士の順序は argsort 依存（安定ではない可能性）。必要なら安定ソートや副次指標で決める。\n",
    "#\n",
    "# ■ 複語復元とモデルの性質\n",
    "#   - BERT-MLM は「位置ごとの1トークン」予測であるため、連続[MASK]で**名詞句**を復元するのは構造上難しい。\n",
    "#   - ビームサーチはその制約を緩和するが、抜本的には「スパン復元型（T5系）」の方が親和性が高い。\n",
    "#\n",
    "# ■ 計算量の見積り\n",
    "#   - おおよそ O( num_mask × num_topk × fwd_cost ) で推論を繰り返す（ビーム内の全経路で前向きが走る）。\n",
    "#   - モデルを eval() にし、no_grad を徹底、系列長（max_length）を適切化することで実時間を抑える。\n",
    "# =========================================================\n",
    "\n",
    "\n",
    "def beam_search(text, tokenizer, bert_mlm, num_topk):\n",
    "    \"\"\"\n",
    "    ビームサーチで文章の穴埋めを行う。\n",
    "\n",
    "    理論注記：\n",
    "      - num_topk は「各ステップで展開する候補数」かつ「保持するビーム幅」として使っている（兼用）。\n",
    "      - 合計スコアは単純加算。scores_topk_inner がロジットの場合、\n",
    "        本来は log-softmax による対数確率和で比較するのが尤度最大化に整合的。\n",
    "    \"\"\"\n",
    "    # 初期の [MASK] 個数（反復回数）。各ステップで最初の1個だけ埋める前提。\n",
    "    num_mask = text.count(\"[MASK]\")\n",
    "\n",
    "    # 現時点の上位候補（テキスト列）と、それぞれの累積スコア。\n",
    "    # 初期状態は「原文（マスクあり）」とスコア0。\n",
    "    text_topk = [text]\n",
    "    scores_topk = np.array([0])\n",
    "\n",
    "    # マスクの数だけステップを繰り返し、各ステップで1つずつ埋める\n",
    "    for _ in range(num_mask):\n",
    "        # 現在の各候補テキストごとに、最初の [MASK] を top-k（=num_topk）で展開\n",
    "        text_candidates = []  # 新たに生成される候補文（展開分すべて）\n",
    "        score_candidates = []  # それらに対応する「累積スコア」\n",
    "\n",
    "        for text_mask, score in zip(text_topk, scores_topk):\n",
    "            # predict_mask_topk は「最初の [MASK]」を top-k 候補で置換した文リストと\n",
    "            # その“スコア”（ロジット or log確率）ベクトルを返す前提\n",
    "            text_topk_inner, scores_topk_inner = predict_mask_topk(\n",
    "                text_mask, tokenizer, bert_mlm, num_topk\n",
    "            )\n",
    "            # 生成された各候補文を集約\n",
    "            text_candidates.extend(text_topk_inner)\n",
    "            # 既存の累積スコアに、今回のトークン分スコアを加算\n",
    "            # （理論的には logsoftmax の和が望ましい。ここでは返り値の尺度に依存）\n",
    "            score_candidates.append(score + scores_topk_inner)\n",
    "\n",
    "        # step 全体で生まれた（ビーム幅×展開幅）本の候補から、合計スコア上位 num_topk を選抜\n",
    "        score_candidates = np.hstack(score_candidates)  # 1次元配列へ連結\n",
    "        idx_list = score_candidates.argsort()[::-1][\n",
    "            :num_topk\n",
    "        ]  # 降順上位kのインデックス\n",
    "        text_topk = [\n",
    "            text_candidates[idx] for idx in idx_list\n",
    "        ]  # 次ステップへ残すトップkテキスト\n",
    "        scores_topk = score_candidates[idx_list]  # 対応スコア\n",
    "\n",
    "    # すべての [MASK] を埋め切った時点で、上位 num_topk 本の候補文を返す\n",
    "    return text_topk\n",
    "\n",
    "\n",
    "text = \"今日は[MASK][MASK]へ行く。\"\n",
    "text_topk = beam_search(text, tokenizer, bert_mlm, 10)\n",
    "print(*text_topk, sep=\"\\n\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 追加の理論的補足と改善余地\n",
    "# ---------------------------------------------------------\n",
    "# 1) スコアの正規化：\n",
    "#    - predict_mask_topk の戻り値がロジットなら、内部で log-softmax に変換し、\n",
    "#      ここでは「累積 log 確率の和」を加算するのが尤度最大化に一致する。\n",
    "#\n",
    "# 2) パラメータ分離：\n",
    "#    - 「展開幅 top_k_each」と「保持幅 beam_size」を分離し、\n",
    "#      展開は広く（例：50）、保持は適度（例：5）にする設計が探索/計算のトレードオフ上有利。\n",
    "#\n",
    "# 3) 冗長候補の統合：\n",
    "#    - 同一テキストが複数経路で生成された場合は重複排除（最後に高スコアのみ保持）で効率向上。\n",
    "#\n",
    "# 4) 品質向上のための事後フィルタ：\n",
    "#    - 助詞・記号の除外、品詞ベースのフィルタ（形態素解析）、右文脈の具体化などを併用すると\n",
    "#      名詞句・固有名詞の自然さが向上しやすい。\n",
    "#\n",
    "# 5) モデル選択：\n",
    "#    - 連続[MASK]の“句”復元に本質的に強いのは、スパン汚染で事前学習した T5 系など。\n",
    "#      BERT-MLM + ビームでも改善は見込めるが、構造的適性は T5 系に分がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f209f40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日は社会社会学会所属。\n",
      "今日は社会社会学会会長。\n",
      "今日は社会社会に属する。\n",
      "今日は時代社会に属する。\n",
      "今日は社会社会学会理事。\n",
      "今日は時代社会にあたる。\n",
      "今日は社会社会にある。\n",
      "今日は社会社会学会会員。\n",
      "今日は時代社会にある。\n",
      "今日は社会社会になる。\n"
     ]
    }
   ],
   "source": [
    "# 5-11（説明コメント付き：5連続 [MASK] をビームサーチで穴埋めして表示）\n",
    "# ======================================================================\n",
    "# 目的：\n",
    "#  - 連続する5箇所の [MASK] を、前セルで定義したビームサーチ（beam_search）で逐次的に埋め、\n",
    "#    最終候補の上位10文を改行区切りで表示する実行セル。\n",
    "#\n",
    "# 前提（このセルが成立するための条件）：\n",
    "#  - tokenizer と bert_mlm が同じモデル名（語彙）でロード済みであること。\n",
    "#  - beam_search 関数と predict_mask_topk 関数が前セルで定義済みであること。\n",
    "#  - predict_mask_topk は「最初の [MASK] を top-k 候補で置換した文リスト」と\n",
    "#    「そのときのスコア（通常はロジット。理想は log-softmax）」を返す仕様であること。\n",
    "#  - ここでは [MASK] の“文字列”を直書きしているため、tokenizer.mask_token が \"[MASK]\" と一致している前提。\n",
    "#   （もし異なる場合は、事前に text の [MASK] を tokenizer.mask_token に置換すること。）\n",
    "#\n",
    "# 理論メモ（なぜビームサーチを使うか）：\n",
    "#  - BERT の MLM は各位置の「1トークン分布」を返すため、貪欲法（その場で argmax 確定）だと\n",
    "#    局所最適（助詞・記号の連鎖）に偏りやすい。\n",
    "#  - ビームサーチは、各ステップで上位k候補（ビーム）を“並走”させ、累積スコアで比較するため、\n",
    "#    系列全体の尤もらしさを近似的に最大化できる（greedy より頑健）。\n",
    "#  - ただし、beam_search の内部で「スコアの足し方」がロジット和のままだと理論的な尤度比較とはずれる。\n",
    "#    可能なら predict_mask_topk 側で log-softmax に変換し、ここでは「累積対数確率の和」で比較するのが望ましい。\n",
    "#\n",
    "# 期待される出力の傾向：\n",
    "#  - 文脈が「今日は [MASK][MASK][MASK][MASK][MASK]」と抽象的なので、頻度の高い助詞や記号（、・は・の・に…）が\n",
    "#    上位に出やすいのは仕様どおり（バグではない）。\n",
    "#  - 右文脈を具体化（例：「今日は [MASK][MASK] 駅 へ行く」）すると名詞が上がりやすい。\n",
    "#\n",
    "# 実行コストの概算：\n",
    "#  - 概ね O( #MASK × ビーム幅 × 前向き計算コスト )。\n",
    "#  - モデルは eval()・no_grad を徹底し、max_length を過不足なく設定することで時間・メモリを抑制。\n",
    "#\n",
    "# 表示：\n",
    "#  - print(*text_topk, sep='\\n') は、ビーム上位10件（文字列リスト）を1行ずつ出力する。\n",
    "# ======================================================================\n",
    "\n",
    "text = \"今日は[MASK][MASK][MASK][MASK][MASK]\"  # 入力文：5連続の [MASK]（span 的な欠落を想定）\n",
    "\n",
    "# ビームサーチの適用：\n",
    "# - 第1ステップ：最初の [MASK] を top-10 展開 → 上位10本の部分解を保持\n",
    "# - 第2〜5ステップ：各部分解で再び「最初の [MASK]」を top-10 展開 → 合計スコア上位10本を保持\n",
    "# - 結果：全ての [MASK] を埋めた候補文の上位10件が得られる\n",
    "text_topk = beam_search(text, tokenizer, bert_mlm, 10)\n",
    "\n",
    "# 出力：上位候補を改行区切りで列挙\n",
    "print(*text_topk, sep=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
