{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f5ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-2\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec9c8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using device = mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=32000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-3\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#  - 日本語BERT（Whole Word Masking 版）を「MLM（Masked Language Modeling）」タスク用の\n",
    "#    モデルとしてロードし、MacBook を含む環境で最適なデバイス（MPS/CUDA/CPU）に載せる。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - MLM（Masked Language Modeling）は、入力トークンの一部を [MASK] に置換し、\n",
    "#    その復元を学習する自己教師ありタスクである。BERTの事前学習の主要素。\n",
    "#  - Whole Word Masking（WWM）は「サブワード単位ではなく“語”単位でまとめてマスクする」\n",
    "#    戦略。語彙断片ではなく語レベルの完形復元圧力を与えるため、語彙的一貫性の向上が期待される。\n",
    "#  - 推論（補完）時は [MASK] の位置の語彙分布（logits）から上位候補を選ぶ。\n",
    "#    学習時は [MASK] 位置以外のロスを無視（ignore_index=-100）するのが一般的（DataCollatorが自動対応可）。\n",
    "#  - 日本語BERT（BertJapaneseTokenizer）は「形態素解析→WordPiece」の二段で分割する。\n",
    "#    分割は辞書・語彙・バージョンに依存するため、再現性は“性質（不変量）”で管理する。\n",
    "#\n",
    "# 実務Tips：\n",
    "#  - MacBook（Apple Silicon）では CUDA は使えないため、Metal(MPS) を優先する。\n",
    "#  - 推論（補完）だけなら model.eval() ＋ torch.no_grad() でメモリ＆速度最適化。\n",
    "#  - 学習する場合は DataCollatorForLanguageModeling を用い、mlm_probability を設定する。\n",
    "#  - トークナイザとモデルは必ず同じモデル名（語彙）を使う（IDずれ防止）。\n",
    "# =========================================================\n",
    "\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "\n",
    "\n",
    "# --- デバイス検出（Mac優先：MPS → CUDA → CPU） ---\n",
    "def get_best_device() -> torch.device:\n",
    "    # Apple Silicon + macOS（MPSバックエンド）\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    # 他環境に移植した場合に備えて CUDA も許容\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # どれも不可なら CPU\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"[info] using device = {device}\")\n",
    "\n",
    "# --- モデル名（WWM版日本語BERT）：トークナイザとモデルはペアで統一 ---\n",
    "model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "\n",
    "# トークナイザ（形態素解析→WordPiece）。特殊トークン：[CLS]/[SEP]/[PAD]/[MASK]/[UNK]\n",
    "# - `mask_token` と `mask_token_id` は MLM の置換に使用する。\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# MLM 用のヘッド付き BERT（語彙サイズ×隠れ次元の出力層を持つ）\n",
    "# - 出力：logits 形状 [B, L, |Vocab|]。各位置の語彙分布を返す。\n",
    "bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# モデルを最適デバイスへ移動（Mac では .cuda() ではなく .to(device) を使う）\n",
    "bert_mlm = bert_mlm.to(device)\n",
    "\n",
    "# 推論モード（補完タスクなど）：dropout を停止\n",
    "bert_mlm.eval()\n",
    "\n",
    "# --- 参考：MLM 推論の最小例（必要ならコメント解除して確認） ---\n",
    "# text = f\"明日は{tokenizer.mask_token}だ。\"\n",
    "# enc = tokenizer(text, return_tensors='pt')\n",
    "# enc = {k: v.to(device) for k, v in enc.items()}\n",
    "# with torch.no_grad():\n",
    "#     logits = bert_mlm(**enc).logits  # [1, L, V]\n",
    "# mask_index = (enc[\"input_ids\"][0] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0].item()\n",
    "# topk = torch.topk(logits[0, mask_index], k=5).indices.tolist()\n",
    "# candidates = [tokenizer.decode([i]) for i in topk]\n",
    "# print(\"Top-5 candidates at [MASK]:\", candidates)\n",
    "\n",
    "# --- 参考：学習時のスケッチ ---\n",
    "# from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer,\n",
    "#     mlm=True,\n",
    "#     mlm_probability=0.15  # 15% を [MASK]（WWM版モデルの事前学習と整合的に設定）\n",
    "# )\n",
    "# # Trainer(...) を用いて学習。PAD位置は自動でロスから除外される（ignore_index=-100）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65bf0913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens: ['今日', 'は', '[MASK]', 'へ', '行く', '。']\n",
      "# ids: [3246, 9, 4, 118, 3488, 8]\n",
      "# mask token: [MASK] mask id: 4 pos(tok): 2 pos(id): 2\n",
      "# input_ids: [[2, 3246, 9, 4, 118, 3488, 8, 3]]\n",
      "# attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "# tokens (with specials): ['[CLS]', '今日', 'は', '[MASK]', 'へ', '行く', '。', '[SEP]']\n",
      "# decode (skip specials): 今日 は へ 行く 。\n"
     ]
    }
   ],
   "source": [
    "# 5-4\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#  - 入力文に含まれる [MASK] を含めて、日本語BERT用トークナイザでサブワード列へ分割し、挙動を確認する。\n",
    "#\n",
    "# 理論メモ（重要ポイント）：\n",
    "#  - MLM（Masked Language Modeling）では、[MASK] 位置の語彙分布を予測する（学習時は [MASK] 以外の損失を無視）。\n",
    "#  - `BertJapaneseTokenizer` は「形態素解析 → WordPiece」の二段で分割する。分割境界は辞書・語彙・バージョンに依存。\n",
    "#  - `tokenize()` は **特殊トークン [CLS]/[SEP] を付けない**“素のサブワード列”を返す。\n",
    "#  - `[MASK]` は **特殊トークンとして保存**される想定（= 文字列 \"[MASK]\" がそのままトークン列に現れる）。\n",
    "#    ※ 安全のため、リテラル \"[MASK]\" 直書きより `tokenizer.mask_token` を使うのが堅牢。\n",
    "#  - `encode()` / `tokenizer(..., return_tensors='pt')` は **ID列**を返し、既定で [CLS]/[SEP] を付与して下流モデルに直結できる。\n",
    "# =========================================================\n",
    "\n",
    "# 保険：tokenizer 未定義なら初期化（WWM版日本語BERTとペアで統一）\n",
    "try:\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    from transformers import BertJapaneseTokenizer\n",
    "\n",
    "    model_name = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# --- 入力文：安全のため tokenizer.mask_token を用いて [MASK] を埋め込む ---\n",
    "text = f\"今日は{tokenizer.mask_token}へ行く。\"\n",
    "\n",
    "# --- サブワード列へ分割（特殊トークン [CLS]/[SEP] は付かない）---\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"# tokens:\", tokens)\n",
    "\n",
    "# --- ID への写像（[MASK] が mask_token_id に対応することを確認）---\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"# ids:\", ids)\n",
    "\n",
    "mask_tok = tokenizer.mask_token  # 例：\"[MASK]\"\n",
    "mask_id = tokenizer.mask_token_id  # 例：mask の語彙ID\n",
    "mask_pos_tok = tokens.index(mask_tok) if mask_tok in tokens else None\n",
    "mask_pos_id = ids.index(mask_id) if mask_id in ids else None\n",
    "print(\n",
    "    \"# mask token:\",\n",
    "    mask_tok,\n",
    "    \"mask id:\",\n",
    "    mask_id,\n",
    "    \"pos(tok):\",\n",
    "    mask_pos_tok,\n",
    "    \"pos(id):\",\n",
    "    mask_pos_id,\n",
    ")\n",
    "\n",
    "# --- 参考：特殊トークン付与あり（モデル入力に近い形；[CLS] と [SEP] を含む）---\n",
    "enc = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "print(\"# input_ids:\", enc[\"input_ids\"].tolist())\n",
    "print(\"# attention_mask:\", enc[\"attention_mask\"].tolist())\n",
    "print(\"# tokens (with specials):\", tokenizer.convert_ids_to_tokens(enc[\"input_ids\"][0]))\n",
    "\n",
    "# --- 復元表示（可逆ではない場合がある点に注意：空白や正規化、未知語の影響）---\n",
    "print(\n",
    "    \"# decode (skip specials):\",\n",
    "    tokenizer.decode(enc[\"input_ids\"][0], skip_special_tokens=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920d8ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using device = mps\n"
     ]
    }
   ],
   "source": [
    "# 5-5\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#  - [MASK] を含む文 `text` を符号化し、MLM（BertForMaskedLM）へ入力して\n",
    "#    各トークン位置の語彙分布（logits）を得る。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - MLM（Masked Language Modeling）では [MASK] 位置での語彙分布 p(token | context) を予測する。\n",
    "#    モデル出力 `logits` は形状 [B, L, |V|]（バッチ×系列長×語彙サイズ）であり、\n",
    "#    「分類スコア」は“各トークン位置における語彙分類のスコア”を意味する。\n",
    "#  - `tokenizer.encode(text, return_tensors='pt')` は **ID列テンソル** を返す（既定で [CLS]/[SEP] 付与）。\n",
    "#    研究・実務では `tokenizer(text, return_tensors='pt', padding=..., truncation=...)` の使用が推奨だが、\n",
    "#    単文・短文であれば `encode` でも問題ない。\n",
    "#  - attention_mask を省略した場合、PAD を含まない単文では実害は基本的にない。\n",
    "#    ただしバッチやパディングを伴う運用では attention_mask を明示するのが安全。\n",
    "#  - デバイス：MacBook(Apple Silicon) では CUDA は使えない。Metal(MPS) を優先し、\n",
    "#    それも不可なら CPU を用いる。モデルとテンソルは**常に同じ device** に揃える。\n",
    "# =========================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# --- デバイス検出（Mac優先：MPS → CUDA → CPU） ---\n",
    "def get_best_device() -> torch.device:\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")  # Apple GPU (Metal)\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")  # 他環境への持ち出し用の保険\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"[info] using device = {device}\")\n",
    "\n",
    "# 文章を符号化し、GPU（またはMPS/CPU）に配置する。\n",
    "# - encode は既定で special tokens（[CLS]/[SEP]）を付与する。\n",
    "# - return_tensors='pt' により torch.Tensor（形状 [B=1, L]）で取得。\n",
    "input_ids = tokenizer.encode(\n",
    "    text, return_tensors=\"pt\"\n",
    ")  # 例：tensor([[CLS, ..., MASK, ..., SEP]])\n",
    "# 元コード： input_ids = input_ids.cuda()\n",
    "# Mac では .cuda() は不可。device に合わせて移動する：\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# 念のためモデル側も同じデバイスへ（5-3でto(device)済みなら実質ノーオペ）\n",
    "bert_mlm = bert_mlm.to(device)\n",
    "bert_mlm.eval()  # 推論のみ。学習なら train() に切替\n",
    "\n",
    "# BERTに入力し、語彙スコア（logits）を得る。\n",
    "# ※ 元コメントの「系列長を揃える必要がない」は“単文かつ短文で PAD を使わない”場合に限って概ね正しい。\n",
    "#    バッチや長文運用では padding / attention_mask を明示すること。\n",
    "with torch.no_grad():  # 推論では計算グラフを作らない：省メモリ・高速化\n",
    "    output = bert_mlm(\n",
    "        input_ids=input_ids\n",
    "    )  # 他に attention_mask 等を省略（単文短文の前提）\n",
    "    scores = output.logits  # 形状 [B, L, |V|]：各位置の語彙ロジット\n",
    "\n",
    "# --- 参考（任意）：[MASK] 位置の Top-k 候補を取得する手順（コメントアウト） ---\n",
    "# mask_id = tokenizer.mask_token_id\n",
    "# mask_pos = (input_ids[0] == mask_id).nonzero(as_tuple=True)[0].item()  # [MASK] の位置（L次元）\n",
    "# topk = torch.topk(scores[0, mask_pos], k=5).indices.tolist()\n",
    "# candidates = [tokenizer.decode([i]) for i in topk]  # 語彙ID → 文字列\n",
    "# print(\"Top-5 candidates at [MASK]:\", candidates)\n",
    "\n",
    "# --- 参考（理論補足）---\n",
    "# - Self-Attention：softmax(QK^T / √d_k) V。系列長 L に対して計算量 O(L^2 H)。\n",
    "#   長文・バッチ運用では L を適切に管理（truncation/stride/動的パディング）すること。\n",
    "# - PAD を含む場合は attention_mask=0 の位置が softmax 前に -∞ 相当で抑制され、注意が向かない（勾配も基本流れない）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f2c228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日は、へ行く。\n"
     ]
    }
   ],
   "source": [
    "# 5-6\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#  - MLM の出力 `scores`（形状 [B, L, |V|]）から [MASK] 位置の最尤トークン（argmax）を選び，\n",
    "#    元の文の [MASK] をそのトークンで置換して，人間可読な文に復元する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - `scores[0, pos]` は語彙分布（ロジット）。argmax は MAP 推定（最尤語彙）に対応する。\n",
    "#  - [MASK] の語彙IDを **固定値で仮定しない**（モデルによって異なる）。必ず `tokenizer.mask_token_id` を用いる。\n",
    "#  - サブワード（WordPiece）の場合，`convert_ids_to_tokens` は '##' 接頭辞が付くことがある。\n",
    "#    文字列復元としては `tokenizer.decode([id])` の方が自然なことが多い（空白処理や正規化を担う）。\n",
    "#  - 複数 [MASK] がある場合：ここでは「独立同時置換」（各位置で argmax）を行う。\n",
    "#    逐次的に再エンコードして条件付きで更新する「反復置換」は別戦略（より厳密だが計算増）。\n",
    "# =========================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "# --- 1) [MASK] の語彙IDを取得（モデル依存；固定値を使わない） ---\n",
    "mask_id = (\n",
    "    tokenizer.mask_token_id\n",
    ")  # 例：東北大日本語BERTでは 4 のことが多いが、必ず動的に取得する\n",
    "mask_tok = tokenizer.mask_token  # 例：\"[MASK]\"\n",
    "\n",
    "# --- 2) 入力ID列から [MASK] の位置を列挙（複数対応） ---\n",
    "# input_ids: 形状 [B=1, L] を想定（5-5の encode に一致）\n",
    "mask_positions = (input_ids[0] == mask_id).nonzero(as_tuple=True)[0].tolist()\n",
    "if len(mask_positions) == 0:\n",
    "    raise ValueError(\n",
    "        \"入力系列に [MASK] が見つかりません。text に tokenizer.mask_token を含めてください。\"\n",
    "    )\n",
    "\n",
    "# --- 3) 各 [MASK] 位置で最尤トークンIDを取得（argmax over vocab） ---\n",
    "# scores: 形状 [B=1, L, |V|] を想定（5-5の出力 logits）\n",
    "pred_token_ids = []\n",
    "for pos in mask_positions:\n",
    "    # scores[0, pos] : [|V|] ベクトル → 最尤語彙ID\n",
    "    best_id = scores[0, pos].argmax(-1).item()\n",
    "    pred_token_ids.append(best_id)\n",
    "\n",
    "# --- 4) ID → 文字列へ変換（decode を優先） ---\n",
    "# decode は空白や '##' の扱いをよしなに調整してくれる（単一IDでも有用）\n",
    "pred_tokens = [\n",
    "    tokenizer.decode([tid], skip_special_tokens=True).strip() for tid in pred_token_ids\n",
    "]\n",
    "\n",
    "# フォールバック（必要時）：WordPieceの『##』を除去して素片文字列だけを得る\n",
    "pred_tokens_fallback = [\n",
    "    tokenizer.convert_ids_to_tokens(tid).lstrip(\"##\") for tid in pred_token_ids\n",
    "]\n",
    "\n",
    "# decode 側で万が一空文字になった箇所はフォールバックを使う\n",
    "pred_tokens = [\n",
    "    pt if pt != \"\" else pf for pt, pf in zip(pred_tokens, pred_tokens_fallback)\n",
    "]\n",
    "\n",
    "# --- 5) 元のテキスト中の [MASK] を順に置換（複数 [MASK] に対応） ---\n",
    "# str.replace は全置換になるため、分割→挿入→連結で「順番に」置換する\n",
    "parts = text.split(mask_tok)  # [seg0, seg1, ..., segN]  （N = マスク数）\n",
    "filled_segments = []\n",
    "for i, seg in enumerate(parts[:-1]):\n",
    "    filled_segments.append(seg)\n",
    "    # i 番目の [MASK] に対する候補を挿入\n",
    "    filled_segments.append(pred_tokens[i] if i < len(pred_tokens) else \"\")\n",
    "filled_segments.append(parts[-1])\n",
    "\n",
    "text_filled = \"\".join(filled_segments)\n",
    "print(text_filled)\n",
    "\n",
    "# --- 備考 ---\n",
    "# - 「独立同時置換」では各 [MASK] を互いに独立に推定するため，相互依存（共起制約）を捉えにくい。\n",
    "#   文脈整合性を高めたい場合は，1つずつ置換→再エンコード→再推論の反復法を検討する。\n",
    "# - サブワードが選ばれた場合でも decode により自然な表記になりやすいが，\n",
    "#   場合によっては不自然な連結になる可能性がある（語境界／表記揺れのため）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2c32cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日は、へ行く。\n",
      "今日ははへ行く。\n",
      "今日はのへ行く。\n",
      "今日はにへ行く。\n",
      "今日はでへ行く。\n",
      "今日は「へ行く。\n",
      "今日はへへ行く。\n",
      "今日はをへ行く。\n",
      "今日はとへ行く。\n",
      "今日はからへ行く。\n"
     ]
    }
   ],
   "source": [
    "# 5-7\n",
    "# =========================================================\n",
    "# 説明のコメント付き：最初の [MASK] を上位K候補で穴埋めする関数\n",
    "# ---------------------------------------------------------\n",
    "# ■ 目的\n",
    "#   - 文章中の「最初の」 [MASK] を、MLM（BertForMaskedLM）の出力ロジットから\n",
    "#     上位K（num_topk）件の最尤候補で置換した文を生成する。\n",
    "#   - 戻り値は「穴埋め後テキストのリスト」と「各候補のスコア（ロジット）」。\n",
    "#\n",
    "# ■ 理論メモ\n",
    "#   - MLM（Masked Language Modeling）は、[MASK] 位置で p(token | context) を推定するタスク。\n",
    "#     モデル出力 logits の形状は [B, L, |Vocab|]。位置 pos の分布は logits[0, pos]。\n",
    "#   - 最尤候補は argmax、上位K候補は topk（確率化するなら softmax / 温度 / top-p 等も可）。\n",
    "#   - 特殊トークンの ID（[MASK], [CLS], [SEP] など）は **モデル依存**。ハードコード禁止。\n",
    "#     → `tokenizer.mask_token_id` / `tokenizer.mask_token` を常に用いる。\n",
    "#   - サブワード（WordPiece）の '##' 接頭辞を手作業で剥がすより、`tokenizer.decode([id])`\n",
    "#     を使う方が自然な表記（空白・正規化）になりやすい。\n",
    "#   - Self-Attention の計算量は O(L^2 H)。本関数は単文・短文想定のため padding/attention_mask 省略でも実害は小さいが、\n",
    "#     実運用（バッチ/長文）では tokenizer(..., padding/truncation, return_tensors='pt') を推奨。\n",
    "#\n",
    "# ■ 実装メモ\n",
    "#   - デバイスは「モデルの実デバイス」に合わせる（MPS/CUDA/CPU）。Macでは MPS（Metal）を想定。\n",
    "#   - [MASK] が複数ある場合は **最初の一つだけ** を対象にする（要件どおり）。拡張は容易。\n",
    "#   - スコア（ロジット）は CPU の NumPy に落として返す（ログや外部I/Oに使いやすい）。\n",
    "# =========================================================\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def predict_mask_topk(\n",
    "    text: str, tokenizer, bert_mlm, num_topk: int\n",
    ") -> Tuple[List[str], \"np.ndarray\"]:\n",
    "    \"\"\"\n",
    "    文章中の「最初の」[MASK]を、スコア上位のトークンで置換する。\n",
    "    上位何位まで使うかは num_topk で指定。\n",
    "    出力は（穴埋め後テキストのリスト, スコア配列[NumPy]）。\n",
    "    \"\"\"\n",
    "    if num_topk <= 0:\n",
    "        raise ValueError(\"num_topk は 1 以上を指定してください。\")\n",
    "\n",
    "    # --- 1) エンコード：既定で [CLS]/[SEP] が付く。return_tensors='pt' で ID列テンソル化（B=1, L）\n",
    "    enc = tokenizer(\n",
    "        text, return_tensors=\"pt\"\n",
    "    )  # 単文・短文のため padding/truncation は省略\n",
    "\n",
    "    # --- 2) デバイス整合：モデルの実デバイスへ移動（Macなら多くは 'mps'）\n",
    "    device = next(bert_mlm.parameters()).device\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "    # --- 3) 推論：勾配は不要（省メモリ・高速化）\n",
    "    bert_mlm.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = bert_mlm(**enc).logits  # [1, L, |Vocab|]\n",
    "\n",
    "    input_ids = enc[\"input_ids\"]  # [1, L]\n",
    "    mask_id = tokenizer.mask_token_id  # モデル依存；固定値を使わない\n",
    "    mask_tok = tokenizer.mask_token  # 文字列 \"[MASK]\"\n",
    "\n",
    "    # --- 4) 「最初の [MASK]」の位置を取得\n",
    "    mask_positions = (input_ids[0] == mask_id).nonzero(as_tuple=True)[0].tolist()\n",
    "    if not mask_positions:\n",
    "        raise ValueError(\n",
    "            \"入力文に [MASK] が見つかりません。text に tokenizer.mask_token を含めてください。\"\n",
    "        )\n",
    "    pos = mask_positions[0]  # 最初の [MASK]\n",
    "\n",
    "    # --- 5) 上位K候補を取得（ロジットの topk）\n",
    "    # logits[0, pos] は語彙サイズ |V| のスコアベクトル\n",
    "    topk = torch.topk(logits[0, pos], k=num_topk)\n",
    "    ids_topk = topk.indices.tolist()  # 候補の語彙ID列\n",
    "    scores_topk = topk.values.detach().to(\"cpu\").numpy()  # NumPy に落として返す\n",
    "\n",
    "    # --- 6) ID → 文字列へ復元（decode を優先；空になった場合はトークン文字列をフォールバック）\n",
    "    tokens_topk = []\n",
    "    for tid in ids_topk:\n",
    "        s = tokenizer.decode([tid], skip_special_tokens=True).strip()\n",
    "        if not s:\n",
    "            s = tokenizer.convert_ids_to_tokens(tid).lstrip(\"##\")\n",
    "        tokens_topk.append(s)\n",
    "\n",
    "    # --- 7) 「最初の1箇所のみ」置換：全置換を避けるため split → 1回だけ挿入 → 連結\n",
    "    pre, sep, post = text.partition(mask_tok)  # 最初の [MASK] で三分割\n",
    "    if sep == \"\":\n",
    "        # 理論的にはここに来ない（上で検出済み）が、防御的に処理\n",
    "        return [text], scores_topk\n",
    "\n",
    "    text_topk: List[str] = [pre + tok + post for tok in tokens_topk]\n",
    "\n",
    "    return text_topk, scores_topk\n",
    "\n",
    "\n",
    "# --- 使用例 ---\n",
    "# 例文：最初の [MASK] のみを置換\n",
    "text = \"今日は[MASK]へ行く。\"\n",
    "text_topk, scores_topk = predict_mask_topk(text, tokenizer, bert_mlm, 10)\n",
    "\n",
    "# 可読表示：上位候補を上から順に（logits は単調変換で確率に相当。softmax するなら別途）\n",
    "print(*text_topk, sep=\"\\n\")\n",
    "# （必要なら確率化）例：\n",
    "# import numpy as np\n",
    "# probs = np.exp(scores_topk - scores_topk.max())  # softmax の安定版の一部\n",
    "# probs = probs / probs.sum()\n",
    "# print(\"probs:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd467f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日は、東京へ行く。\n"
     ]
    }
   ],
   "source": [
    "# 5-8\n",
    "# =========================================================\n",
    "# 説明のコメント付き：貪欲法（Greedy）による逐次穴埋め\n",
    "# ---------------------------------------------------------\n",
    "# ■ 目的\n",
    "#   - [MASK] を含む文章に対し、左から順に 1 箇所ずつ最尤候補（top-1, argmax）で置換していき、\n",
    "#     すべての [MASK] を埋めた文を返す。\n",
    "#\n",
    "# ■ 理論メモ\n",
    "#   - MLM（Masked Language Modeling）は「各位置のトークン分布 p(token | context)」を返すが、\n",
    "#     同時に複数の [MASK] を最適化する（joint）わけではない。\n",
    "#   - 本関数は **貪欲（greedy）**：最左の [MASK] を top-1 で即確定 → 文を更新 → 次の [MASK] … を繰り返す。\n",
    "#     - 長所：実装・計算が簡単（前向き推論を #MASK 回まわすだけ）\n",
    "#     - 短所：**順序依存**かつ**局所最適**に陥りやすい（後の候補が前決めに縛られる）\n",
    "#   - 代替案（必要に応じて検討）：\n",
    "#     - **ビーム探索**：上位 b 個の候補で分岐し、スコア合算の高い文を保持\n",
    "#     - **確率的生成**：top-k / top-p（nucleus）サンプリングで多様性を確保\n",
    "#     - **スパン復元型**モデル（T5 など）：複数トークンの欠落（span corruption）に強い\n",
    "#\n",
    "# ■ 実装上の注意\n",
    "#   - `predict_mask_topk` は 1 箇所の [MASK] を対象にする前提。ここでは「最初の 1 箇所」を top-1 で埋める挙動を利用。\n",
    "#   - `tokenizer.mask_token` を使って [MASK] 文字列を特定（モデル依存で文字列が異なる可能性に備える）。\n",
    "#   - ループ回数は **現在のテキスト中の [MASK] 個数**。各反復で文が更新され、次の [MASK] の分布も変わる。\n",
    "#   - まれに decode の結果が空文字/特殊トークンになる場合があるため、`predict_mask_topk` 側でフォールバック処理済み。\n",
    "# =========================================================\n",
    "\n",
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "def greedy_prediction(text: str, tokenizer, bert_mlm) -> str:\n",
    "    \"\"\"\n",
    "    [MASK] を含む文章を入力として、左から順に 1 箇所ずつ最尤候補（top-1）で置換し、\n",
    "    すべての [MASK] を埋めた文章を返す。\n",
    "    \"\"\"\n",
    "    # モデル依存の [MASK] リテラル（例：\"[MASK]\"）\n",
    "    mask_tok = getattr(tokenizer, \"mask_token\", \"[MASK]\")\n",
    "\n",
    "    # 互換性のため：もし text にハードコード \"[MASK]\" が含まれ、mask_tok が異なるなら置換\n",
    "    if mask_tok != \"[MASK]\" and \"[MASK]\" in text:\n",
    "        text = text.replace(\"[MASK]\", mask_tok)\n",
    "\n",
    "    # 現在の文に残っている [MASK] の数だけ繰り返し\n",
    "    num_masks = text.count(mask_tok)\n",
    "    for _ in range(num_masks):\n",
    "        # 1 箇所だけ（最初の [MASK]）を top-1 で置換\n",
    "        filled_list, _scores = predict_mask_topk(text, tokenizer, bert_mlm, num_topk=1)\n",
    "        # predict_mask_topk は常に少なくとも 1 件返す想定（例外時は raise）\n",
    "        new_text = filled_list[0]\n",
    "\n",
    "        # 防御的：もし何らかの理由でテキストが変わらなければ早期終了（無限ループ回避）\n",
    "        if new_text == text:\n",
    "            break\n",
    "\n",
    "        text = new_text\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# --- 使用例（2 マスクを貪欲に埋める） ---\n",
    "text = \"今日は[MASK][MASK]へ行く。\"\n",
    "result = greedy_prediction(text, tokenizer, bert_mlm)\n",
    "print(result)\n",
    "\n",
    "# 参考：\n",
    "# - 生成の質を上げたい場合は、greedy の代わりに top-k / top-p サンプリングやビーム探索を検討。\n",
    "# - 「今日は [MASK] 駅 へ行く」のように右文脈を具体化すると、名詞候補が上がりやすい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c17bd591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日は社会社会的な地位\n"
     ]
    }
   ],
   "source": [
    "# 5-9\n",
    "# =========================================================\n",
    "# 目的：\n",
    "#   - 5 連続の [MASK] を含む文を、貪欲法（greedy）で左から順に 1 トークンずつ埋めて最終文を得る。\n",
    "#\n",
    "# 理論メモ：\n",
    "#   - BERT の MLM は「各位置の 1 トークン」分布 p(token | 文脈) を返す（自己回帰ではない）。\n",
    "#   - 貪欲法は「最左の [MASK] を argmax で即確定 → 文を更新 → 次の [MASK] …」の繰り返し。\n",
    "#     長所：計算が軽い。短所：局所最適に陥りやすく、記号や助詞の“安全牌”が連続しがち。\n",
    "#   - 5 連続 [MASK] は内容語（名詞句）よりも、高頻度の助詞・記号が上位に来やすい（1 トークン最尤の積み重ね）。\n",
    "#   - 品質を上げたい場合：ビーム探索／top-k・top-p サンプリング／名詞フィルタ／右文脈強化（例：「…[MASK][MASK]駅へ行く」）が有効。\n",
    "#\n",
    "# 前提：\n",
    "#   - `predict_mask_topk(text, tokenizer, bert_mlm, k)` と `greedy_prediction(text, tokenizer, bert_mlm)` は前セルで定義済み。\n",
    "#   - `tokenizer` と `bert_mlm` は同一モデル名でロード済み、かつ同一 device（MPS/CUDA/CPU）に配置済み。\n",
    "# =========================================================\n",
    "\n",
    "# 入力：5 連続 [MASK]\n",
    "text = \"今日は[MASK][MASK][MASK][MASK][MASK]\"\n",
    "\n",
    "# 実行：貪欲法で順次埋める\n",
    "result = greedy_prediction(text, tokenizer, bert_mlm)\n",
    "print(result)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# （任意）逐次の埋め替え過程を可視化したい場合は、以下のデバッグ関数を使う：\n",
    "# ---------------------------------------------------------\n",
    "def greedy_prediction_debug(text, tokenizer, bert_mlm):\n",
    "    \"\"\"\n",
    "    貪欲法の各ステップで文を表示するデバッグ版。\n",
    "    \"\"\"\n",
    "    mask_tok = getattr(tokenizer, \"mask_token\", \"[MASK]\")\n",
    "    # 互換：text が \"[MASK]\" リテラルの場合、モデル依存の mask_tok に揃える\n",
    "    if mask_tok != \"[MASK]\" and \"[MASK]\" in text:\n",
    "        text = text.replace(\"[MASK]\", mask_tok)\n",
    "\n",
    "    step = 0\n",
    "    while mask_tok in text:\n",
    "        step += 1\n",
    "        print(f\"# step {step} (before): {text}\")\n",
    "        filled_list, _ = predict_mask_topk(text, tokenizer, bert_mlm, num_topk=1)\n",
    "        text = filled_list[0]\n",
    "        print(f\"# step {step}  (after): {text}\")\n",
    "    return text\n",
    "\n",
    "\n",
    "# デバッグ出力を見たいときに有効化：\n",
    "# _ = greedy_prediction_debug('今日は[MASK][MASK][MASK][MASK][MASK]', tokenizer, bert_mlm)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# （応用）助詞・記号を簡易フィルタして内容語を優先したい場合の例（ヒューリスティック）\n",
    "# ---------------------------------------------------------\n",
    "import re\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "# “名詞っぽさ”の非常に粗いヒューリスティック（厳密には形態素解析を推奨）\n",
    "_PUNCT_OR_PARTICLES = set(\n",
    "    [\n",
    "        \"、\",\n",
    "        \"。\",\n",
    "        \"・\",\n",
    "        \"「\",\n",
    "        \"」\",\n",
    "        \"（\",\n",
    "        \"）\",\n",
    "        \"[\",\n",
    "        \"]\",\n",
    "        \"(\",\n",
    "        \")\",\n",
    "        \"は\",\n",
    "        \"の\",\n",
    "        \"に\",\n",
    "        \"で\",\n",
    "        \"を\",\n",
    "        \"と\",\n",
    "        \"へ\",\n",
    "        \"から\",\n",
    "        \"より\",\n",
    "        \"や\",\n",
    "        \"も\",\n",
    "        \"が\",\n",
    "    ]\n",
    ")\n",
    "_JA_WORD_PATTERN = re.compile(\n",
    "    r\"^[\\u3040-\\u30ff\\u3400-\\u9fffA-Za-z0-9ー・]+$\"\n",
    ")  # 仮名・漢字・英数・長音・中黒\n",
    "\n",
    "\n",
    "def predict_mask_topk_filtered(\n",
    "    text: str, tokenizer, bert_mlm, num_topk: int, k_expand: int = 50\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    predict_mask_topk の派生：top-k を広めに取ってから（k_expand）、助詞・記号を粗く除外し、\n",
    "    上位 num_topk を返す。完全ではないが内容語が出やすくなることがある。\n",
    "    \"\"\"\n",
    "    enc = tokenizer(text, return_tensors=\"pt\")\n",
    "    device = next(bert_mlm.parameters()).device\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    bert_mlm.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = bert_mlm(**enc).logits\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    mask_id = tokenizer.mask_token_id\n",
    "    mask_tok = tokenizer.mask_token\n",
    "\n",
    "    pos_list = (input_ids[0] == mask_id).nonzero(as_tuple=True)[0].tolist()\n",
    "    if not pos_list:\n",
    "        return [text]\n",
    "    pos = pos_list[0]\n",
    "\n",
    "    # 広めの top-k を取り、その中からフィルタで間引く\n",
    "    k = max(num_topk, k_expand)\n",
    "    cand_ids = torch.topk(logits[0, pos], k=k).indices.tolist()\n",
    "\n",
    "    def _token_str(tid: int) -> str:\n",
    "        s = tokenizer.decode([tid], skip_special_tokens=True).strip()\n",
    "        return s if s else tokenizer.convert_ids_to_tokens(tid).lstrip(\"##\")\n",
    "\n",
    "    cands = []\n",
    "    for tid in cand_ids:\n",
    "        s = _token_str(tid)\n",
    "        # 粗フィルタ：助詞・記号っぽいものを除外\n",
    "        if (s in _PUNCT_OR_PARTICLES) or (not _JA_WORD_PATTERN.match(s)):\n",
    "            continue\n",
    "        cands.append(s)\n",
    "        if len(cands) >= num_topk:\n",
    "            break\n",
    "\n",
    "    if not cands:\n",
    "        # すべて弾かれたら元の top-1 を返す（安全弁）\n",
    "        cands = [_token_str(cand_ids[0])]\n",
    "\n",
    "    pre, sep, post = text.partition(mask_tok)\n",
    "    return [pre + c + post for c in cands]\n",
    "\n",
    "\n",
    "# 使い方（任意）：\n",
    "# text = '今日は[MASK][MASK][MASK][MASK][MASK]'\n",
    "# t, _ = predict_mask_topk(text, tokenizer, bert_mlm, 1)  # まず 1 個目\n",
    "# text = t[0]\n",
    "# text = predict_mask_topk_filtered(text, tokenizer, bert_mlm, num_topk=1)[0]  # フィルタ版で次を埋める …といった併用も可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e406a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
