{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-1\n",
    "!mkdir chap9\n",
    "%cd ./chap9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3601a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-3\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 日本語の事前学習済みモデル\n",
    "MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ecfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-4\n",
    "# =============================================================================\n",
    "# SC_tokenizer（誤変換補正用トークナイザ）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的（理論）：\n",
    "# - 「誤変換を含む文」を入力 x とし、「正しい文」をラベル y（語彙上のトークンID列）として\n",
    "#   学習することで、**各位置ごとに“正しいトークンID”を分類**するタスクへ還元する。\n",
    "# - これは BERT をエンコーダとし、出力ヘッドが **語彙サイズ |V|** のクラス分類を\n",
    "#   各タイムステップに対して行う設計（トークン分類）に相当する。\n",
    "#   * 損失関数の典型：位置 t ごとに CrossEntropyLoss（教師は正解トークンID）。\n",
    "#   * BERT の “Masked LM” と違い、ここでは**全面的に教師強制**で全位置を監督できる（PAD を除外するなら ignore_index を活用）。\n",
    "# - この形式は **置換型の誤り**（打鍵ミス・漢字/かな揺れ・全半角揺れ等）に強いが、\n",
    "#   **挿入/削除**や**語順入れ替え**などの編集距離を伴う変化は「位置合わせ」の前提を崩すため苦手。\n",
    "#   必要に応じてアライメント戦略（例：差分アルゴリズムでの整列→同一長に拡張）やシーケンス生成系（seq2seq）を検討する。\n",
    "# - 推論時は各位置の予測トークンIDから文字列を復号し、元文の空白を保持しつつ**スパン単位で置換**して復元する。\n",
    "#   サブワード（WordPiece）の '##' 接頭辞は接続時に除去する。\n",
    "#\n",
    "# 実装上の注意：\n",
    "# - encode_plus_tagged では、正解文の input_ids をそのまま labels に格納する。\n",
    "#   学習ループ側で特殊トークンや PAD を損失に含めない場合、labels の該当位置を **-100** に置換しておく（ignore_index）。\n",
    "# - encode_plus_untagged では、WordPiece の各サブワードを元文字列の部分スパンにマッピングする。\n",
    "#   繰り返し部分や空白が多い場合、現在の **前方貪欲マッチ**は誤対応のリスクがある（必要なら正規表現や LCS によるロバスト化を検討）。\n",
    "# - convert_bert_output_to_text では NFKC 正規化を適用している。\n",
    "#   学習・推論で正規化を**一貫**させないと、スパンの境界と生成表記の齟齬を招く可能性がある。\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class SC_tokenizer(BertJapaneseTokenizer):\n",
    "\n",
    "    def encode_plus_tagged(self, wrong_text, correct_text, max_length=128):\n",
    "        \"\"\"\n",
    "        ファインチューニング時に使用。\n",
    "        誤変換を含む文章と正しい文章を入力とし、\n",
    "        符号化を行いBERTに入力できる形式にする。\n",
    "\n",
    "        理論メモ：\n",
    "        - 入力 x = wrong_text のトークン列に対し、教師 y = correct_text のトークンID列を\n",
    "          そのまま labels として与えることで、各位置の正解トークンを分類するタスクにする。\n",
    "        - モデル側は「トークン分類ヘッド（出力次元＝語彙サイズ |V|）」を持ち、\n",
    "          CrossEntropyLoss(logits_t, label_id_t) を位置 t で計算し、時系列平均を取るのが一般的。\n",
    "        - PAD/CLS/SEP を損失から除外したい場合、labels の該当位置を -100 にする（ignore_index）。\n",
    "        \"\"\"\n",
    "        # 誤変換した文章をトークン化し、符号化\n",
    "        encoding = self(\n",
    "            wrong_text, max_length=max_length, padding=\"max_length\", truncation=True\n",
    "        )\n",
    "        # 正しい文章をトークン化し、符号化\n",
    "        encoding_correct = self(\n",
    "            correct_text, max_length=max_length, padding=\"max_length\", truncation=True\n",
    "        )\n",
    "        # 正しい文章の符号をラベルとする\n",
    "        # 注意：このままだと [CLS]/[SEP]/[PAD] も学習対象に含まれる。\n",
    "        # もし PAD を損失から除きたいなら、学習側で labels を -100 に置換する処理を追加する。\n",
    "        encoding[\"labels\"] = encoding_correct[\"input_ids\"]\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def encode_plus_untagged(self, text, max_length=None, return_tensors=None):\n",
    "        \"\"\"\n",
    "        文章を符号化し、それぞれのトークンの文章中の位置も特定しておく。\n",
    "\n",
    "        理論メモ：\n",
    "        - 後段の復号（convert_bert_output_to_text）では、各サブワードが\n",
    "          原文テキストのどのスパンに対応するかが必要になる。\n",
    "        - ここでは形態素（MeCab）→サブワード（WordPiece）→原文スパンという対応を\n",
    "          前方探索で求めている。繰り返しや空白が多い場合は曖昧性が残る点に注意。\n",
    "        \"\"\"\n",
    "        # 文章のトークン化を行い、\n",
    "        # それぞれのトークンと文章中の文字列を対応づける。\n",
    "        tokens = []  # トークンを追加していく。\n",
    "        tokens_original = []  # トークンに対応する文章中の文字列を追加していく。\n",
    "        words = self.word_tokenizer.tokenize(text)  # MeCabで単語に分割\n",
    "        for word in words:\n",
    "            # 単語をサブワードに分割\n",
    "            tokens_word = self.subword_tokenizer.tokenize(word)\n",
    "            tokens.extend(tokens_word)\n",
    "            if tokens_word[0] == \"[UNK]\":  # 未知語への対応\n",
    "                tokens_original.append(word)\n",
    "            else:\n",
    "                tokens_original.extend(\n",
    "                    [token.replace(\"##\", \"\") for token in tokens_word]\n",
    "                )\n",
    "\n",
    "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
    "        # アルゴリズム：前方貪欲一致（最短一致ではなく、順次スライド）\n",
    "        # 注意：同一サブ文字列が繰り返される場合に誤対応のリスクがある。\n",
    "        position = 0\n",
    "        spans = []  # トークンの位置を追加していく。\n",
    "        for token in tokens_original:\n",
    "            l = len(token)\n",
    "            while 1:\n",
    "                if token != text[position : position + l]:\n",
    "                    position += 1\n",
    "                else:\n",
    "                    spans.append([position, position + l])\n",
    "                    position += l\n",
    "                    break\n",
    "\n",
    "        # 符号化を行いBERTに入力できる形式にする。\n",
    "        # prepare_for_model が [CLS]/[SEP] の付与と長さ正規化（padding/truncation）を行う。\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\" if max_length else False,\n",
    "            truncation=True if max_length else False,\n",
    "        )\n",
    "        sequence_length = len(encoding[\"input_ids\"])\n",
    "        # 特殊トークン[CLS]に対するダミーのspanを追加（-1 は「実体なし」の番兵）。\n",
    "        spans = [[-1, -1]] + spans[: sequence_length - 2]\n",
    "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
    "        spans = spans + [[-1, -1]] * (sequence_length - len(spans))\n",
    "\n",
    "        # 必要に応じてtorch.Tensorにする。\n",
    "        if return_tensors == \"pt\":\n",
    "            encoding = {k: torch.tensor([v]) for k, v in encoding.items()}\n",
    "\n",
    "        return encoding, spans\n",
    "\n",
    "    def convert_bert_output_to_text(self, text, labels, spans):\n",
    "        \"\"\"\n",
    "        推論時に使用。\n",
    "        文章と、各トークンのラベルの予測値、文章中での位置を入力とする。\n",
    "        そこから、BERTによって予測された文章に変換。\n",
    "\n",
    "        理論メモ：\n",
    "        - labels は「各位置の語彙ID（予測トークン）」と想定。すなわちモデル出力は\n",
    "          各位置で |V| 分類を行うトークン分類ヘッドの argmax 等。\n",
    "        - サブワード '##' は接続時に除去し、文字列として連結。\n",
    "        - 本実装は NFKC 正規化を通すため、**学習・評価で NFKC を統一**しないと\n",
    "          スパン境界と生成表記のずれが起き得る（評価の完全一致が過小になる可能性）。\n",
    "        \"\"\"\n",
    "        assert len(spans) == len(labels)\n",
    "\n",
    "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
    "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
    "        spans = [span for span in spans if span[0] != -1]\n",
    "\n",
    "        # BERTが予測した文章を作成\n",
    "        # 原文の空白や未対称な部分は、スパンの「穴」をそのまま複製して保持。\n",
    "        predicted_text = \"\"\n",
    "        position = 0\n",
    "        for label, span in zip(labels, spans):\n",
    "            start, end = span\n",
    "            if position != start:  # 空白の処理（原文の未対応部分をそのままコピー）\n",
    "                predicted_text += text[position:start]\n",
    "            predicted_token = self.convert_ids_to_tokens(label)\n",
    "            predicted_token = predicted_token.replace(\"##\", \"\")\n",
    "            predicted_token = unicodedata.normalize(\"NFKC\", predicted_token)\n",
    "            predicted_text += predicted_token\n",
    "            position = end\n",
    "\n",
    "        return predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a08e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'SC_tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 9-5\n",
    "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a503cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 759, 18204, 11, 4618, 15, 10, 3, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], 'labels': [2, 759, 18204, 11, 8274, 15, 10, 3, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# 9-6\n",
    "# =============================================================================\n",
    "# 目的（理論）：\n",
    "# - 「誤変換を含む文（wrong_text）」をモデル入力とし、「正しい文（correct_text）」の\n",
    "#   トークンID列を **教師ラベル（labels）** として与える最小例。\n",
    "# - これにより各位置 t で 「正しいトークンID」を分類する **トークン分類問題** として学習できる。\n",
    "#   * 推奨損失：CrossEntropyLoss（語彙サイズ |V| クラスの多クラス分類、PAD 等は ignore_index=-100 で除外）\n",
    "# - 注意：max_length を超えると **切り詰め（truncation）** されるため、wrong と correct の整合が崩れる。\n",
    "#   学習で両系列が等長にパディング・切り詰めされること（同じ max_length/手順）を必ず保証すること。\n",
    "# - 出力の encoding には、wrong_text の BERT 入力（input_ids/attention_mask/token_type_ids）に加えて、\n",
    "#   correct_text の input_ids が **encoding['labels']** として格納される実装（9-4 の SC_tokenizer 参照）。\n",
    "# =============================================================================\n",
    "\n",
    "wrong_text = \"優勝トロフィーを変換した\"  # 誤変換を含む入力（例：返還→変換）\n",
    "correct_text = \"優勝トロフィーを返還した\"  # 正しい表記\n",
    "encoding = tokenizer.encode_plus_tagged(wrong_text, correct_text, max_length=12)\n",
    "print(encoding)\n",
    "# 期待される中身（例）：\n",
    "#  - 'input_ids'       : wrong_text をトークナイズ→ID化→[CLS]/[SEP] 付与→max_length へパディング/切り詰め\n",
    "#  - 'token_type_ids'  : 単文なので通常は全 0\n",
    "#  - 'attention_mask'  : 実トークン 1 / PAD 0\n",
    "#  - 'labels'          : correct_text の input_ids（= 教師）。学習側で PAD/特殊トークンに -100 を適用推奨。\n",
    "# 理論メモ：\n",
    "#  - 本手法は「置換型誤り」には強い一方、挿入/削除や語順入れ替え等の編集距離を伴う誤りには弱い。\n",
    "#    その場合は、アライメント（diff/LCS）で整列して等長化するか、seq2seq 系（エンコーダ–デコーダ）を検討する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0cfd808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# encoding\n",
      "{'input_ids': tensor([[    2,   759, 18204,    11,  4618,    15,    10,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "# spans\n",
      "[[-1, -1], [0, 2], [2, 7], [7, 8], [8, 10], [10, 11], [11, 12], [-1, -1]]\n"
     ]
    }
   ],
   "source": [
    "# 9-7\n",
    "# =============================================================================\n",
    "# 目的（理論）：\n",
    "# - 学習/推論時に、原文テキストの各サブワード（WordPiece）が「元テキスト中のどの位置（span）」に\n",
    "#   対応するかを取得する最小例である。これにより、トークン分類の予測結果（各位置の語彙ID）を\n",
    "#   文字列へ正確に復号（置換）できる。\n",
    "#\n",
    "# 背景：\n",
    "# - encode_plus_untagged は、形態素分割（MeCab）→サブワード分割（WordPiece）の後、\n",
    "#   各サブワードから '##' を除いた素片を原文に前方探索でアラインし、[start, end) の半開区間で\n",
    "#   スパン列（spans）を構築する。\n",
    "# - prepare_for_model により [CLS]/[SEP] の付与とパディング/切り詰めが行われるため、\n",
    "#   それら特殊トークンや PAD に対応する spans は [-1, -1] の番兵を入れておく（「実体なし」の意味）。\n",
    "#\n",
    "# 出力の読み方：\n",
    "# - encoding: dict（'input_ids', 'token_type_ids', 'attention_mask' など）。return_tensors='pt' のため\n",
    "#   各値は形状 [1, T] の torch.Tensor（バッチ次元 1 を含む）。\n",
    "# - spans: 長さ T のリスト。各要素は [start, end]（原文の0始まりインデックス）。\n",
    "#   * special/PAD の位置は [-1, -1]。\n",
    "#   * 先頭は [CLS] のため必ず [-1, -1]、終端側も [SEP] と PAD による [-1, -1] が並ぶ。\n",
    "#\n",
    "# 注意点：\n",
    "# - 前方貪欲一致は、同一部分文字列が繰り返される文や空白が多い文で誤マッチのリスクがある。\n",
    "#   必要に応じて LCS/編集距離によるアライメントでロバスト化を検討。\n",
    "# - 学習・推論で NFKC 正規化ポリシーを統一しないと、スパン境界と生成表記の差異が生じ得る。\n",
    "# =============================================================================\n",
    "\n",
    "wrong_text = \"優勝トロフィーを変換した\"\n",
    "encoding, spans = tokenizer.encode_plus_untagged(wrong_text, return_tensors=\"pt\")\n",
    "print(\"# encoding\")\n",
    "print(encoding)\n",
    "print(\"# spans\")\n",
    "print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0a5d545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "優勝トロフィーを返還した\n"
     ]
    }
   ],
   "source": [
    "# 9-8\n",
    "# =============================================================================\n",
    "# 目的（理論）：\n",
    "# - 9-7 で得た spans（原文中のサブワード位置列）と、各位置の **予測トークンID列（predicted_labels）** を\n",
    "#   用いて、SC_tokenizer.convert_bert_output_to_text により「正しい文章」を復元する最小例。\n",
    "#\n",
    "# 処理の流れ（convert_bert_output_to_text の要点）：\n",
    "#  1) 事前条件：len(spans) == len(labels)（ここで labels=predicted_labels）。\n",
    "#     - spans は [CLS], 本文サブワード, [SEP]（および PAD があれば PAD）に対応し、特殊/PAD は [-1, -1]。\n",
    "#     - ラベル列も **同じ長さ**で、対応位置に語彙ID（token_id）を持つこと。\n",
    "#  2) 特殊/PAD 位置の除去：\n",
    "#     - span[0] == -1 の要素は「実体なし」として、labels/spans の両方から落とす。\n",
    "#  3) サブワード復元：\n",
    "#     - token_id → token へ変換し、WordPiece の接頭辞 '##' を除去して連結。\n",
    "#     - 連結前に **NFKC 正規化**で全/半角・合成文字の揺れを矯正（学習/推論での正規化ポリシーは統一すること）。\n",
    "#  4) 空白・非対象領域の保持：\n",
    "#     - 連結中、原文の position と次のサブワード開始 start にギャップがあれば、原文の該当部分をそのまま複写。\n",
    "#\n",
    "# 理論メモ：\n",
    "# - ここでの labels は「語彙サイズ |V| クラスの **トークン分類** の予測結果（argmax など）」を想定。\n",
    "# - 位置合わせ（spans）は 9-7 の encode_plus_untagged で算出（MeCab→WordPiece→前方探索）。\n",
    "#   反復文字列や空白が多い場合は前方貪欲一致が誤対応するリスクがあるため、大規模実運用では LCS/編集距離による\n",
    "#   アライメントの頑健化を検討。\n",
    "# - 予測列の長さが spans と一致しないと assert により失敗する。モデル出力 → ラベル化の段で\n",
    "#   [CLS]/[SEP]/PAD を含んだ長さ合わせを行うこと。\n",
    "# =============================================================================\n",
    "\n",
    "predicted_labels = [\n",
    "    2,\n",
    "    759,\n",
    "    18204,\n",
    "    11,\n",
    "    8274,\n",
    "    15,\n",
    "    10,\n",
    "    3,\n",
    "]  # 例：各位置の予測 token_id（語彙ID）\n",
    "predicted_text = tokenizer.convert_bert_output_to_text(\n",
    "    wrong_text, predicted_labels, spans\n",
    ")\n",
    "print(predicted_text)\n",
    "# 出力想定：\n",
    "# - モデルが '変換'→'返還' のような誤変換を修正できていれば、期待する正しい表記に復元される。\n",
    "# - 語彙外/未知などで難しい場合は近傍のサブワード列に置換される可能性がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3c3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
