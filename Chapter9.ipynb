{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-1\n",
    "!mkdir chap9\n",
    "%cd ./chap9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3601a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-3\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 日本語の事前学習済みモデル\n",
    "MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ecfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-4\n",
    "# =============================================================================\n",
    "# SC_tokenizer（誤変換補正用トークナイザ）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的（理論）：\n",
    "# - 「誤変換を含む文」を入力 x とし、「正しい文」をラベル y（語彙上のトークンID列）として\n",
    "#   学習することで、**各位置ごとに“正しいトークンID”を分類**するタスクへ還元する。\n",
    "# - これは BERT をエンコーダとし、出力ヘッドが **語彙サイズ |V|** のクラス分類を\n",
    "#   各タイムステップに対して行う設計（トークン分類）に相当する。\n",
    "#   * 損失関数の典型：位置 t ごとに CrossEntropyLoss（教師は正解トークンID）。\n",
    "#   * BERT の “Masked LM” と違い、ここでは**全面的に教師強制**で全位置を監督できる（PAD を除外するなら ignore_index を活用）。\n",
    "# - この形式は **置換型の誤り**（打鍵ミス・漢字/かな揺れ・全半角揺れ等）に強いが、\n",
    "#   **挿入/削除**や**語順入れ替え**などの編集距離を伴う変化は「位置合わせ」の前提を崩すため苦手。\n",
    "#   必要に応じてアライメント戦略（例：差分アルゴリズムでの整列→同一長に拡張）やシーケンス生成系（seq2seq）を検討する。\n",
    "# - 推論時は各位置の予測トークンIDから文字列を復号し、元文の空白を保持しつつ**スパン単位で置換**して復元する。\n",
    "#   サブワード（WordPiece）の '##' 接頭辞は接続時に除去する。\n",
    "#\n",
    "# 実装上の注意：\n",
    "# - encode_plus_tagged では、正解文の input_ids をそのまま labels に格納する。\n",
    "#   学習ループ側で特殊トークンや PAD を損失に含めない場合、labels の該当位置を **-100** に置換しておく（ignore_index）。\n",
    "# - encode_plus_untagged では、WordPiece の各サブワードを元文字列の部分スパンにマッピングする。\n",
    "#   繰り返し部分や空白が多い場合、現在の **前方貪欲マッチ**は誤対応のリスクがある（必要なら正規表現や LCS によるロバスト化を検討）。\n",
    "# - convert_bert_output_to_text では NFKC 正規化を適用している。\n",
    "#   学習・推論で正規化を**一貫**させないと、スパンの境界と生成表記の齟齬を招く可能性がある。\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class SC_tokenizer(BertJapaneseTokenizer):\n",
    "\n",
    "    def encode_plus_tagged(self, wrong_text, correct_text, max_length=128):\n",
    "        \"\"\"\n",
    "        ファインチューニング時に使用。\n",
    "        誤変換を含む文章と正しい文章を入力とし、\n",
    "        符号化を行いBERTに入力できる形式にする。\n",
    "\n",
    "        理論メモ：\n",
    "        - 入力 x = wrong_text のトークン列に対し、教師 y = correct_text のトークンID列を\n",
    "          そのまま labels として与えることで、各位置の正解トークンを分類するタスクにする。\n",
    "        - モデル側は「トークン分類ヘッド（出力次元＝語彙サイズ |V|）」を持ち、\n",
    "          CrossEntropyLoss(logits_t, label_id_t) を位置 t で計算し、時系列平均を取るのが一般的。\n",
    "        - PAD/CLS/SEP を損失から除外したい場合、labels の該当位置を -100 にする（ignore_index）。\n",
    "        \"\"\"\n",
    "        # 誤変換した文章をトークン化し、符号化\n",
    "        encoding = self(\n",
    "            wrong_text, max_length=max_length, padding=\"max_length\", truncation=True\n",
    "        )\n",
    "        # 正しい文章をトークン化し、符号化\n",
    "        encoding_correct = self(\n",
    "            correct_text, max_length=max_length, padding=\"max_length\", truncation=True\n",
    "        )\n",
    "        # 正しい文章の符号をラベルとする\n",
    "        # 注意：このままだと [CLS]/[SEP]/[PAD] も学習対象に含まれる。\n",
    "        # もし PAD を損失から除きたいなら、学習側で labels を -100 に置換する処理を追加する。\n",
    "        encoding[\"labels\"] = encoding_correct[\"input_ids\"]\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def encode_plus_untagged(self, text, max_length=None, return_tensors=None):\n",
    "        \"\"\"\n",
    "        文章を符号化し、それぞれのトークンの文章中の位置も特定しておく。\n",
    "\n",
    "        理論メモ：\n",
    "        - 後段の復号（convert_bert_output_to_text）では、各サブワードが\n",
    "          原文テキストのどのスパンに対応するかが必要になる。\n",
    "        - ここでは形態素（MeCab）→サブワード（WordPiece）→原文スパンという対応を\n",
    "          前方探索で求めている。繰り返しや空白が多い場合は曖昧性が残る点に注意。\n",
    "        \"\"\"\n",
    "        # 文章のトークン化を行い、\n",
    "        # それぞれのトークンと文章中の文字列を対応づける。\n",
    "        tokens = []  # トークンを追加していく。\n",
    "        tokens_original = []  # トークンに対応する文章中の文字列を追加していく。\n",
    "        words = self.word_tokenizer.tokenize(text)  # MeCabで単語に分割\n",
    "        for word in words:\n",
    "            # 単語をサブワードに分割\n",
    "            tokens_word = self.subword_tokenizer.tokenize(word)\n",
    "            tokens.extend(tokens_word)\n",
    "            if tokens_word[0] == \"[UNK]\":  # 未知語への対応\n",
    "                tokens_original.append(word)\n",
    "            else:\n",
    "                tokens_original.extend(\n",
    "                    [token.replace(\"##\", \"\") for token in tokens_word]\n",
    "                )\n",
    "\n",
    "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
    "        # アルゴリズム：前方貪欲一致（最短一致ではなく、順次スライド）\n",
    "        # 注意：同一サブ文字列が繰り返される場合に誤対応のリスクがある。\n",
    "        position = 0\n",
    "        spans = []  # トークンの位置を追加していく。\n",
    "        for token in tokens_original:\n",
    "            l = len(token)\n",
    "            while 1:\n",
    "                if token != text[position : position + l]:\n",
    "                    position += 1\n",
    "                else:\n",
    "                    spans.append([position, position + l])\n",
    "                    position += l\n",
    "                    break\n",
    "\n",
    "        # 符号化を行いBERTに入力できる形式にする。\n",
    "        # prepare_for_model が [CLS]/[SEP] の付与と長さ正規化（padding/truncation）を行う。\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\" if max_length else False,\n",
    "            truncation=True if max_length else False,\n",
    "        )\n",
    "        sequence_length = len(encoding[\"input_ids\"])\n",
    "        # 特殊トークン[CLS]に対するダミーのspanを追加（-1 は「実体なし」の番兵）。\n",
    "        spans = [[-1, -1]] + spans[: sequence_length - 2]\n",
    "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
    "        spans = spans + [[-1, -1]] * (sequence_length - len(spans))\n",
    "\n",
    "        # 必要に応じてtorch.Tensorにする。\n",
    "        if return_tensors == \"pt\":\n",
    "            encoding = {k: torch.tensor([v]) for k, v in encoding.items()}\n",
    "\n",
    "        return encoding, spans\n",
    "\n",
    "    def convert_bert_output_to_text(self, text, labels, spans):\n",
    "        \"\"\"\n",
    "        推論時に使用。\n",
    "        文章と、各トークンのラベルの予測値、文章中での位置を入力とする。\n",
    "        そこから、BERTによって予測された文章に変換。\n",
    "\n",
    "        理論メモ：\n",
    "        - labels は「各位置の語彙ID（予測トークン）」と想定。すなわちモデル出力は\n",
    "          各位置で |V| 分類を行うトークン分類ヘッドの argmax 等。\n",
    "        - サブワード '##' は接続時に除去し、文字列として連結。\n",
    "        - 本実装は NFKC 正規化を通すため、**学習・評価で NFKC を統一**しないと\n",
    "          スパン境界と生成表記のずれが起き得る（評価の完全一致が過小になる可能性）。\n",
    "        \"\"\"\n",
    "        assert len(spans) == len(labels)\n",
    "\n",
    "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
    "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
    "        spans = [span for span in spans if span[0] != -1]\n",
    "\n",
    "        # BERTが予測した文章を作成\n",
    "        # 原文の空白や未対称な部分は、スパンの「穴」をそのまま複製して保持。\n",
    "        predicted_text = \"\"\n",
    "        position = 0\n",
    "        for label, span in zip(labels, spans):\n",
    "            start, end = span\n",
    "            if position != start:  # 空白の処理（原文の未対応部分をそのままコピー）\n",
    "                predicted_text += text[position:start]\n",
    "            predicted_token = self.convert_ids_to_tokens(label)\n",
    "            predicted_token = predicted_token.replace(\"##\", \"\")\n",
    "            predicted_token = unicodedata.normalize(\"NFKC\", predicted_token)\n",
    "            predicted_text += predicted_token\n",
    "            position = end\n",
    "\n",
    "        return predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a08e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'SC_tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 9-5\n",
    "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a503cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 759, 18204, 11, 4618, 15, 10, 3, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], 'labels': [2, 759, 18204, 11, 8274, 15, 10, 3, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# 9-6\n",
    "# =============================================================================\n",
    "# 目的（理論）：\n",
    "# - 「誤変換を含む文（wrong_text）」をモデル入力とし、「正しい文（correct_text）」の\n",
    "#   トークンID列を **教師ラベル（labels）** として与える最小例。\n",
    "# - これにより各位置 t で 「正しいトークンID」を分類する **トークン分類問題** として学習できる。\n",
    "#   * 推奨損失：CrossEntropyLoss（語彙サイズ |V| クラスの多クラス分類、PAD 等は ignore_index=-100 で除外）\n",
    "# - 注意：max_length を超えると **切り詰め（truncation）** されるため、wrong と correct の整合が崩れる。\n",
    "#   学習で両系列が等長にパディング・切り詰めされること（同じ max_length/手順）を必ず保証すること。\n",
    "# - 出力の encoding には、wrong_text の BERT 入力（input_ids/attention_mask/token_type_ids）に加えて、\n",
    "#   correct_text の input_ids が **encoding['labels']** として格納される実装（9-4 の SC_tokenizer 参照）。\n",
    "# =============================================================================\n",
    "\n",
    "wrong_text = \"優勝トロフィーを変換した\"  # 誤変換を含む入力（例：返還→変換）\n",
    "correct_text = \"優勝トロフィーを返還した\"  # 正しい表記\n",
    "encoding = tokenizer.encode_plus_tagged(wrong_text, correct_text, max_length=12)\n",
    "print(encoding)\n",
    "# 期待される中身（例）：\n",
    "#  - 'input_ids'       : wrong_text をトークナイズ→ID化→[CLS]/[SEP] 付与→max_length へパディング/切り詰め\n",
    "#  - 'token_type_ids'  : 単文なので通常は全 0\n",
    "#  - 'attention_mask'  : 実トークン 1 / PAD 0\n",
    "#  - 'labels'          : correct_text の input_ids（= 教師）。学習側で PAD/特殊トークンに -100 を適用推奨。\n",
    "# 理論メモ：\n",
    "#  - 本手法は「置換型誤り」には強い一方、挿入/削除や語順入れ替え等の編集距離を伴う誤りには弱い。\n",
    "#    その場合は、アライメント（diff/LCS）で整列して等長化するか、seq2seq 系（エンコーダ–デコーダ）を検討する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0cfd808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# encoding\n",
      "{'input_ids': tensor([[    2,   759, 18204,    11,  4618,    15,    10,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "# spans\n",
      "[[-1, -1], [0, 2], [2, 7], [7, 8], [8, 10], [10, 11], [11, 12], [-1, -1]]\n"
     ]
    }
   ],
   "source": [
    "# 9-7\n",
    "# =============================================================================\n",
    "# 目的（理論）：\n",
    "# - 学習/推論時に、原文テキストの各サブワード（WordPiece）が「元テキスト中のどの位置（span）」に\n",
    "#   対応するかを取得する最小例である。これにより、トークン分類の予測結果（各位置の語彙ID）を\n",
    "#   文字列へ正確に復号（置換）できる。\n",
    "#\n",
    "# 背景：\n",
    "# - encode_plus_untagged は、形態素分割（MeCab）→サブワード分割（WordPiece）の後、\n",
    "#   各サブワードから '##' を除いた素片を原文に前方探索でアラインし、[start, end) の半開区間で\n",
    "#   スパン列（spans）を構築する。\n",
    "# - prepare_for_model により [CLS]/[SEP] の付与とパディング/切り詰めが行われるため、\n",
    "#   それら特殊トークンや PAD に対応する spans は [-1, -1] の番兵を入れておく（「実体なし」の意味）。\n",
    "#\n",
    "# 出力の読み方：\n",
    "# - encoding: dict（'input_ids', 'token_type_ids', 'attention_mask' など）。return_tensors='pt' のため\n",
    "#   各値は形状 [1, T] の torch.Tensor（バッチ次元 1 を含む）。\n",
    "# - spans: 長さ T のリスト。各要素は [start, end]（原文の0始まりインデックス）。\n",
    "#   * special/PAD の位置は [-1, -1]。\n",
    "#   * 先頭は [CLS] のため必ず [-1, -1]、終端側も [SEP] と PAD による [-1, -1] が並ぶ。\n",
    "#\n",
    "# 注意点：\n",
    "# - 前方貪欲一致は、同一部分文字列が繰り返される文や空白が多い文で誤マッチのリスクがある。\n",
    "#   必要に応じて LCS/編集距離によるアライメントでロバスト化を検討。\n",
    "# - 学習・推論で NFKC 正規化ポリシーを統一しないと、スパン境界と生成表記の差異が生じ得る。\n",
    "# =============================================================================\n",
    "\n",
    "wrong_text = \"優勝トロフィーを変換した\"\n",
    "encoding, spans = tokenizer.encode_plus_untagged(wrong_text, return_tensors=\"pt\")\n",
    "print(\"# encoding\")\n",
    "print(encoding)\n",
    "print(\"# spans\")\n",
    "print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0a5d545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "優勝トロフィーを返還した\n"
     ]
    }
   ],
   "source": [
    "# 9-8\n",
    "# =============================================================================\n",
    "# 目的（理論）：\n",
    "# - 9-7 で得た spans（原文中のサブワード位置列）と、各位置の **予測トークンID列（predicted_labels）** を\n",
    "#   用いて、SC_tokenizer.convert_bert_output_to_text により「正しい文章」を復元する最小例。\n",
    "#\n",
    "# 処理の流れ（convert_bert_output_to_text の要点）：\n",
    "#  1) 事前条件：len(spans) == len(labels)（ここで labels=predicted_labels）。\n",
    "#     - spans は [CLS], 本文サブワード, [SEP]（および PAD があれば PAD）に対応し、特殊/PAD は [-1, -1]。\n",
    "#     - ラベル列も **同じ長さ**で、対応位置に語彙ID（token_id）を持つこと。\n",
    "#  2) 特殊/PAD 位置の除去：\n",
    "#     - span[0] == -1 の要素は「実体なし」として、labels/spans の両方から落とす。\n",
    "#  3) サブワード復元：\n",
    "#     - token_id → token へ変換し、WordPiece の接頭辞 '##' を除去して連結。\n",
    "#     - 連結前に **NFKC 正規化**で全/半角・合成文字の揺れを矯正（学習/推論での正規化ポリシーは統一すること）。\n",
    "#  4) 空白・非対象領域の保持：\n",
    "#     - 連結中、原文の position と次のサブワード開始 start にギャップがあれば、原文の該当部分をそのまま複写。\n",
    "#\n",
    "# 理論メモ：\n",
    "# - ここでの labels は「語彙サイズ |V| クラスの **トークン分類** の予測結果（argmax など）」を想定。\n",
    "# - 位置合わせ（spans）は 9-7 の encode_plus_untagged で算出（MeCab→WordPiece→前方探索）。\n",
    "#   反復文字列や空白が多い場合は前方貪欲一致が誤対応するリスクがあるため、大規模実運用では LCS/編集距離による\n",
    "#   アライメントの頑健化を検討。\n",
    "# - 予測列の長さが spans と一致しないと assert により失敗する。モデル出力 → ラベル化の段で\n",
    "#   [CLS]/[SEP]/PAD を含んだ長さ合わせを行うこと。\n",
    "# =============================================================================\n",
    "\n",
    "predicted_labels = [\n",
    "    2,\n",
    "    759,\n",
    "    18204,\n",
    "    11,\n",
    "    8274,\n",
    "    15,\n",
    "    10,\n",
    "    3,\n",
    "]  # 例：各位置の予測 token_id（語彙ID）\n",
    "predicted_text = tokenizer.convert_bert_output_to_text(\n",
    "    wrong_text, predicted_labels, spans\n",
    ")\n",
    "print(predicted_text)\n",
    "# 出力想定：\n",
    "# - モデルが '変換'→'返還' のような誤変換を修正できていれば、期待する正しい表記に復元される。\n",
    "# - 語彙外/未知などで難しい場合は近傍のサブワード列に置換される可能性がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f3c3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] bert_mlm loaded on: mps\n",
      "# input :  優勝トロフィーを変換した。\n",
      "# output:  優勝トロフィーを獲得した。\n"
     ]
    }
   ],
   "source": [
    "# 9-9 / 9-10 の CUDA エラー修正（Mac/MPS・CPU でも動く汎用化）\n",
    "# =============================================================================\n",
    "# エラー原因：\n",
    "# - 環境の PyTorch が「CUDA 非対応」でビルドされているため、`.cuda()` 呼び出しで失敗。\n",
    "#   → Apple Silicon(Mac) は通常 CUDA が無いので、MPS または CPU を使う必要がある。\n",
    "#\n",
    "# 対応方針：\n",
    "# 1) デバイスを自動選択（MPS → CUDA → CPU の順）。\n",
    "# 2) モデル・テンソル移動は `.cuda()` ではなく **`.to(device)`** に統一。\n",
    "# 3) 推論中心なら **`.eval()`** を付けてドロップアウト無効化。\n",
    "# 4) 9-10 の `encoding` も `.to(device)` に変更。\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "# --- 元のコード（参考：この2行が CUDA 非対応環境で落ちる） ---\n",
    "# bert_mlm = BertForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "# bert_mlm = bert_mlm.cuda()\n",
    "\n",
    "\n",
    "# --- 修正版：可搬なデバイス自動選択 ---\n",
    "def pick_device():\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")  # Apple Silicon (Metal)\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")  # NVIDIA CUDA\n",
    "    return torch.device(\"cpu\")  # フォールバック\n",
    "\n",
    "\n",
    "device = pick_device()\n",
    "bert_mlm = BertForMaskedLM.from_pretrained(MODEL_NAME).to(device).eval()\n",
    "print(f\"[info] bert_mlm loaded on: {device}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 9-10（修正版）：encoding も `.to(device)` に統一\n",
    "#  - 注意：MLM は本来 [MASK] 位置のみを予測するタスク。全位置 argmax は自己再現に寄りやすい。\n",
    "#    実運用では、怪しい部分を [MASK] 化して top-k から候補を選ぶ方が安定。\n",
    "# =============================================================================\n",
    "\n",
    "# 前段で用意済みの tokenizer / SC_tokenizer とする\n",
    "text = \"優勝トロフィーを変換した。\"\n",
    "\n",
    "# spans（原文中のサブワード位置）も同時取得\n",
    "encoding, spans = tokenizer.encode_plus_untagged(text, return_tensors=\"pt\")\n",
    "encoding = {\n",
    "    k: v.to(device) for k, v in encoding.items()\n",
    "}  # ← ここを .cuda() ではなく .to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = bert_mlm(**encoding)  # logits 形状：[B=1, T, |V|]\n",
    "    scores = output.logits\n",
    "    labels_predicted = scores[0].argmax(-1).to(\"cpu\").numpy().tolist()\n",
    "\n",
    "# 予測トークン列 → 文字列復元（special/PAD は内部で除外、'##' は除去、NFKC で正規化）\n",
    "predict_text = tokenizer.convert_bert_output_to_text(text, labels_predicted, spans)\n",
    "\n",
    "print(\"# input : \", text)\n",
    "print(\"# output: \", predict_text)\n",
    "\n",
    "# =============================================================================\n",
    "# メモ（理論）：\n",
    "# - 学習側（9-4）の encode_plus_tagged は「正解文の input_ids を labels に格納」→\n",
    "#   各位置の多クラス CE 損失で学習できる（PAD/特殊は ignore_index=-100 を推奨）。\n",
    "# - 推論側（本コード）は “全位置 argmax” のため、真の誤り訂正というより自己再現に寄る。\n",
    "#   改善案：疑わしい位置のみ [MASK]、top-k 候補をビーム探索 or 言語的制約で選ぶ。\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86a0b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.7928\n"
     ]
    }
   ],
   "source": [
    "# 9-11（説明コメント付き：MLM で「正解文トークン列」を教師にした最小学習ループ）\n",
    "# =============================================================================\n",
    "# 目的（理論）：\n",
    "# - 誤変換文 wrong_text を入力 x とし、正しい文 correct_text の input_ids を教師 y（各位置の正解トークンID）として\n",
    "#   BertForMaskedLM による **トークンごとの多クラス分類（語彙サイズ |V|）** で学習する最小例。\n",
    "# - CrossEntropyLoss は「各位置 t のロジット z_{t,*} と教師 label_{t}（語彙ID）」で計算。\n",
    "#   ただし **[CLS]/[SEP]/[PAD] など損失に含めたくない位置は -100（ignore_index）** にするのが定石。\n",
    "# - 注意：MLM は本来 [MASK] 位置のみの復元で事前学習されているため、全位置を一様に監督すると\n",
    "#   「自己再現」に寄りやすい。実務では（1）誤り疑い位置だけを [MASK] 化、（2）候補 top-k から置換判定、\n",
    "#   などを併用すると安定。\n",
    "# =============================================================================\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"wrong_text\": \"優勝トロフィーを変換した。\",\n",
    "        \"correct_text\": \"優勝トロフィーを返還した。\",\n",
    "    },\n",
    "    {\n",
    "        \"wrong_text\": \"人と森は強制している。\",\n",
    "        \"correct_text\": \"人と森は共生している。\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# 各データを符号化し、データローダへ入力できる形に整形\n",
    "# - encode_plus_tagged（9-4 で実装済み想定）は：\n",
    "#     input_ids/attention_mask/token_type_ids（= wrong_text 側）に加え、\n",
    "#     labels として correct_text の input_ids を格納する。\n",
    "# - max_length は wrong/correct 双方で同じ長さになる（prepare_for_modelの挙動）。\n",
    "max_length = 32\n",
    "dataset_for_loader = []\n",
    "for sample in data:\n",
    "    wrong_text = sample[\"wrong_text\"]\n",
    "    correct_text = sample[\"correct_text\"]\n",
    "    encoding = tokenizer.encode_plus_tagged(\n",
    "        wrong_text, correct_text, max_length=max_length\n",
    "    )\n",
    "    # Transformers の損失は long のラベル ID を想定（-100 も long）\n",
    "    encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "    dataset_for_loader.append(encoding)\n",
    "\n",
    "# データローダを作成（小サンプルなので 1 バッチ）\n",
    "dataloader = DataLoader(dataset_for_loader, batch_size=2)\n",
    "\n",
    "# --- デバイス選択：bert_mlm の実デバイスに合わせる（.cuda() は使わない） ---\n",
    "device = next(bert_mlm.parameters()).device  # 例：mps / cuda / cpu\n",
    "\n",
    "# トークナイザから特殊トークンIDを取得（BERT系では多くが [PAD]=0, [CLS]=101, [SEP]=102）\n",
    "pad_id = tokenizer.pad_token_id\n",
    "cls_id = tokenizer.cls_token_id\n",
    "sep_id = tokenizer.sep_token_id\n",
    "\n",
    "# ミニバッチを BERT へ入力し、損失を計算\n",
    "# - ラベルをそのまま渡すと特殊/PAD まで学習対象になるため、ignore_index=-100 に置換する\n",
    "bert_mlm.train()  # 学習想定（推論なら .eval() と no_grad を使う）\n",
    "for batch in dataloader:\n",
    "    # デバイスへ移動\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    # --- ignore_index マスクの構築（損失に含めない位置を -100 にする）---\n",
    "    # 1) PAD 位置（attention_mask==0）を除外\n",
    "    labels = batch[\"labels\"].clone()\n",
    "    amask = batch[\"attention_mask\"]  # 1=実トークン, 0=PAD\n",
    "    labels[amask == 0] = -100\n",
    "\n",
    "    # 2) [CLS]/[SEP] も除外（多くのケースで学習対象外にするのが定石）\n",
    "    inp = batch[\"input_ids\"]\n",
    "    labels[inp == cls_id] = -100\n",
    "    labels[inp == sep_id] = -100\n",
    "\n",
    "    # BertForMaskedLM は labels を与えると内部で CrossEntropyLoss を計算して output.loss を返す\n",
    "    # （ignore_index は -100 が既定）\n",
    "    # token_type_ids が無い場合は自動で None 扱いだが、あるなら一緒に渡す\n",
    "    output = bert_mlm(\n",
    "        input_ids=inp,\n",
    "        attention_mask=amask,\n",
    "        token_type_ids=batch.get(\"token_type_ids\", None),\n",
    "        labels=labels,\n",
    "    )\n",
    "    loss = output.loss  # 損失（バッチ×時系列の平均）\n",
    "\n",
    "    # 例：逆伝播（最小例。実運用では optimizer/scheduler/AMP/勾配クリップ等を併用）\n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    print(f\"loss: {loss.item():.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 追加メモ（理論と実装の橋渡し）：\n",
    "# - 「正解文の input_ids を labels にする」設計は **トークン置換** には有効。\n",
    "#   ただし挿入/削除や語順入れ替えが頻出する場合、位置合わせの仮定が崩れるため seq2seq/CTC などを検討。\n",
    "# - 本設計を“MLMらしく”するなら、wrong_text のうち置換したい部分だけを [MASK] 化し、\n",
    "#   labels も該当位置以外は -100 にして監督する手もある（局所最適化&副作用低減）。\n",
    "# - 実学習では AdamW + weight decay、学習率 1e-5〜5e-5、warmup/linear スケジューラ、\n",
    "#   勾配クリップ、mixed precision 等を併用すると安定。\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a22d8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   244  100   244    0     0    168      0  0:00:01  0:00:01 --:--:--   168\n",
      "100 64.9M  100 64.9M    0     0  1772k      0  0:00:37  0:00:37 --:--:-- 2082k\n",
      "x jwtd/\n",
      "x jwtd/train.jsonl\n",
      "x jwtd/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 9-12\n",
    "!curl -L \"https://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=https://nlp.ist.i.kyoto-u.ac.jp/nl-resource/JWTD/jwtd.tar.gz&name=JWTD.tar.gz\" -o JWTD.tar.gz\n",
    "!tar zxvf JWTD.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd49ef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習と検証用のデータセット：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'SC_tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 漢字誤変換の総数：235490\n",
      "- トークンの対応関係のつく文章の総数: 173992\n",
      "  (全体の74%)\n",
      "テスト用のデータセット：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'SC_tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 漢字誤変換の総数：3061\n",
      "- トークンの対応関係のつく文章の総数: 2289\n",
      "  (全体の75%)\n"
     ]
    }
   ],
   "source": [
    "# 9-4（説明コメント付き）：誤変換訂正タスク用データセット作成（トークン対応制約あり）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的：\n",
    "# - JWTD（日本語誤変換データ）から「漢字変換」に関するサンプルのみを抽出し、\n",
    "#   BERT 推論/学習に適した形式（wrong_text, correct_text のペア配列）へ整形する。\n",
    "# - SC_tokenizer（BertJapaneseTokenizer系）でトークン列を比較し、\n",
    "#   「トークン数が一致」かつ「差分トークン数が閾値以下」のものだけを採用して\n",
    "#   “位置合わせ” を単純化する（= token-level 監督が可能になる）。\n",
    "#\n",
    "# 理論メモ：\n",
    "# - 本手法は “置換型誤り” を前提とするため、挿入/削除を伴うケースは除外する設計になっている。\n",
    "#   （トークン数一致の制約）→ 位置 t ごとに語彙多クラス分類として監督できる。\n",
    "# - 事前に NFKC 正規化（互換分解＋合成）を行い、全/半角や互換文字の揺れを吸収して\n",
    "#   トークン化の安定性・比較の一貫性を高めている。\n",
    "# - Pandas の `query(..., inplace=True)` / `rename(..., inplace=True)` は\n",
    "#   引数 DataFrame を破壊的に更新する点に注意（呼び出し側とデータ再利用の設計に影響）。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def create_dataset(data_df):\n",
    "\n",
    "    tokenizer = SC_tokenizer.from_pretrained(\n",
    "        MODEL_NAME\n",
    "    )  # SC_tokenizer は 9-4 で定義済み想定\n",
    "    # BertJapaneseTokenizer ベースの tokenize() を使用\n",
    "\n",
    "    def check_token_count(row):\n",
    "        \"\"\"\n",
    "        誤変換の文章と正しい文章でトークンに対応がつくかどうかを判定。\n",
    "        （条件は上の文章を参照）\n",
    "\n",
    "        判定ロジックの要点：\n",
    "        - WordPiece トークン列の長さが一致（= 挿入/削除なし）。\n",
    "        - 差分トークン数 <= 閾値（“局所的な置換” のみを許容）。\n",
    "          ※ 閾値（threthold_count）は実験設計上のハイパーパラメータ。\n",
    "        \"\"\"\n",
    "        wrong_text_tokens = tokenizer.tokenize(row[\"wrong_text\"])\n",
    "        correct_text_tokens = tokenizer.tokenize(row[\"correct_text\"])\n",
    "\n",
    "        # 1) トークン数一致：位置合わせ（t 対応）を保証するための必須条件\n",
    "        if len(wrong_text_tokens) != len(correct_text_tokens):\n",
    "            return False\n",
    "\n",
    "        # 2) 差分が“少数”であることを要求（局所置換の想定）\n",
    "        diff_count = 0\n",
    "        threthold_count = (\n",
    "            2  # NOTE: 'threshold' の綴りミスだが、機能上は問題なし（修正は任意）\n",
    "        )\n",
    "        for wrong_text_token, correct_text_token in zip(\n",
    "            wrong_text_tokens, correct_text_tokens\n",
    "        ):\n",
    "\n",
    "            if wrong_text_token != correct_text_token:\n",
    "                diff_count += 1\n",
    "                if diff_count > threthold_count:\n",
    "                    # 許容差分を超えたら除外（挿入/削除に近い複雑な差分や広範な置換を排除）\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def normalize(text):\n",
    "        \"\"\"\n",
    "        文字列の正規化：\n",
    "        - 前後空白の除去（strip）\n",
    "        - NFKC 正規化（互換文字の正規化、全半角統一など）\n",
    "        ※ 学習・推論・評価の全段で同じ正規化を適用することで、一貫性を担保するのが原則。\n",
    "        \"\"\"\n",
    "        text = text.strip()\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "        return text\n",
    "\n",
    "    # --- データ抽出：カテゴリが「漢字誤変換」のものだけを残す（破壊的操作に注意） ---\n",
    "    category_type = \"kanji-conversion\"\n",
    "    data_df.query(\n",
    "        \"category == @category_type\", inplace=True\n",
    "    )  # inplace=True なので data_df が更新される\n",
    "    data_df.rename(\n",
    "        columns={\"pre_text\": \"wrong_text\", \"post_text\": \"correct_text\"}, inplace=True\n",
    "    )\n",
    "\n",
    "    # --- 正規化とフィルタリング ---\n",
    "    # 1) 文字列正規化（NFKC）で表記揺れを抑制\n",
    "    data_df[\"wrong_text\"] = data_df[\"wrong_text\"].map(normalize)\n",
    "    data_df[\"correct_text\"] = data_df[\"correct_text\"].map(normalize)\n",
    "\n",
    "    # 2) 統計出力のためにフィルタ前件数を保持\n",
    "    kanji_conversion_num = len(data_df)\n",
    "\n",
    "    # 3) トークン対応が成立するサンプルのみ抽出（挿入/削除や大規模置換を排除）\n",
    "    data_df = data_df[data_df.apply(check_token_count, axis=1)]\n",
    "    same_tokens_count_num = len(data_df)\n",
    "\n",
    "    # 4) 抽出率のログ（学習データの “素性” を把握して再現性を高める）\n",
    "    print(\n",
    "        f\"- 漢字誤変換の総数：{kanji_conversion_num}\",\n",
    "        f\"- トークンの対応関係のつく文章の総数: {same_tokens_count_num}\",\n",
    "        f\"  (全体の{same_tokens_count_num/kanji_conversion_num*100:.0f}%)\",\n",
    "        sep=\"\\n\",\n",
    "    )\n",
    "\n",
    "    # 返り値：学習・検証・テスト作成で共通利用しやすい、辞書リスト形式（records）\n",
    "    # 例：{'wrong_text': '...', 'correct_text': '...'}\n",
    "    return data_df[[\"wrong_text\", \"correct_text\"]].to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "# --- データのロード（train/test の JSON Lines） ---\n",
    "# 期待スキーマ（例）：\n",
    "#   {\n",
    "#     \"category\": \"kanji-conversion\",\n",
    "#     \"pre_text\": \"...\",   # 誤変換文\n",
    "#     \"post_text\": \"...\"   # 正しい文\n",
    "#     ...\n",
    "#   }\n",
    "# ※ 列名は直後に rename 済み（wrong_text, correct_text）\n",
    "train_df = pd.read_json(\"./jwtd/train.jsonl\", orient=\"records\", lines=True)\n",
    "test_df = pd.read_json(\"./jwtd/test.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "# --- 学習＋検証用データセットの作成 ---\n",
    "print(\"学習と検証用のデータセット：\")\n",
    "dataset = create_dataset(train_df)\n",
    "\n",
    "# シャッフル（※ 乱数 seed 未固定。再現実験では random.seed の固定推奨）\n",
    "random.shuffle(dataset)\n",
    "n = len(dataset)\n",
    "\n",
    "# ホールドアウト分割（8:2）。データ分布に偏りがある場合は層化分割の検討が望ましい。\n",
    "n_train = int(n * 0.8)\n",
    "dataset_train = dataset[:n_train]\n",
    "dataset_val = dataset[n_train:]\n",
    "\n",
    "# --- テスト用データセットの作成 ---\n",
    "print(\"テスト用のデータセット：\")\n",
    "dataset_test = create_dataset(test_df)\n",
    "\n",
    "# 補足（設計上の注意）：\n",
    "# - この段階では “置換型誤り” に限定したデータだけが残るため、\n",
    "#   モデルが学習する分布も置換中心にバイアスされる。\n",
    "#   実運用で挿入/削除・語順の乱れが多い場合、seq2seq/CTC など別定式化の導入を検討。\n",
    "# - Pandas の inplace 操作により、呼び出し元の DataFrame が更新される点に注意。\n",
    "#   他処理と共有するなら .copy() 運用を推奨（ここでは元コードを尊重して未変更）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37717f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
