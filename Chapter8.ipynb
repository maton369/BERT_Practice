{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a1aa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maton/BERT_Practice/chap8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maton/.pyenv/versions/3.10.13/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# 8-1\n",
    "!mkdir chap8\n",
    "%cd ./chap8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba06fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-3\n",
    "import itertools\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForTokenClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 日本語学習済みモデル\n",
    "MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8cf15c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ＡＢＣ -> ABC\n",
      "ABC -> ABC\n",
      "１２３ -> 123\n",
      "123 -> 123\n",
      "アイウ -> アイウ\n",
      "ｱｲｳ -> アイウ\n"
     ]
    }
   ],
   "source": [
    "# 8-4（説明コメント付き：Unicode 正規化 NFKC による表記ゆれの吸収）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的：\n",
    "#  - Unicode の **互換性分解 + 合成**（NFKC）で、全角/半角・互換文字（例：半角ｶﾅ・ローマ数字・① 等）\n",
    "#    を正規化し、検索/重複排除/機械学習前処理での表記ゆれを減らす。\n",
    "#\n",
    "# 理論メモ（NFC/NFD/NFKC/NFKD の違い）：\n",
    "#  - NFC: 正規分解 → 合成（互換性は無視）。見た目を保ちながら正規の合成形式へ。\n",
    "#  - NFD: 正規分解のみ（結合文字にバラす）。検索・照合の下処理に使うことがある。\n",
    "#  - NFKC: **互換分解**（見た目は同じでも “意味的に同一視される” 文字を分解）→ 合成。\n",
    "#          例：全角英数/記号、半角ｶﾅ、ローマ数字 Ⅳ、丸数字 ① などを通常の文字に畳み込む。\n",
    "#  - NFKD: 互換分解のみ。\n",
    "# 互換分解は “情報の落ち” が起こり得る（例：①→1、㍍→メートル→m など）。監査用途では注意。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "# NFKC で正規化する関数\n",
    "# - 全角英数→半角、半角ｶﾅ→全角カタカナ、結合記号の統合 等を一括で行う\n",
    "normalize = lambda s: unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "# 動作例：全角/半角の統一（学習前の前処理・検索キー作成などで有効）\n",
    "print(f'ＡＢＣ -> {normalize(\"ＡＢＣ\")}')  # 全角アルファベット → 半角 \"ABC\"\n",
    "print(f'ABC -> {normalize(\"ABC\")}')  # 既に半角 → 変化なし\n",
    "print(f'１２３ -> {normalize(\"１２３\")}')  # 全角数字 → 半角 \"123\"\n",
    "print(f'123 -> {normalize(\"123\")}')  # 既に半角 → 変化なし\n",
    "print(f'アイウ -> {normalize(\"アイウ\")}')  # 全角カタカナ → 変化なし\n",
    "print(f'ｱｲｳ -> {normalize(\"ｱｲｳ\")}')  # 半角ｶﾅ → 全角カタカナ \"アイウ\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 追加の知見（必要ならテストして確認）：\n",
    "#  - 濁点付き半角ｶﾅ（例：ｶﾞ）も結合が解決され全角「ガ」に統一される：\n",
    "#      normalize(\"ｶﾞ\") == \"ガ\"\n",
    "#  - 互換文字の折り畳み：\n",
    "#      normalize(\"ⅠⅡⅢ\") -> \"III\"  （ローマ数字 → ラテン大文字）\n",
    "#      normalize(\"①②\")   -> \"12\"   （丸数字 → 通常数字）\n",
    "#  - 正規化は “可逆でない” ことがあるため、**原文は別項目で保存**しておくと安全。\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bea67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-5（説明コメント付き：日本語BERT用 NER 前処理ユーティリティ）\n",
    "# =============================================================================\n",
    "# 前提：\n",
    "#  - BertJapaneseTokenizer（transformers）を継承して、固有表現抽出（NER）向けの\n",
    "#    2種類のエンコード（教師あり/教師なし）と、モデル出力から固有表現スパンを復元する\n",
    "#    補助関数を提供するクラスである。\n",
    "#  - ラベリング方式は BIO ではなく「整数 ID によるスパン連結」方式（O=0, その他=タイプID）。\n",
    "#    → 連続する同一ラベルのトークン列を 1 つのエンティティにまとめる前提。\n",
    "#  - 重複・ネスト・オーバーラップするエンティティは表現できない（単純スパンのみ）。\n",
    "#  - prepare_for_model により [CLS]/[SEP] の特殊トークンが付与される前提で、ラベルを先頭/末尾0に整合。\n",
    "#  - max_length を超える部分は切り捨てられるため、切断されたエンティティは欠落し得る点に注意。\n",
    "#  - encode_plus_untagged では、MeCab 分かち書き（word_tokenizer）→ WordPiece（subword_tokenizer）\n",
    "#    で二段階トークナイズを行い、トークン表層を原文へシーケンシャルマッチしてスパンを得る。\n",
    "#    同一トークンの繰り返しが多い文でも、左から貪欲にマッチしていくため順次対応できる。\n",
    "#  - return_tensors='pt' 指定時、encoding のみ Tensor 化する（spans はリストのまま返る）。\n",
    "#    下流でテンソルを期待する場合は適宜変換すること。\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class NER_tokenizer(BertJapaneseTokenizer):\n",
    "\n",
    "    def encode_plus_tagged(self, text, entities, max_length):\n",
    "        \"\"\"\n",
    "        文章とそれに含まれる固有表現が与えられた時に、\n",
    "        符号化とラベル列の作成を行う。\n",
    "\n",
    "        引数:\n",
    "          text: str\n",
    "            元の文章（原文）。エンティティ抽出の対象。\n",
    "          entities: List[Dict]\n",
    "            [{'span': [start, end], 'type_id': int}, ...] の形を想定。\n",
    "            span は原文 text における [start, end) の半開区間、type_id は 0 以外の整数。\n",
    "          max_length: int\n",
    "            BERT 入力の最大系列長。prepare_for_model で [CLS]/[SEP] を含む長さに正規化される。\n",
    "\n",
    "        戻り値:\n",
    "          encoding: Dict[str, List[int]]\n",
    "            input_ids / attention_mask / token_type_ids / labels を含む辞書。\n",
    "            labels は [0]=CLS, 末尾 [0]=SEP/必要に応じ PAD を 0 で詰める。\n",
    "        \"\"\"\n",
    "        # --- 前処理: エンティティを開始位置で昇順ソート ---\n",
    "        entities = sorted(entities, key=lambda x: x[\"span\"][0])\n",
    "\n",
    "        # --- 原文を「非固有表現片(O=0)」「固有表現片(=type_id)」に分割して並べる ---\n",
    "        splitted = []  # 分割後の文字列片を順に蓄積\n",
    "        position = 0  # 直前に処理した末尾の次インデックス\n",
    "        for entity in entities:\n",
    "            start = entity[\"span\"][0]\n",
    "            end = entity[\"span\"][1]\n",
    "            label = entity[\"type_id\"]\n",
    "            # 非固有表現部分（直近の position からエンティティ開始まで）→ ラベル 0\n",
    "            splitted.append({\"text\": text[position:start], \"label\": 0})\n",
    "            # 固有表現部分（[start:end)）→ ラベル type_id\n",
    "            splitted.append({\"text\": text[start:end], \"label\": label})\n",
    "            position = end\n",
    "        # 最後のエンティティ以降の末尾テキストも非固有表現として追加\n",
    "        splitted.append({\"text\": text[position:], \"label\": 0})\n",
    "        # 空文字は除去（連続するエンティティで start==end 等により空が生じ得るため）\n",
    "        splitted = [s for s in splitted if s[\"text\"]]\n",
    "\n",
    "        # --- 片ごとにトークン化し、各トークンへ片に対応するラベルを付与 ---\n",
    "        tokens = []  # WordPiece トークン列\n",
    "        labels = []  # トークンに対するラベル（0 or type_id）\n",
    "        for text_splitted in splitted:\n",
    "            text = text_splitted[\"text\"]\n",
    "            label = text_splitted[\"label\"]\n",
    "            # BertJapaneseTokenizer.tokenize は日本語前処理（MeCab）+ WordPiece を内部で実施\n",
    "            tokens_splitted = self.tokenize(text)\n",
    "            labels_splitted = [label] * len(tokens_splitted)\n",
    "            tokens.extend(tokens_splitted)\n",
    "            labels.extend(labels_splitted)\n",
    "\n",
    "        # --- トークン列を ID 列に変換し、BERT 入力辞書へ整形 ---\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids, max_length=max_length, padding=\"max_length\", truncation=True\n",
    "        )  # prepare_for_model が [CLS]/[SEP] 等を付与し、長さを揃える\n",
    "\n",
    "        # --- ラベルの特殊トークン・PAD 整合 ---\n",
    "        # 先頭に CLS=0 を付与、本文ラベルは最大長-2（CLS/SEP）までに切詰め、末尾に SEP=0\n",
    "        labels = [0] + labels[: max_length - 2] + [0]\n",
    "        # さらに不足分（PAD 部分）を 0 で埋め、最終的に長さを max_length に一致させる\n",
    "        labels = labels + [0] * (max_length - len(labels))\n",
    "        encoding[\"labels\"] = labels\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def encode_plus_untagged(self, text, max_length=None, return_tensors=None):\n",
    "        \"\"\"\n",
    "        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
    "\n",
    "        引数:\n",
    "          text: str\n",
    "            元の文章。\n",
    "          max_length: Optional[int]\n",
    "            指定時は padding='max_length', truncation=True で固定長化。\n",
    "            未指定時は可変長（特殊トークン付与は維持）。\n",
    "          return_tensors: Optional[str]\n",
    "            'pt' を指定すると encoding の各値を torch.Tensor(batch次元つき) に変換。\n",
    "\n",
    "        戻り値:\n",
    "          encoding: Dict[str, List[int]] or Dict[str, torch.Tensor]\n",
    "            prepare_for_model の出力（[CLS]/[SEP] 付与後）。\n",
    "          spans: List[List[int]]\n",
    "            各トークンに対応する原文上の [start, end) 位置。\n",
    "            特殊トークン([CLS],[SEP],[PAD])の位置は [-1,-1] のダミーにする。\n",
    "        \"\"\"\n",
    "        # --- 形態素（MeCab）→ サブワード（WordPiece）の二段トークナイズ ---\n",
    "        tokens = []  # WordPiece トークン\n",
    "        tokens_original = (\n",
    "            []\n",
    "        )  # スパン計算用の「表層文字列」列（'##' 除去/UNK は単語そのまま）\n",
    "        words = self.word_tokenizer.tokenize(text)  # MeCab による単語列\n",
    "        for word in words:\n",
    "            # 単語を WordPiece に分割\n",
    "            tokens_word = self.subword_tokenizer.tokenize(word)\n",
    "            tokens.extend(tokens_word)\n",
    "            if (\n",
    "                tokens_word[0] == \"[UNK]\"\n",
    "            ):  # 未知語は WordPiece に分割できない → 原単語をそのまま使う\n",
    "                tokens_original.append(word)\n",
    "            else:\n",
    "                # '##' 接頭辞を削って表層形を復元（スパン同定に用いる）\n",
    "                tokens_original.extend(\n",
    "                    [token.replace(\"##\", \"\") for token in tokens_word]\n",
    "                )\n",
    "\n",
    "        # --- 原文上のスパンを左から順次マッチで同定（空白/記号も考慮して進める）---\n",
    "        position = 0  # 原文上の走査位置（左から前進のみ）\n",
    "        spans = []  # 各トークンの [start, end) を蓄積\n",
    "        for token in tokens_original:\n",
    "            l = len(token)\n",
    "            while 1:\n",
    "                # 現在位置から長さ l を切り出して一致判定（空白等を飛ばすため不一致なら1文字進める）\n",
    "                if token != text[position : position + l]:\n",
    "                    position += 1\n",
    "                else:\n",
    "                    spans.append([position, position + l])\n",
    "                    position += l\n",
    "                    break\n",
    "        # ここでの手法は「逐次マッチ」のため、同一部分文字列の繰り返しがあっても左から順に整合が取れる。\n",
    "        # ただし、原文編集（正規化や空白折り畳み）を別途行う場合は、同じ変換を token 側にも適用しておくこと。\n",
    "\n",
    "        # --- BERT 入力辞書へ整形（固定長/可変長を分岐）---\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\" if max_length else False,\n",
    "            truncation=True if max_length else False,\n",
    "        )\n",
    "        sequence_length = len(\n",
    "            encoding[\"input_ids\"]\n",
    "        )  # [CLS]/[SEP] を含む最終トークン列長\n",
    "\n",
    "        # --- 特殊トークン分のダミー span を付与し、長さを encoding に一致させる ---\n",
    "        # 先頭 [CLS] 用のダミー\n",
    "        spans = [[-1, -1]] + spans[: sequence_length - 2]\n",
    "        # 末尾 [SEP] と PAD 分をダミーで埋める（長さを sequence_length に揃える）\n",
    "        spans = spans + [[-1, -1]] * (sequence_length - len(spans))\n",
    "\n",
    "        # --- 必要なら Tensor 化（encoding のみ。spans はリストで返す） ---\n",
    "        if return_tensors == \"pt\":\n",
    "            encoding = {k: torch.tensor([v]) for k, v in encoding.items()}\n",
    "\n",
    "        return encoding, spans\n",
    "\n",
    "    def convert_bert_output_to_entities(self, text, labels, spans):\n",
    "        \"\"\"\n",
    "        文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n",
    "\n",
    "        引数:\n",
    "          text: str\n",
    "            原文。\n",
    "          labels: Sequence[int]\n",
    "            各トークンのラベル（O=0, その他は type_id）。[CLS]/[SEP]/[PAD] 位置の分も含む想定。\n",
    "          spans: Sequence[List[int]]\n",
    "            各トークンの [start, end)。特殊トークンは [-1,-1]。\n",
    "\n",
    "        戻り値:\n",
    "          entities: List[Dict]\n",
    "            {\"name\": 原文片, \"span\": [start,end], \"type_id\": ラベルID} の配列。\n",
    "        \"\"\"\n",
    "        # --- 特殊トークン（span=-1）に対応するラベル/スパンを除去して本文トークンに限定 ---\n",
    "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
    "        spans = [span for span in spans if span[0] != -1]\n",
    "\n",
    "        # --- 連続する同一ラベルをまとめて 1 スパンのエンティティに復元 ---\n",
    "        # itertools.groupby により、labels の連続区間ごとにグルーピングする。\n",
    "        # 例: labels=[0,0,2,2,0,3] → (0区間)(2区間)(0区間)(3区間)\n",
    "        # ラベル0（O）は無視し、非0の区間のみエンティティを生成する。\n",
    "        entities = []\n",
    "        for label, group in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
    "\n",
    "            group = list(group)  # group は [(idx,label), ...] の列\n",
    "            start = spans[group[0][0]][0]  # 区間先頭トークンの start\n",
    "            end = spans[group[-1][0]][1]  # 区間末尾トークンの end（半開区間の終端）\n",
    "\n",
    "            if label != 0:  # O 以外のラベルのみエンティティ化\n",
    "                entity = {\n",
    "                    \"name\": text[start:end],  # 原文断片（復元テキスト）\n",
    "                    \"span\": [start, end],  # 原文上の [start,end)\n",
    "                    \"type_id\": label,  # ラベル ID（BIO ではなく type_id そのもの）\n",
    "                }\n",
    "                entities.append(entity)\n",
    "\n",
    "        # 注意：\n",
    "        #  - BIO 方式ではないため、同一ラベルが連続すれば 1 エンティティに連結される。\n",
    "        #    同じ type_id が離れて複数回出現する場合は、それぞれ別エンティティとして復元される。\n",
    "        #  - ネスト/オーバーラップは扱えない（単純連続区間のみ）。\n",
    "        #  - max_length による切詰めでエンティティの一部が失われると、復元されない場合がある。\n",
    "\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f4f30b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 8-6\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c10f011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 10271, 28486, 5, 546, 3000, 1518, 233, 13, 5, 1878, 2682, 9, 10750, 308, 10, 8, 3, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], 'labels': [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# 8-7（説明コメント付き：NER_tokenizer.encode_plus_tagged の動作確認）\n",
    "# =============================================================================\n",
    "# 目的：\n",
    "#  - 原文 `text` と、原文上のスパン（[start, end) の半開区間）＋ラベルIDからなる `entities`\n",
    "#    を与え、BERT 入力（input_ids/attention_mask/token_type_ids）に整形すると同時に、\n",
    "#    各トークンに対応する **ラベル列**（O=0, エンティティは type_id）を作成する。\n",
    "#\n",
    "# 理論メモ：\n",
    "#  - encode_plus_tagged は「BIO 方式」ではなく **連結方式**（同一ラベルが連続するトークン列を1つの\n",
    "#    エンティティとみなす）前提でラベルを作る。特殊トークン [CLS]/[SEP]/[PAD] は 0 として埋める。\n",
    "#  - `span=[start,end]` は **半開区間**（start 文字を含み end 文字を含まない）。インデックスは 0 始まり。\n",
    "#  - トークン化は WordPiece なので、1語が複数トークンに分割されても、分割前の片に付けたラベルが\n",
    "#    そのまま各トークンに複製される設計。\n",
    "#  - `max_length` 超過分は切り捨てられるため、末尾側のエンティティが途中で切れると、対応トークンに\n",
    "#    ラベルが乗らず欠落し得る点に注意（長さ設計 or スライディングで対処）。\n",
    "#  - 直前に正規化（例：NFKC）を行った場合、**スパンは正規化後の文字列基準**で与えること（前後で\n",
    "#    文字長が変わるとズレる）。\n",
    "# =============================================================================\n",
    "\n",
    "# （互換/保守）NER_tokenizer が未インポート/未定義でも動くように最小限のフォールバックを用意\n",
    "try:\n",
    "    NER_tokenizer\n",
    "except NameError:\n",
    "    # ここではクラス本体は 8-5 で定義済み想定。未定義なら明示エラーにする。\n",
    "    raise RuntimeError(\n",
    "        \"NER_tokenizer クラスが未定義です。先に 8-5 の定義を実行してください。\"\n",
    "    )\n",
    "\n",
    "# 既存の `tokenizer` が NER_tokenizer でない場合は、同名で置き換えても構わない運用なら差し替える\n",
    "try:\n",
    "    tokenizer\n",
    "    has_encode_tagged = hasattr(tokenizer, \"encode_plus_tagged\")\n",
    "except NameError:\n",
    "    has_encode_tagged = False\n",
    "\n",
    "if not has_encode_tagged:\n",
    "    # 学習時と同じ語彙を使う（未定義なら東北大BERTにフォールバック）\n",
    "    try:\n",
    "        MODEL_NAME\n",
    "    except NameError:\n",
    "        MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\"\n",
    "    tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --------------------------- 元のコード（＋説明コメント） ---------------------------\n",
    "\n",
    "# 入力テキスト（インデックスは 0 始まり）\n",
    "# 文字位置: 0:昨,1:日,2:の,3:み,4:ら,5:い,6:事,7:務,8:所,9:と,10:の,11:打,12:ち,13:合,14:わ,15:せ,16:は,17:順,18:調,19:だ,20:っ,21:た,22:。\n",
    "text = \"昨日のみらい事務所との打ち合わせは順調だった。\"\n",
    "\n",
    "# エンティティの指定：\n",
    "#  - name は人間可読の補助で、実際のラベル付与は span と type_id によって行われる\n",
    "#  - span=[3,9) → 原文中の「みらい事務所」（3〜8文字目）を指定（end=9 は「と」なので非包含）\n",
    "#  - type_id=1 → O（背景）を 0 とするクラス設計の「1番」カテゴリ\n",
    "entities = [{\"name\": \"みらい事務所\", \"span\": [3, 9], \"type_id\": 1}]\n",
    "\n",
    "# ラベル列作成つきの符号化（最大長は [CLS]/[SEP] を含むトークン長に正規化される）\n",
    "# labels は先頭[CLS]=0、本文（max_length-2 まで）、末尾[SEP]=0、残りPAD=0 で埋められる。\n",
    "encoding = tokenizer.encode_plus_tagged(text, entities, max_length=20)\n",
    "\n",
    "# 出力の中身は Hugging Face の標準キー（input_ids, attention_mask, token_type_ids）に加え、\n",
    "# 1:1 に対応する labels が入っている。確認用にそのまま表示。\n",
    "print(encoding)\n",
    "\n",
    "# 追加の確認（任意）：\n",
    "# - トークン列長とラベル列長が一致していること\n",
    "# - ラベル中の 1 の連続区間が「みらい事務所」の WordPiece 分割数に一致していること\n",
    "# 例：\n",
    "# tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "# print(tokens)\n",
    "# print(encoding[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85a51968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# encoding\n",
      "{'input_ids': tensor([[    2,     1,     5,  1543,   125,     9,  6749, 28550,  2953, 28550,\n",
      "         28566, 21202, 28683, 14050, 12475,    12,    31,     8,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "# spans\n",
      "[[-1, -1], [0, 2], [2, 3], [3, 5], [5, 6], [6, 7], [7, 9], [9, 10], [10, 12], [12, 13], [13, 14], [15, 18], [18, 19], [19, 23], [24, 27], [27, 28], [28, 30], [30, 31], [-1, -1]]\n"
     ]
    }
   ],
   "source": [
    "# 8-8\n",
    "text = \"騰訊の英語名はTencent Holdings Ltdである。\"\n",
    "encoding, spans = tokenizer.encode_plus_untagged(text, return_tensors=\"pt\")\n",
    "print(\"# encoding\")\n",
    "print(encoding)\n",
    "print(\"# spans\")\n",
    "print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55cb9eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': '騰訊の', 'span': [0, 3], 'type_id': 1}, {'name': 'ncent Holdings Ltdで', 'span': [9, 28], 'type_id': 1}]\n"
     ]
    }
   ],
   "source": [
    "# 8-9\n",
    "labels_predicted = [0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "entities = tokenizer.convert_bert_output_to_entities(text, labels_predicted, spans)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ff5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
